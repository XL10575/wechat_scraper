#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆç‰ˆè‡ªåŠ¨ä¸‹è½½ä¸Šä¼ å·¥å…·
ç»“åˆäº†ï¼š1.æ£€æµ‹é“¾æ¥ 2.ä¸‹è½½æ–‡ä»¶ 3.ä¸Šä¼ è‡³äº‘æ–‡æ¡£ 4.ç§»åŠ¨åˆ°çŸ¥è¯†åº“
æ”¯æŒPDFå’ŒDOCXæ ¼å¼ï¼ŒåŒ…å«é‡å¤æ£€æŸ¥
"""

import os
import json
import time
import requests
import argparse
from pathlib import Path
from typing import Optional, Dict, List, Union
from loguru import logger
from feishu_user_client import FeishuUserAPIClient
from simple_url_scraper import SimpleUrlScraper
import re
from datetime import datetime

class IntegratedAutoUploader:
    """æ•´åˆç‰ˆè‡ªåŠ¨ä¸‹è½½ä¸Šä¼ å™¨"""
    
    def __init__(self, app_id: str = None, app_secret: str = None):
        """åˆå§‹åŒ–æ•´åˆä¸Šä¼ å™¨"""
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.app_id = app_id or os.getenv('FEISHU_APP_ID')
        self.app_secret = app_secret or os.getenv('FEISHU_APP_SECRET')
        
        if not self.app_id or not self.app_secret:
            # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
            try:
                with open('user_feishu_config.json', 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    self.app_id = self.app_id or config.get('app_id')
                    self.app_secret = self.app_secret or config.get('app_secret')
            except:
                pass
                
        if not self.app_id or not self.app_secret:
            raise ValueError("âŒ é£ä¹¦APP IDå’ŒSecretæœªé…ç½®ï¼è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶")
        
        # åˆå§‹åŒ–é£ä¹¦å®¢æˆ·ç«¯
        self.feishu_client = FeishuUserAPIClient(self.app_id, self.app_secret)
        
        # åˆå§‹åŒ–ä¸‹è½½å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.url_scraper = None
        
        # é…ç½®ä¿¡æ¯ - ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–
        self.space_id = os.getenv('FEISHU_SPACE_ID', "7511922459407450115")
        self.parent_wiki_token = "Rkr5w3y8hib7dRk1KpFcMZ7tnGc"  # ç›®æ ‡çˆ¶é¡µé¢token
        self.ro_folder_token = "BTZkfStogleXeZdbyH7cEyvdnog"  # ROå…¬ä¼—å·æ–‡ç« æ–‡ä»¶å¤¹
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path("output/auto_download")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸Šä¼ è®°å½•
        self.upload_log_file = "integrated_upload_log.json"
        self.upload_log = self._load_upload_log()
        
        logger.info("ğŸš€ æ•´åˆç‰ˆè‡ªåŠ¨ä¸‹è½½ä¸Šä¼ å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š é…ç½®ä¿¡æ¯: Space ID={self.space_id}")
    
    def _load_upload_log(self) -> Dict:
        """åŠ è½½ä¸Šä¼ æ—¥å¿—"""
        try:
            if os.path.exists(self.upload_log_file):
                with open(self.upload_log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½ä¸Šä¼ æ—¥å¿—å¤±è´¥: {e}")
        return {}
    
    def _save_upload_log(self):
        """ä¿å­˜ä¸Šä¼ æ—¥å¿—"""
        try:
            with open(self.upload_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.upload_log, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸Šä¼ æ—¥å¿—å¤±è´¥: {e}")
    
    def _is_url_processed(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†"""
        return url in self.upload_log and self.upload_log[url].get('status') == 'completed'
    
    def _mark_url_processed(self, url: str, file_path: str, wiki_token: str = None, error: str = None):
        """æ ‡è®°URLå¤„ç†çŠ¶æ€"""
        self.upload_log[url] = {
            'timestamp': time.time(),
            'file_path': str(file_path) if file_path else None,
            'wiki_token': wiki_token,
            'status': 'completed' if wiki_token else 'failed',
            'error': error
        }
        self._save_upload_log()
    
    def _get_scraper(self) -> SimpleUrlScraper:
        """è·å–URLæŠ“å–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self.url_scraper:
            logger.info("ğŸ”§ åˆå§‹åŒ–URLæŠ“å–å™¨...")
            self.url_scraper = SimpleUrlScraper(headless=True)
        return self.url_scraper
    
    def _is_valid_wechat_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¾®ä¿¡æ–‡ç« URL"""
        if not url or not url.startswith('http'):
            return False
        return 'mp.weixin.qq.com/s' in url
    
    def _extract_title_from_url_light(self, url: str) -> Optional[str]:
        """è½»é‡çº§æ–¹æ³•ä»URLè·å–æ–‡ç« æ ‡é¢˜ï¼Œä¸å¯åŠ¨æµè§ˆå™¨"""
        try:
            logger.debug(f"ğŸš€ å°è¯•è½»é‡çº§è·å–æ ‡é¢˜: {url[:50]}...")
            
            # ä½¿ç”¨requestsè·å–é¡µé¢HTML
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç®€å•è§£ætitleæ ‡ç­¾
            import re
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', response.text, re.IGNORECASE)
            if title_match:
                title = title_match.group(1).strip()
                # æ¸…ç†å¾®ä¿¡å…¬ä¼—å·ç‰¹æœ‰çš„æ ‡é¢˜æ ¼å¼
                if title:
                    logger.debug(f"âœ… è½»é‡çº§æ–¹æ³•è·å–æ ‡é¢˜æˆåŠŸ: {title}")
                    return title
            
            # å°è¯•ä»og:titleè·å–
            og_title_match = re.search(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\'][^>]*>', response.text, re.IGNORECASE)
            if og_title_match:
                title = og_title_match.group(1).strip()
                if title:
                    logger.debug(f"âœ… ä»og:titleè·å–æ ‡é¢˜æˆåŠŸ: {title}")
                    return title
            
            logger.debug(f"âš ï¸ è½»é‡çº§æ–¹æ³•æœªæ‰¾åˆ°æ ‡é¢˜")
            return None
            
        except Exception as e:
            logger.debug(f"âš ï¸ è½»é‡çº§è·å–æ ‡é¢˜å¤±è´¥: {e}")
            return None
    
    def check_file_duplicate_by_title(self, title: str, filename: str = None) -> bool:
        """é€šè¿‡æ ‡é¢˜æ£€æŸ¥äº‘æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¸­æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶"""
        try:
            logger.info(f"ğŸ” å¼€å§‹å…¨é¢é‡åæ£€æµ‹: {title}")
            
            # 1. æ£€æŸ¥äº‘æ–‡æ¡£ä¸­æ˜¯å¦æœ‰é‡åæ–‡ä»¶
            if filename:
                logger.debug(f"ğŸ—‚ï¸ æ£€æŸ¥äº‘æ–‡æ¡£é‡å: {filename}")
                drive_exists = self.feishu_client.check_file_exists_in_drive(
                    self.ro_folder_token, 
                    filename
                )
                if drive_exists:
                    logger.warning(f"ğŸ“‹ äº‘æ–‡æ¡£ä¸­å·²å­˜åœ¨åŒåæ–‡ä»¶: {filename}")
                    return True
            
            # 2. æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰é‡åæ–‡ä»¶  
            logger.debug(f"ğŸ“š æ£€æŸ¥çŸ¥è¯†åº“é‡å: {title}")
            wiki_exists = self.feishu_client.check_file_exists_in_wiki(
                self.space_id, 
                title, 
                self.parent_wiki_token
            )
            
            if wiki_exists:
                logger.warning(f"ğŸ“‹ çŸ¥è¯†åº“ä¸­å·²å­˜åœ¨åŒåæ–‡ä»¶: {title}")
                return True
            
            logger.debug(f"âœ… é‡åæ£€æµ‹é€šè¿‡ï¼Œæ— é‡å¤æ–‡ä»¶")
            return False
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ–‡ä»¶é‡å¤æ—¶å‡ºé”™: {e}")
            return False
    
    def download_article(self, url: str, format_type: str = "pdf") -> Union[Path, str, None]:
        """ä¸‹è½½å¾®ä¿¡æ–‡ç« ä¸ºæŒ‡å®šæ ¼å¼
        
        Args:
            url: å¾®ä¿¡æ–‡ç« URL
            format_type: æ–‡ä»¶æ ¼å¼ ("pdf" æˆ– "docx")
            
        Returns:
            Path: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ï¼ˆæˆåŠŸï¼‰
            "DUPLICATE_SKIPPED": è·³è¿‡ä¸‹è½½ï¼ˆæ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼‰
            None: ä¸‹è½½å¤±è´¥
        """
        try:
            logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡ç« : {url[:50]}...")
            logger.info(f"ğŸ“„ æ ¼å¼: {format_type.upper()}")
            
            # ğŸ”¥ å»¶è¿Ÿæµè§ˆå™¨åˆå§‹åŒ–ï¼šå…ˆè½»é‡çº§è·å–æ ‡é¢˜ï¼Œå†åšé‡å¤æ£€æµ‹
            logger.info(f"ğŸš€ ä½¿ç”¨è½»é‡çº§æ–¹æ³•è·å–æ–‡ç« æ ‡é¢˜...")
            
            # ç®€å•ä»URLè·å–åŸºæœ¬ä¿¡æ¯ï¼Œä¸åˆå§‹åŒ–æµè§ˆå™¨
            title = self._extract_title_from_url_light(url)
            if not title:
                # å¦‚æœè½»é‡çº§æ–¹æ³•å¤±è´¥ï¼Œæ‰ä½¿ç”¨æµè§ˆå™¨
                logger.info(f"âš ï¸ è½»é‡çº§æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æµè§ˆå™¨è·å–æ–‡ç« ä¿¡æ¯...")
                scraper = self._get_scraper()
                article_info = scraper.extract_article_info(url)
                if not article_info or 'error' in article_info:
                    logger.error("æ— æ³•è·å–æ–‡ç« ä¿¡æ¯")
                    return None
                title = article_info.get('title', 'unknown_article')
            
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:80]
            if not safe_title.strip():
                safe_title = f"article_{datetime.now().strftime('%H%M%S')}"
            
            # ç”Ÿæˆæ–‡ä»¶å
            file_ext = f".{format_type}"
            filename = f"{safe_title}{file_ext}"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ˆåŒæ—¶æ£€æŸ¥äº‘æ–‡æ¡£å’ŒçŸ¥è¯†åº“ï¼‰
            logger.info(f"ğŸ” é‡å¤æ£€æµ‹ï¼š{safe_title}")
            if self.check_file_duplicate_by_title(safe_title, filename):
                logger.warning(f"ğŸš« è·³è¿‡ä¸‹è½½ï¼Œå·²å­˜åœ¨åŒåæ–‡ä»¶: {filename}")
                return "DUPLICATE_SKIPPED"  # ğŸ”¥ ç‰¹æ®Šè¿”å›å€¼è¡¨ç¤ºè·³è¿‡è€Œéå¤±è´¥
            
            # ğŸ“¥ çœŸæ­£å¼€å§‹ä¸‹è½½ï¼Œæ­¤æ—¶æ‰åˆå§‹åŒ–æµè§ˆå™¨
            logger.info(f"âœ… é‡å¤æ£€æµ‹é€šè¿‡ï¼Œå¼€å§‹å®é™…ä¸‹è½½...")
            scraper = self._get_scraper()
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            output_path = self.output_dir / filename
            
            # å¤„ç†æ–‡ä»¶åå†²çª
            counter = 1
            original_path = output_path
            while output_path.exists():
                filename = f"{safe_title}_{counter}{file_ext}"
                output_path = self.output_dir / filename
                counter += 1
            
            # ä¸‹è½½æ–‡ä»¶
            logger.info(f"ğŸ’¾ ä¿å­˜ä¸º: {filename}")
            
            success = False
            if format_type == "pdf":
                success = scraper.save_as_pdf(url, str(output_path))
            elif format_type == "docx":
                success = scraper.save_as_docx(url, str(output_path))
            
            if success and output_path.exists():
                file_size = output_path.stat().st_size
                logger.success(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
                return output_path
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {filename}")
                return None
                
        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ç« æ—¶å‡ºé”™: {e}")
            return None
    
    def upload_to_drive(self, file_path: Path) -> Optional[str]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°é£ä¹¦äº‘æ–‡æ¡£
        
        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶tokenï¼Œå¤±è´¥è¿”å›None
        """
        try:
            filename = file_path.name
            title = file_path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            
            logger.info(f"â˜ï¸ ä¸Šä¼ æ–‡ä»¶åˆ°äº‘æ–‡æ¡£: {filename}")
            
            # ğŸ†• ä¸Šä¼ å‰å†æ¬¡æ£€æŸ¥äº‘æ–‡æ¡£é‡åï¼ˆé˜²æ­¢å¤šçº¿ç¨‹å¹¶å‘é—®é¢˜ï¼‰
            logger.debug(f"ğŸ” ä¸Šä¼ å‰æœ€åæ£€æŸ¥äº‘æ–‡æ¡£é‡å: {filename}")
            if self.feishu_client.check_file_exists_in_drive(self.ro_folder_token, filename):
                logger.warning(f"ğŸ“‹ äº‘æ–‡æ¡£ä¸Šä¼ æ—¶å‘ç°é‡åæ–‡ä»¶ï¼Œè·³è¿‡ä¸Šä¼ : {filename}")
                # å°è¯•è·å–å·²å­˜åœ¨æ–‡ä»¶çš„tokenï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
                return "DUPLICATE"
            
            file_token = self.feishu_client.upload_file_to_drive(
                str(file_path),
                parent_node=self.ro_folder_token,
                parent_type="explorer"
            )
            
            if file_token:
                drive_url = f"https://thedream.feishu.cn/file/{file_token}"
                logger.success(f"âœ… äº‘æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {drive_url}")
                return file_token
            else:
                logger.error(f"âŒ äº‘æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {filename}")
                return None
                
        except Exception as e:
            logger.error(f"ä¸Šä¼ åˆ°äº‘æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return None
    
    def move_to_wiki(self, file_token: str, file_name: str) -> Optional[str]:
        """ç§»åŠ¨æ–‡ä»¶åˆ°çŸ¥è¯†åº“
        
        Args:
            file_token: äº‘æ–‡æ¡£æ–‡ä»¶token
            file_name: æ–‡ä»¶å
            
        Returns:
            çŸ¥è¯†åº“æ–‡æ¡£tokenï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.info(f"ğŸ“š ç§»åŠ¨æ–‡ä»¶åˆ°çŸ¥è¯†åº“: {file_name}")
            
            # ğŸ†• æ³¨é‡Šæ‰æœ‰é—®é¢˜çš„tokenæ£€æŸ¥ï¼Œé¿å…400é”™è¯¯
            # è¯¥æ£€æŸ¥ä¼šç”¨äº‘æ–‡æ¡£tokenæŸ¥è¯¢çŸ¥è¯†åº“APIï¼Œå¯¼è‡´400é”™è¯¯
            logger.debug(f"ğŸ” è·³è¿‡tokené‡å¤æ£€æŸ¥ï¼ˆé¿å…APIé”™è¯¯ï¼‰...")
            
            # ğŸ†• é¢å¤–æ£€æŸ¥ï¼šåŸºäºæ–‡ä»¶ååœ¨çŸ¥è¯†åº“ä¸­æœç´¢é‡å¤
            title_without_ext = os.path.splitext(file_name)[0]
            if self.feishu_client.check_file_exists_in_wiki(self.space_id, title_without_ext, self.parent_wiki_token):
                logger.warning(f"ğŸš« çŸ¥è¯†åº“ä¸­å·²å­˜åœ¨åŒåæ–‡æ¡£ï¼Œå–æ¶ˆè½¬ç§»: {title_without_ext}")
                return "DUPLICATE_TITLE"
            
            # ä½¿ç”¨move_docs_to_wiki API
            url = f"https://open.feishu.cn/open-apis/wiki/v2/spaces/{self.space_id}/nodes/move_docs_to_wiki"
            
            # è·å–è®¿é—®ä»¤ç‰Œ
            if not self.feishu_client.ensure_valid_token():
                logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ")
                return None
            
            headers = {
                'Authorization': f'Bearer {self.feishu_client.access_token}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "obj_token": file_token,
                "obj_type": "file",
                "parent_wiki_token": self.parent_wiki_token
            }
            
            logger.info(f"ğŸ”„ APIè°ƒç”¨: {url}")
            logger.debug(f"ğŸ“‹ è¯·æ±‚æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
            
            response = requests.post(url, headers=headers, json=data)
            logger.info(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    data_result = result.get('data', {})
                    if 'wiki_token' in data_result:
                        wiki_token = data_result['wiki_token']
                        wiki_url = f"https://thedream.feishu.cn/wiki/{wiki_token}"
                        logger.success(f"âœ… ç§»åŠ¨åˆ°çŸ¥è¯†åº“æˆåŠŸ: {wiki_url}")
                        return wiki_token
                    elif 'task_id' in data_result:
                        task_id = data_result['task_id']
                        logger.info(f"â³ ç§»åŠ¨ä»»åŠ¡å·²æäº¤: {task_id}")
                        return task_id  # è¿”å›task_idä½œä¸ºæ ‡è¯†
                else:
                    logger.error(f"âŒ APIè¿”å›é”™è¯¯: {result.get('msg')}")
            else:
                logger.error(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {response.text}")
            
            return None
            
        except Exception as e:
            logger.error(f"ç§»åŠ¨åˆ°çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            return None
    
    def process_single_url(self, url: str, format_type: str = "pdf") -> bool:
        """å¤„ç†å•ä¸ªURLçš„å®Œæ•´æµç¨‹
        
        Args:
            url: å¾®ä¿¡æ–‡ç« URL
            format_type: æ–‡ä»¶æ ¼å¼ ("pdf" æˆ– "docx")
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            url = url.strip()
            logger.info("=" * 60)
            logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†URL: {url[:50]}...")
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†
            if self._is_url_processed(url):
                logger.warning(f"ğŸ“‹ URLå·²å¤„ç†ï¼Œè·³è¿‡: {url[:50]}...")
                return True
            
            # éªŒè¯URLæ ¼å¼
            if not self._is_valid_wechat_url(url):
                logger.error(f"âŒ æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                self._mark_url_processed(url, None, error="æ— æ•ˆURL")
                return False
            
            # æ­¥éª¤1: ä¸‹è½½æ–‡ç« 
            logger.info("ğŸ“¥ æ­¥éª¤1: ä¸‹è½½æ–‡ç« ")
            file_path = self.download_article(url, format_type)
            if not file_path:
                self._mark_url_processed(url, None, error="ä¸‹è½½å¤±è´¥")
                return False
            
            # æ­¥éª¤2: ä¸Šä¼ åˆ°äº‘æ–‡æ¡£
            logger.info("â˜ï¸ æ­¥éª¤2: ä¸Šä¼ åˆ°äº‘æ–‡æ¡£")
            file_token = self.upload_to_drive(file_path)
            
            # å¤„ç†é‡å¤æ–‡ä»¶çš„æƒ…å†µ
            if file_token == "DUPLICATE":
                logger.warning(f"ğŸ“‹ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {file_path.name}")
                self._mark_url_processed(url, file_path, error="æ–‡ä»¶å·²å­˜åœ¨")
                return True  # è¿”å›Trueå› ä¸ºè¿™ä¸æ˜¯é”™è¯¯
            
            if not file_token:
                self._mark_url_processed(url, file_path, error="äº‘æ–‡æ¡£ä¸Šä¼ å¤±è´¥")
                return False
            
            # æ­¥éª¤3: ç§»åŠ¨åˆ°çŸ¥è¯†åº“
            logger.info("ğŸ“š æ­¥éª¤3: ç§»åŠ¨åˆ°çŸ¥è¯†åº“")
            wiki_result = self.move_to_wiki(file_token, file_path.name)
            
            # ğŸ†• å¤„ç†é‡å¤æ–‡ä»¶çš„æƒ…å†µ
            if wiki_result in ["DUPLICATE_IN_WIKI", "DUPLICATE_TITLE"]:
                if wiki_result == "DUPLICATE_IN_WIKI":
                    logger.warning(f"ğŸš« æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­ï¼Œåœæ­¢å¤„ç†: {file_path.name}")
                    self._mark_url_processed(url, file_path, error="æ–‡ä»¶å·²å­˜åœ¨äºçŸ¥è¯†åº“")
                elif wiki_result == "DUPLICATE_TITLE":
                    logger.warning(f"ğŸš« çŸ¥è¯†åº“ä¸­å·²å­˜åœ¨åŒåæ–‡æ¡£ï¼Œåœæ­¢å¤„ç†: {file_path.name}")
                    self._mark_url_processed(url, file_path, error="çŸ¥è¯†åº“ä¸­å­˜åœ¨åŒåæ–‡æ¡£")
                
                # æ¸…ç†æœ¬åœ°æ–‡ä»¶
                try:
                    if file_path.exists():
                        file_path.unlink()
                        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†æœ¬åœ°æ–‡ä»¶: {file_path.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
                
                return False  # è¿”å›Falseè¡¨ç¤ºé‡å¤ï¼Œéœ€è¦åœ¨ä¸Šå±‚å¤„ç†
            
            if not wiki_result:
                self._mark_url_processed(url, file_path, error="çŸ¥è¯†åº“ç§»åŠ¨å¤±è´¥")
                return False
            
            # æ ‡è®°å¤„ç†å®Œæˆ
            self._mark_url_processed(url, file_path, wiki_result)
            
            logger.success(f"ğŸ‰ å®Œæ•´æµç¨‹å¤„ç†æˆåŠŸ: {file_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†URLæ—¶å‡ºé”™: {e}")
            self._mark_url_processed(url, None, error=str(e))
            return False
    
    def process_multiple_urls(self, urls: List[str], format_type: str = "pdf", delay: int = 2) -> Dict[str, int]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            format_type: æ–‡ä»¶æ ¼å¼
            delay: å¤„ç†é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        stats = {'total': len(urls), 'success': 0, 'failed': 0, 'skipped': 0}
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {stats['total']} ä¸ªURL")
        logger.info(f"ğŸ“„ æ–‡ä»¶æ ¼å¼: {format_type.upper()}")
        logger.info(f"â±ï¸ å¤„ç†é—´éš”: {delay}ç§’")
        
        for i, url in enumerate(urls, 1):
            logger.info(f"\nğŸ“Š è¿›åº¦: [{i}/{stats['total']}]")
            
            try:
                success = self.process_single_url(url, format_type)
                if success:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡çš„æƒ…å†µ
                    if self._is_url_processed(url) and self.upload_log[url].get('error') in [None, "æ— æ•ˆURL"]:
                        if "è·³è¿‡" in str(self.upload_log[url]):
                            stats['skipped'] += 1
                        else:
                            stats['success'] += 1
                    else:
                        stats['success'] += 1
                else:
                    stats['failed'] += 1
                    
            except Exception as e:
                logger.error(f"å¤„ç†URL {i} æ—¶å‡ºé”™: {e}")
                stats['failed'] += 1
            
            # æ·»åŠ å»¶è¿Ÿï¼ˆæœ€åä¸€ä¸ªä¸å»¶è¿Ÿï¼‰
            if i < len(urls) and delay > 0:
                logger.info(f"â³ ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        
        # è¾“å‡ºç»Ÿè®¡
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
        logger.info(f"  æ€»URLæ•°: {stats['total']}")
        logger.info(f"  æˆåŠŸå¤„ç†: {stats['success']}")
        logger.info(f"  è·³è¿‡é‡å¤: {stats['skipped']}")
        logger.info(f"  å¤„ç†å¤±è´¥: {stats['failed']}")
        
        if stats['failed'] == 0:
            logger.success("ğŸ‰ å…¨éƒ¨å¤„ç†æˆåŠŸï¼")
        else:
            logger.warning(f"âš ï¸ æœ‰ {stats['failed']} ä¸ªURLå¤„ç†å¤±è´¥")
        
        return stats
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.url_scraper:
                self.url_scraper.cleanup()
                logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"èµ„æºæ¸…ç†æ—¶å‡ºé”™: {e}")

def load_urls_from_file(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        logger.info(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ªURL: {file_path}")
        return urls
    except Exception as e:
        logger.error(f"âŒ åŠ è½½URLæ–‡ä»¶å¤±è´¥: {e}")
        return []

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ•´åˆç‰ˆè‡ªåŠ¨ä¸‹è½½ä¸Šä¼ å·¥å…·')
    parser.add_argument('--input', type=str, help='è¾“å…¥URLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--url', type=str, help='å•ä¸ªURL')
    parser.add_argument('--format', type=str, default='pdf', choices=['pdf', 'docx'], help='æ–‡ä»¶æ ¼å¼')
    parser.add_argument('--delay', type=int, default=3, help='å¤„ç†é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--max-files', type=int, help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°')
    parser.add_argument('--auto-mode', action='store_true', help='è‡ªåŠ¨æ¨¡å¼ï¼ˆä»GitHub Actionsè°ƒç”¨ï¼‰')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ä¸Šä¼ å™¨
    try:
        uploader = IntegratedAutoUploader()
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    try:
        if args.auto_mode:
            # GitHub Actionsè‡ªåŠ¨æ¨¡å¼
            logger.info("ğŸš€ GitHub Actions - ROæ–‡ç« è‡ªåŠ¨æ›´æ–°å¼€å§‹")
            logger.info("=" * 60)
            
            # æ£€æŸ¥é£ä¹¦é…ç½®
            if os.path.exists('user_feishu_config.json'):
                logger.info("âœ… é£ä¹¦åº”ç”¨é…ç½®å·²åŠ è½½")
            else:
                logger.error("âŒ é£ä¹¦é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            # å¤„ç†æ”¶é›†åˆ°çš„æ–‡ç« 
            if args.input and os.path.exists(args.input):
                urls = load_urls_from_file(args.input)
                if urls:
                    logger.info(f"ğŸ“š æ­¥éª¤3: å¤„ç†æ–‡ç« ä¸‹è½½ä¸Šä¼ ...")
                    
                    # é™åˆ¶å¤„ç†æ•°é‡
                    if args.max_files and len(urls) > args.max_files:
                        urls = urls[:args.max_files]
                        logger.info(f"ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡ä¸º: {args.max_files}")
                    
                    success_count = 0
                    for i, url in enumerate(urls, 1):
                        logger.info(f"ğŸ“„ å¤„ç† {i}/{len(urls)}: {url[:50]}...")
                        logger.info(f"   URL: {url[:80]}...")
                        
                        try:
                            success = uploader.process_single_url(url, args.format)
                            if success:
                                success_count += 1
                                logger.info(f"   âœ… ä¸Šä¼ æˆåŠŸ")
                            else:
                                logger.info(f"   âŒ ä¸Šä¼ å¤±è´¥")
                        except Exception as e:
                            logger.error(f"   âŒ å¤„ç†å‡ºé”™: {e}")
                        
                        # æ·»åŠ å»¶è¿Ÿ
                        if i < len(urls):
                            logger.info(f"   â³ ç­‰å¾… {args.delay} ç§’...")
                            time.sleep(args.delay)
                    
                    # è¾“å‡ºç»Ÿè®¡
                    success_rate = (success_count / len(urls) * 100) if urls else 0
                    logger.info(f"ğŸ“Š å¤„ç†å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸ")
                    
                    # GitHub Actions è¾“å‡ºæ ¼å¼
                    print(f"ğŸ‰ ROè‡ªåŠ¨æ›´æ–°å®Œæˆï¼")
                    print(f"ğŸ“Š æ”¶é›†æ–‡ç« : {len(urls)} ç¯‡")
                    print(f"ğŸ“Š æˆåŠŸä¸Šä¼ : {success_count}/{len(urls)} ç¯‡")
                    print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„URL")
            else:
                logger.error("âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š")
                
        elif args.url:
            # å•ä¸ªURLå¤„ç†
            logger.info(f"ğŸ¯ å¤„ç†å•ä¸ªURL: {args.url}")
            success = uploader.process_single_url(args.url, args.format)
            if success:
                logger.success("âœ… å¤„ç†æˆåŠŸ!")
            else:
                logger.error("âŒ å¤„ç†å¤±è´¥!")
                
        elif args.input:
            # æ‰¹é‡URLå¤„ç†
            urls = load_urls_from_file(args.input)
            if urls:
                if args.max_files and len(urls) > args.max_files:
                    urls = urls[:args.max_files]
                    logger.info(f"ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡ä¸º: {args.max_files}")
                
                stats = uploader.process_multiple_urls(urls, args.format, args.delay)
                logger.info("ğŸ“Š å¤„ç†å®Œæˆ")
            else:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„URL")
        else:
            # é»˜è®¤æ¨¡å¼
            logger.info("æ•´åˆç‰ˆè‡ªåŠ¨ä¸Šä¼ å™¨å·²å‡†å¤‡å°±ç»ª")
            logger.info("ä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨å‚æ•°")
            
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
    finally:
        uploader.cleanup()

if __name__ == "__main__":
    main() 
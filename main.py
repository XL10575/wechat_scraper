#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WeChat Article URL Processor
å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·

ä¸“é—¨å¤„ç†ç”¨æˆ·æä¾›çš„å¾®ä¿¡æ–‡ç« URLï¼Œç›´æ¥ä¿å­˜ä¸ºé£ä¹¦çŸ¥è¯†åº“æ ¼å¼
"""

import argparse
import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Any

from loguru import logger
from simple_url_scraper import SimpleUrlScraper
from feishu_exporter import FeishuExporter
from utils import save_to_json
from config import LOG_FILE


def setup_logging(log_level: str = "INFO") -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    logger.add(
        sys.stderr,
        level=log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        colorize=True
    )
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        LOG_FILE,
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="10 MB",
        retention="7 days",
        encoding="utf-8"
    )


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…· - ç›´æ¥å¤„ç†URLå¹¶ä¿å­˜ä¸ºå¤šç§æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ ä½¿ç”¨æ–¹æ³•ï¼š

1. å¤„ç†å•ä¸ªURLï¼ˆPDFæ ¼å¼ï¼Œä¿æŒåŸå§‹æ ·å¼ï¼‰ï¼š
   %(prog)s --url "https://mp.weixin.qq.com/s/LiS8ytwKsKP9yAHGp96G_Q" --format pdf

2. å¤„ç†URLæ–‡ä»¶ï¼ˆå¤šç§æ ¼å¼ï¼‰ï¼š
   %(prog)s --urls_file urls.txt --format all

3. å¤„ç†å¤šä¸ªURLï¼ˆé£ä¹¦æ ¼å¼ï¼‰ï¼š
   %(prog)s --urls "url1" "url2" "url3" --format individual

4. æ— å¤´æ¨¡å¼ï¼š
   %(prog)s --urls_file urls.txt --headless --format pdf

âœ¨ ç‰¹ç‚¹ï¼š
- ğŸ¯ ç›´æ¥å¤„ç†å¾®ä¿¡æ–‡ç« URLï¼Œæ— éœ€æœç´¢
- ğŸ–¥ï¸ å¯è§æ¨¡å¼ä¾¿äºæ‰‹åŠ¨å¤„ç†éªŒè¯ç 
- ğŸ“„ å®Œç¾é£ä¹¦é›†æˆï¼ˆindividualæ ¼å¼ï¼‰
- ğŸ“‘ PDFæ ¼å¼ä¿æŒåŸå§‹æ ·å¼å’Œå›¾ç‰‡
- ğŸ”„ æ™ºèƒ½é‡è¯•å’Œé”™è¯¯å¤„ç†
- ğŸ“Š è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡

ğŸ“‹ æ ¼å¼è¯´æ˜ï¼š
- pdf: ä¿æŒåŸå§‹æ ¼å¼çš„PDFæ–‡ä»¶ï¼ˆæ¨èï¼ï¼‰
- complete_html: åŒ…å«å›¾ç‰‡çš„å®Œæ•´HTMLæ–‡ä»¶
- individual: æ¯ç¯‡æ–‡ç« å•ç‹¬Markdownæ–‡ä»¶ï¼ˆé€‚åˆé£ä¹¦ï¼‰
- markdown: åˆå¹¶çš„Markdownæ–‡ä»¶
- json: ç»“æ„åŒ–JSONæ•°æ®
- all: æ‰€æœ‰æ ¼å¼éƒ½å¯¼å‡º

ğŸ“‹ URLæ ¼å¼ç¤ºä¾‹ï¼š
   https://mp.weixin.qq.com/s/LiS8ytwKsKP9yAHGp96G_Q
        """
    )
    
    # URLè¾“å…¥æ–¹å¼ (äº’æ–¥ç»„)
    url_group = parser.add_mutually_exclusive_group(required=True)
    url_group.add_argument(
        '--url',
        type=str,
        help='å•ä¸ªå¾®ä¿¡æ–‡ç« URL'
    )
    url_group.add_argument(
        '--urls',
        nargs='+',
        help='å¤šä¸ªå¾®ä¿¡æ–‡ç« URL (ç©ºæ ¼åˆ†éš”)'
    )
    url_group.add_argument(
        '--urls_file',
        type=str,
        help='åŒ…å«å¾®ä¿¡æ–‡ç« URLçš„æ–‡ä»¶è·¯å¾„ (æ¯è¡Œä¸€ä¸ªURL)'
    )
    
    # è¾“å‡ºæ ¼å¼
    parser.add_argument(
        '--format',
        choices=['json', 'markdown', 'html', 'individual', 'pdf', 'complete_html', 'all'],
        default='pdf',
        help='è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤pdfï¼Œä¿æŒåŸå§‹æ ¼å¼ï¼‰'
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        '--output', '--output_filename',
        type=str,
        help='è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶åå‰ç¼€'
    )
    
    # æµè§ˆå™¨é€‰é¡¹
    parser.add_argument(
        '--headless',
        action='store_true',
        help='æ— å¤´æ¨¡å¼è¿è¡Œï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰'
    )
    
    # æ—¥å¿—é€‰é¡¹
    parser.add_argument(
        '--log_level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤INFOï¼‰'
    )
    
    return parser.parse_args()


def load_urls_from_file(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            urls = []
            for line_num, line in enumerate(f, 1):
                url = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if url and not url.startswith('#'):
                    # éªŒè¯URLæ ¼å¼
                    if url.startswith('http') and 'mp.weixin.qq.com/s' in url:
                        urls.append(url)
                    else:
                        logger.warning(f"ç¬¬{line_num}è¡Œ: æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
        
        logger.info(f"ä»æ–‡ä»¶ {file_path} åŠ è½½äº† {len(urls)} ä¸ªæœ‰æ•ˆURL")
        return urls
        
    except Exception as e:
        logger.error(f"è¯»å–URLæ–‡ä»¶å¤±è´¥: {e}")
        return []


def main() -> int:
    """ä¸»å‡½æ•°"""
    try:
        # è§£æå‚æ•°
        args = parse_arguments()
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(args.log_level)
        
        logger.info("=" * 60)
        logger.info("ğŸš€ å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·å¯åŠ¨")
        logger.info("=" * 60)
        
        # è·å–URLåˆ—è¡¨
        urls = []
        
        if args.url:
            urls = [args.url]
            logger.info(f"å¤„ç†å•ä¸ªURL: {args.url}")
            
        elif args.urls:
            urls = args.urls
            logger.info(f"å¤„ç†å‘½ä»¤è¡Œæä¾›çš„ {len(urls)} ä¸ªURL")
            
        elif args.urls_file:
            urls = load_urls_from_file(args.urls_file)
            if not urls:
                logger.error("âŒ URLæ–‡ä»¶ä¸ºç©ºæˆ–è¯»å–å¤±è´¥")
                return 1
        
        # éªŒè¯URL
        valid_urls = []
        for url in urls:
            if url.startswith('http') and 'mp.weixin.qq.com/s' in url:
                valid_urls.append(url)
            else:
                logger.warning(f"è·³è¿‡æ— æ•ˆURL: {url}")
        
        if not valid_urls:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å¾®ä¿¡æ–‡ç« URL")
            return 1
        
        logger.info(f"ğŸ“‹ æœ‰æ•ˆURLæ•°é‡: {len(valid_urls)}")
        logger.info(f"ğŸ–¥ï¸ æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if args.headless else 'å¯è§æ¨¡å¼'}")
        logger.info(f"ğŸ“„ è¾“å‡ºæ ¼å¼: {args.format}")
        
        # åˆå§‹åŒ–URLå¤„ç†å™¨
        logger.info("ğŸ”§ åˆå§‹åŒ–URLå¤„ç†å™¨...")
        scraper = SimpleUrlScraper(headless=args.headless)
        
        start_time = time.time()
        
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            if args.output:
                base_filename = args.output
            else:
                base_filename = f"wechat_articles_{timestamp}"
            
            # å¯¼å‡ºç»“æœ
            success = False
            output_files = []
            
            if args.format == 'pdf' or args.format == 'all':
                # PDFæ ¼å¼ - ä¿æŒåŸå§‹æ ·å¼
                logger.info("ğŸ’¾ ç”ŸæˆPDFæ–‡ä»¶ï¼ˆä¿æŒåŸå§‹æ ¼å¼ï¼‰...")
                pdf_dir = f"output/pdf"
                saved_pdfs = scraper.process_urls_as_pdf(valid_urls, pdf_dir)
                
                if saved_pdfs:
                    logger.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(saved_pdfs)} ä¸ªPDFæ–‡ä»¶")
                    output_files.append(f"{pdf_dir}/ ({len(saved_pdfs)} ä¸ªPDFæ–‡ä»¶)")
                    success = True
            
            if args.format == 'complete_html' or args.format == 'all':
                # å®Œæ•´HTMLæ ¼å¼ - åŒ…å«å›¾ç‰‡
                logger.info("ğŸ’¾ ç”Ÿæˆå®Œæ•´HTMLæ–‡ä»¶ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰...")
                html_dir = "output/complete_html"
                html_success_count = 0
                
                from tqdm import tqdm
                for i, url in enumerate(tqdm(valid_urls, desc="ç”ŸæˆHTML")):
                    try:
                        # è·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                        article_info = scraper.extract_article_info(url)
                        if 'error' not in article_info:
                            title = article_info.get('title', f'æ–‡ç« _{i+1}')
                            import re
                            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                            html_filename = f"{i+1:02d}_{safe_title}.html"
                            html_path = f"{html_dir}/{html_filename}"
                            
                            if scraper.save_complete_html(url, html_path):
                                html_success_count += 1
                    except Exception as e:
                        logger.error(f"ç”ŸæˆHTMLå¤±è´¥: {e}")
                
                if html_success_count > 0:
                    logger.success(f"âœ… æˆåŠŸç”Ÿæˆ {html_success_count} ä¸ªå®Œæ•´HTMLæ–‡ä»¶")
                    output_files.append(f"{html_dir}/ ({html_success_count} ä¸ªHTMLæ–‡ä»¶)")
                    success = True
            
            # å¤„ç†ä¼ ç»Ÿæ ¼å¼ï¼ˆéœ€è¦å…ˆè·å–æ–‡ç« å†…å®¹ï¼‰
            if args.format in ['json', 'markdown', 'html', 'individual', 'all']:
                logger.info("ğŸš€ å¼€å§‹æå–æ–‡ç« å†…å®¹...")
                articles = scraper.process_urls(valid_urls)
                
                if not articles:
                    logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« å†…å®¹")
                else:
                    logger.info("ğŸ’¾ å¯¼å‡ºä¼ ç»Ÿæ ¼å¼æ–‡ä»¶...")
                    
                    if args.format == 'json' or args.format == 'all':
                        # å¯¼å‡ºJSON
                        json_filename = f"{base_filename}.json"
                        if save_to_json(articles, json_filename):
                            logger.success(f"âœ… JSONå·²å¯¼å‡º: {json_filename}")
                            output_files.append(json_filename)
                            success = True
                    
                    if args.format in ['markdown', 'html', 'individual', 'all']:
                        # ä½¿ç”¨é£ä¹¦å¯¼å‡ºå™¨
                        exporter = FeishuExporter()
                        
                        if args.format == 'markdown' or args.format == 'all':
                            md_filename = f"{base_filename}.md"
                            if exporter.export_to_markdown(articles, md_filename):
                                output_files.append(md_filename)
                                success = True
                        
                        if args.format == 'html' or args.format == 'all':
                            html_filename = f"{base_filename}.html"
                            if exporter.export_to_html(articles, html_filename):
                                output_files.append(html_filename)
                                success = True
                        
                        if args.format == 'individual' or args.format == 'all':
                            if exporter.export_articles_individually(articles, 'markdown'):
                                output_files.append("output/individual_articles/")
                                success = True
            
            if not success:
                logger.error("âŒ å¯¼å‡ºå¤±è´¥")
                return 1
            
            # ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = time.time() - start_time
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
            logger.info("=" * 60)
            logger.info(f"ğŸ“„ æ€»URLæ•°é‡: {len(valid_urls)}")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            logger.info(f"ğŸš€ å¹³å‡è€—æ—¶: {elapsed_time/len(valid_urls):.2f}ç§’/ç¯‡")
            
            logger.info("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
            for file in output_files:
                logger.info(f"  ğŸ“„ {file}")
            
            # æ ¼å¼ä½¿ç”¨æç¤º
            if args.format == 'pdf' or args.format == 'all':
                logger.info("\nğŸ‰ PDFæ ¼å¼ä¼˜åŠ¿:")
                logger.info("  â€¢ å®Œç¾ä¿æŒåŸæ–‡ç« çš„æ‰€æœ‰æ ¼å¼å’Œæ ·å¼")
                logger.info("  â€¢ åŒ…å«æ‰€æœ‰å›¾ç‰‡ã€è¡¨æ ¼ã€é“¾æ¥ç­‰å†…å®¹")
                logger.info("  â€¢ å¯ç›´æ¥æ‰“å°æˆ–åˆ†äº«ï¼Œæ ¼å¼ä¸ä¼šæ”¹å˜")
                logger.info("  â€¢ é€‚åˆå­˜æ¡£å’Œæ­£å¼æ–‡æ¡£ç”¨é€”")
            
            if args.format == 'complete_html' or args.format == 'all':
                logger.info("\nğŸŒ å®Œæ•´HTMLæ ¼å¼ä¼˜åŠ¿:")
                logger.info("  â€¢ ä¿æŒåŸå§‹ç½‘é¡µæ ¼å¼")
                logger.info("  â€¢ å›¾ç‰‡ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç¦»çº¿å¯ç”¨")
                logger.info("  â€¢ å¯åœ¨ä»»ä½•æµè§ˆå™¨ä¸­æ‰“å¼€")
                logger.info("  â€¢ æ”¯æŒå¤åˆ¶ç²˜è´´åˆ°å…¶ä»–åº”ç”¨")
            
            if args.format in ['individual', 'all']:
                logger.info("\nğŸš€ é£ä¹¦çŸ¥è¯†åº“ä½¿ç”¨æŒ‡å—:")
                logger.info("  1. æ‰“å¼€ output/individual_articles/ æ–‡ä»¶å¤¹")
                logger.info("  2. é€‰æ‹©æ‰€æœ‰ .md æ–‡ä»¶")
                logger.info("  3. æ‹–æ‹½åˆ°é£ä¹¦çŸ¥è¯†åº“è¿›è¡Œæ‰¹é‡ä¸Šä¼ ")
                logger.info("  4. æ¯ä¸ªæ–‡ä»¶ä¼šè‡ªåŠ¨åˆ›å»ºä¸ºç‹¬ç«‹çš„çŸ¥è¯†åº“é¡µé¢")
            
            logger.success("ğŸ‰ URLå¤„ç†ä»»åŠ¡å®Œæˆ!")
            return 0
            
        finally:
            # æ¸…ç†èµ„æº
            logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
            scraper.cleanup()
    
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 130
    
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        logger.debug("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        return 1


def show_usage_examples():
    """æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("\nğŸš€ å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·")
    print("=" * 50)
    print("âœ¨ ç›´æ¥å¤„ç†å¾®ä¿¡æ–‡ç« URLï¼Œæ”¯æŒå¤šç§æ ¼å¼è¾“å‡º")
    print("âœ¨ PDFæ ¼å¼å®Œç¾ä¿æŒåŸæ–‡æ ·å¼å’Œå›¾ç‰‡")
    print("âœ¨ å®Œç¾é›†æˆé£ä¹¦çŸ¥è¯†åº“")
    print("")
    print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•ç¤ºä¾‹:")
    print("")
    print("1. ğŸ¯ ç”ŸæˆPDFï¼ˆä¿æŒåŸå§‹æ ¼å¼ï¼Œæ¨èï¼ï¼‰:")
    print('   python main.py --url "https://mp.weixin.qq.com/s/xxxxx" --format pdf')
    print("")
    print("2. ğŸ“ æ‰¹é‡ç”ŸæˆPDF:")
    print("   python main.py --urls_file urls.txt --format pdf")
    print("")
    print("3. ğŸŒ ç”Ÿæˆå®Œæ•´HTMLï¼ˆåŒ…å«å›¾ç‰‡ï¼‰:")
    print('   python main.py --url "https://mp.weixin.qq.com/s/xxxxx" --format complete_html')
    print("")
    print("4. ğŸ“„ é£ä¹¦æ ¼å¼ï¼ˆMarkdownï¼‰:")
    print("   python main.py --urls_file urls.txt --format individual")
    print("")
    print("5. ğŸ”§ ç”Ÿæˆæ‰€æœ‰æ ¼å¼:")
    print("   python main.py --urls_file urls.txt --format all")
    print("")
    print("6. ğŸ–¥ï¸ æ— å¤´æ¨¡å¼:")
    print("   python main.py --urls_file urls.txt --headless --format pdf")
    print("")
    print("ğŸ“‹ æ ¼å¼è¯´æ˜:")
    print("  â€¢ pdf          - PDFæ–‡ä»¶ï¼Œä¿æŒåŸå§‹æ ¼å¼ï¼ˆæ¨èï¼ï¼‰")
    print("  â€¢ complete_html - å®Œæ•´HTMLï¼ŒåŒ…å«å›¾ç‰‡")
    print("  â€¢ individual   - ç‹¬ç«‹Markdownæ–‡ä»¶ï¼ˆé€‚åˆé£ä¹¦æ‰¹é‡ä¸Šä¼ ï¼‰")
    print("  â€¢ markdown     - åˆå¹¶Markdownæ–‡ä»¶")
    print("  â€¢ json         - ç»“æ„åŒ–JSONæ•°æ®")
    print("  â€¢ all          - æ‰€æœ‰æ ¼å¼éƒ½ç”Ÿæˆ")
    print("")
    print("ğŸ“‹ URLæ ¼å¼è¦æ±‚:")
    print("   https://mp.weixin.qq.com/s/æ–‡ç« ID")
    print("")
    print("ğŸ” è·å–å®Œæ•´å¸®åŠ©:")
    print("   python main.py --help")
    print("")


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    if len(sys.argv) == 1:
        show_usage_examples()
        sys.exit(0)
    
    exit_code = main()
    sys.exit(exit_code) 
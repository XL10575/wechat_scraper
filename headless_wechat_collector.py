#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— å¤´å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ”¶é›†å™¨
é€‚ç”¨äºGitHub Actionsç­‰è‡ªåŠ¨åŒ–ç¯å¢ƒ
åŸºäºå·²ä¿å­˜çš„ç™»å½•çŠ¶æ€è¿›è¡Œæ–‡ç« æ”¶é›†
"""

import requests
import json
import time
import pickle
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from loguru import logger
import os


class HeadlessWeChatCollector:
    """æ— å¤´å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ”¶é›†å™¨"""
    
    def __init__(self, cookies_file: str = "wechat_cookies.pkl", session_file: str = "wechat_session.json"):
        """åˆå§‹åŒ–æ”¶é›†å™¨
        
        Args:
            cookies_file: cookiesæ–‡ä»¶è·¯å¾„
            session_file: ä¼šè¯ä¿¡æ¯æ–‡ä»¶è·¯å¾„
        """
        self.cookies_file = cookies_file
        self.session_file = session_file
        self.session = requests.Session()
        self.token = ""
        self.user_info = {}
        
        # é¢‘ç‡æ§åˆ¶
        self.request_interval = 2.0
        self.last_request_time = 0
        
        logger.info("ğŸš€ æ— å¤´å¾®ä¿¡æ–‡ç« æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_session(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„ç™»å½•ä¼šè¯"""
        try:
            # åŠ è½½cookies
            if os.path.exists(self.cookies_file):
                with open(self.cookies_file, 'rb') as f:
                    cookies = pickle.load(f)
                    self.session.cookies.update(cookies)
                logger.info("âœ… CookiesåŠ è½½æˆåŠŸ")
            else:
                logger.error(f"âŒ Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
                return False
            
            # åŠ è½½ä¼šè¯ä¿¡æ¯
            if os.path.exists(self.session_file):
                with open(self.session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                    user_agent = session_data.get('user_agent')
                    if user_agent:
                        self.session.headers.update({'User-Agent': user_agent})
                logger.info("âœ… ä¼šè¯ä¿¡æ¯åŠ è½½æˆåŠŸ")
            else:
                # ä½¿ç”¨é»˜è®¤User-Agent
                self.session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                })
                logger.warning("âš ï¸ ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤User-Agent")
            
            # è®¾ç½®å…¶ä»–å¿…è¦çš„è¯·æ±‚å¤´
            self.session.headers.update({
                'Referer': 'https://mp.weixin.qq.com/',
                'Origin': 'https://mp.weixin.qq.com'
            })
            
            # éªŒè¯ç™»å½•çŠ¶æ€
            return self._verify_login_status()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä¼šè¯å¤±è´¥: {e}")
            return False
    
    def _verify_login_status(self) -> bool:
        """éªŒè¯ç™»å½•çŠ¶æ€"""
        try:
            logger.info("ğŸ” éªŒè¯ç™»å½•çŠ¶æ€...")
            
            # å°è¯•è®¿é—®ä¸»é¡µè·å–token
            home_url = "https://mp.weixin.qq.com"
            response = self.session.get(home_url, timeout=10)
            
            if response.status_code == 200 and 'token=' in response.text:
                # æå–token
                import re
                token_match = re.search(r'token=([^&"\']+)', response.text)
                if token_match:
                    self.token = token_match.group(1)
                    logger.info(f"âœ… ç™»å½•çŠ¶æ€æœ‰æ•ˆï¼Œtoken: {self.token[:10]}...")
                    
                    # è·å–ç”¨æˆ·ä¿¡æ¯
                    self._get_user_info()
                    return True
                else:
                    logger.error("âŒ æ— æ³•æå–token")
                    return False
            else:
                logger.error(f"âŒ ç™»å½•çŠ¶æ€æ— æ•ˆï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            user_url = "https://mp.weixin.qq.com/cgi-bin/loginpage"
            params = {'t': 'wxm-login', 'lang': 'zh_CN', 'token': self.token}
            
            response = self.session.get(user_url, params=params, timeout=10)
            if response.status_code == 200:
                # ç®€å•è§£æç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
                logger.info("âœ… ç”¨æˆ·ä¿¡æ¯è·å–æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
    
    def _wait_for_rate_limit(self):
        """é¢‘ç‡æ§åˆ¶"""
        current_time = time.time()
        time_diff = current_time - self.last_request_time
        
        if time_diff < self.request_interval:
            sleep_time = self.request_interval - time_diff
            logger.debug(f"â³ é¢‘ç‡æ§åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f} ç§’")
            time.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def search_account(self, keyword: str) -> List[Dict]:
        """æœç´¢å…¬ä¼—å·
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            
        Returns:
            å…¬ä¼—å·åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” æœç´¢å…¬ä¼—å·: {keyword}")
            self._wait_for_rate_limit()
            
            search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
            params = {
                'action': 'search_biz',
                'begin': 0,
                'count': 10,
                'query': keyword,
                'token': self.token,
                'lang': 'zh_CN',
                'f': 'json',
                'ajax': 1
            }
            
            response = self.session.get(search_url, params=params, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                
                if result.get('base_resp', {}).get('ret') == 0:
                    accounts = result.get('list', [])
                    logger.info(f"âœ… æ‰¾åˆ° {len(accounts)} ä¸ªå…¬ä¼—å·")
                    return accounts
                else:
                    error_msg = result.get('base_resp', {}).get('err_msg', 'æœªçŸ¥é”™è¯¯')
                    logger.error(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
                    return []
            else:
                logger.error(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ æœç´¢å‡ºé”™: {e}")
            return []
    
    def collect_articles(self, account: Dict, start_date: datetime, end_date: datetime, max_articles: int = 50) -> List[Dict]:
        """æ”¶é›†æŒ‡å®šå…¬ä¼—å·çš„æ–‡ç« 
        
        Args:
            account: å…¬ä¼—å·ä¿¡æ¯
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_articles: æœ€å¤§æ–‡ç« æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            fakeid = account.get('fakeid', '')
            if not fakeid:
                logger.error("âŒ æ— æ³•è·å–å…¬ä¼—å·ID")
                return []
            
            account_name = account.get('nickname', 'æœªçŸ¥å…¬ä¼—å·')
            logger.info(f"ğŸ“¥ å¼€å§‹æ”¶é›†æ–‡ç« : {account_name}")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            logger.info(f"ğŸ“Š æœ€å¤§æ•°é‡: {max_articles}")
            
            articles = []
            begin = 0
            page_size = 20
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_timestamp = int(start_date.timestamp())
            end_timestamp = int((end_date + timedelta(days=1) - timedelta(seconds=1)).timestamp())
            
            while len(articles) < max_articles:
                self._wait_for_rate_limit()
                
                logger.info(f"ğŸ“„ è·å–ç¬¬ {begin//page_size + 1} é¡µï¼Œå·²æ”¶é›† {len(articles)} ç¯‡")
                
                articles_url = "https://mp.weixin.qq.com/cgi-bin/appmsgpublish"
                params = {
                    'sub': 'list',
                    'search_field': 'null',
                    'begin': begin,
                    'count': page_size,
                    'query': '',
                    'fakeid': fakeid,
                    'type': 101_003,
                    'free_publish_type': 1,
                    'sub_action': 'list_ex',
                    'token': self.token,
                    'lang': 'zh_CN',
                    'f': 'json',
                    'ajax': 1
                }
                
                response = self.session.get(articles_url, params=params, timeout=15)
                
                if response.status_code != 200:
                    logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                    break
                
                result = response.json()
                
                if result.get('base_resp', {}).get('ret') != 0:
                    error_msg = result.get('base_resp', {}).get('err_msg', 'æœªçŸ¥é”™è¯¯')
                    logger.error(f"âŒ APIè¿”å›é”™è¯¯: {error_msg}")
                    break
                
                # è§£ææ–‡ç« 
                page_articles = self._parse_articles_from_response(result)
                
                if not page_articles:
                    logger.info("ğŸ“„ æ²¡æœ‰æ›´å¤šæ–‡ç« ")
                    break
                
                # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ–‡ç« 
                filtered_articles = []
                for article in page_articles:
                    article_time = article.get('create_time', 0)
                    if start_timestamp <= article_time <= end_timestamp:
                        filtered_articles.append(article)
                    elif article_time < start_timestamp:
                        # å¦‚æœæ–‡ç« æ—¶é—´æ—©äºå¼€å§‹æ—¶é—´ï¼Œè¯´æ˜å·²ç»è¶…å‡ºèŒƒå›´ï¼Œåœæ­¢æ”¶é›†
                        logger.info("â° å·²æ”¶é›†åˆ°æ—¶é—´èŒƒå›´å¤–çš„æ–‡ç« ï¼Œåœæ­¢æ”¶é›†")
                        return articles[:max_articles]
                
                articles.extend(filtered_articles)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ–‡ç« 
                if len(page_articles) < page_size:
                    logger.info("ğŸ“„ å·²æ”¶é›†å®Œæ‰€æœ‰æ–‡ç« ")
                    break
                
                begin += page_size
                
                # é¿å…æ— é™å¾ªç¯
                if begin > 1000:
                    logger.warning("âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶")
                    break
            
            result_articles = articles[:max_articles]
            logger.info(f"âœ… æ”¶é›†å®Œæˆï¼Œå…± {len(result_articles)} ç¯‡æ–‡ç« ")
            return result_articles
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æ–‡ç« å¤±è´¥: {e}")
            return []
    
    def _parse_articles_from_response(self, result: Dict) -> List[Dict]:
        """ä»APIå“åº”ä¸­è§£ææ–‡ç« åˆ—è¡¨"""
        articles = []
        
        try:
            publish_page_str = result.get('publish_page', '')
            if not publish_page_str:
                return articles
            
            publish_page = json.loads(publish_page_str)
            publish_list = publish_page.get('publish_list', [])
            
            for publish_item in publish_list:
                publish_info_str = publish_item.get('publish_info', '')
                if not publish_info_str:
                    continue
                
                publish_info = json.loads(publish_info_str)
                appmsgex_list = publish_info.get('appmsgex', [])
                
                for appmsg in appmsgex_list:
                    link = appmsg.get('link', '').replace('\\/', '/')
                    
                    article = {
                        'title': appmsg.get('title', ''),
                        'url': link,  # ä½¿ç”¨urlå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
                        'link': link,  # ä¿ç•™åŸå­—æ®µå
                        'author_name': appmsg.get('author_name', ''),
                        'digest': appmsg.get('digest', ''),
                        'update_time': appmsg.get('update_time', 0),
                        'create_time': appmsg.get('create_time', 0),
                        'publish_time': datetime.fromtimestamp(appmsg.get('create_time', 0)).strftime('%Y-%m-%d %H:%M:%S') if appmsg.get('create_time') else ''
                    }
                    articles.append(article)
            
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« åˆ—è¡¨å‡ºé”™: {e}")
        
        return articles
    
    def auto_collect_articles(self, account_name: str, days_back: int = 7, max_articles: int = 50) -> List[Dict]:
        """è‡ªåŠ¨æ”¶é›†æŒ‡å®šå…¬ä¼—å·çš„æ–‡ç« 
        
        Args:
            account_name: å…¬ä¼—å·åç§°
            days_back: å‘å‰æ”¶é›†å¤šå°‘å¤©çš„æ–‡ç« 
            max_articles: æœ€å¤§æ–‡ç« æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            # 1. åŠ è½½ç™»å½•çŠ¶æ€
            if not self.load_session():
                logger.error("âŒ åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥")
                return []
            
            # 2. æœç´¢å…¬ä¼—å·
            accounts = self.search_account(account_name)
            if not accounts:
                logger.error(f"âŒ æœªæ‰¾åˆ°å…¬ä¼—å·: {account_name}")
                return []
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…¬ä¼—å·
            target_account = accounts[0]
            logger.info(f"âœ… é€‰æ‹©å…¬ä¼—å·: {target_account.get('nickname', '')} ({target_account.get('alias', '')})")
            
            # 3. è®¡ç®—æ—¶é—´èŒƒå›´
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            # 4. æ”¶é›†æ–‡ç« 
            articles = self.collect_articles(target_account, start_date, end_date, max_articles)
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ”¶é›†å¤±è´¥: {e}")
            return []


def main():
    """æµ‹è¯•å‡½æ•°"""
    collector = HeadlessWeChatCollector()
    
    # æµ‹è¯•è‡ªåŠ¨æ”¶é›†
    articles = collector.auto_collect_articles('ä»™å¢ƒä¼ è¯´ROæ–°å¯èˆª', days_back=7, max_articles=20)
    
    if articles:
        print(f"\nâœ… æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« :")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title']}")
            print(f"   URL: {article['url']}")
            print(f"   æ—¶é—´: {article['publish_time']}")
            print()
    else:
        print("âŒ æœªæ”¶é›†åˆ°æ–‡ç« ")


if __name__ == "__main__":
    main() 
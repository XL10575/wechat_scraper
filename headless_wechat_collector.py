#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— å¤´å¾®ä¿¡æ–‡ç« æ”¶é›†å™¨

ä¸“ä¸ºGitHub Actionsç­‰æ— å¤´ç¯å¢ƒè®¾è®¡ï¼ŒåŸºäºä¿å­˜çš„ç™»å½•çŠ¶æ€è¿›è¡Œæ–‡ç« æ”¶é›†
"""

import pickle
import json
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from loguru import logger

class HeadlessWeChatCollector:
    """æ— å¤´å¾®ä¿¡æ–‡ç« æ”¶é›†å™¨"""
    
    def __init__(self, cookies_file: str = "wechat_cookies.pkl", session_file: str = "wechat_session.json"):
        """åˆå§‹åŒ–æ”¶é›†å™¨
        
        Args:
            cookies_file: cookiesæ–‡ä»¶è·¯å¾„
            session_file: ä¼šè¯ä¿¡æ¯æ–‡ä»¶è·¯å¾„
        """
        self.cookies_file = cookies_file
        self.session_file = session_file
        self.session = requests.Session()
        self.token = None
        self.user_agent = None
        
        # è®¾ç½®è¯·æ±‚è¶…æ—¶å’Œé‡è¯•
        self.session.timeout = 30
        self.max_retries = 3
        
        logger.info("ğŸš€ æ— å¤´å¾®ä¿¡æ–‡ç« æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_session(self) -> bool:
        """åŠ è½½ä¿å­˜çš„ç™»å½•çŠ¶æ€"""
        try:
            # 1. åŠ è½½cookies
            try:
                with open(self.cookies_file, 'rb') as f:
                    cookies = pickle.load(f)
                    self.session.cookies.update(cookies)
                logger.info("âœ… CookiesåŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ CookiesåŠ è½½å¤±è´¥: {e}")
                return False
            
            # 2. åŠ è½½ä¼šè¯ä¿¡æ¯
            try:
                with open(self.session_file, 'r', encoding='utf-8') as f:
                    session_data = json.load(f)
                    self.user_agent = session_data.get('user_agent')
                    if self.user_agent:
                        self.session.headers.update({'User-Agent': self.user_agent})
                logger.info("âœ… ä¼šè¯ä¿¡æ¯åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ ä¼šè¯ä¿¡æ¯åŠ è½½å¤±è´¥: {e}")
                return False
            
            # 3. éªŒè¯ç™»å½•çŠ¶æ€å¹¶è·å–token
            return self._verify_login_status()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _verify_login_status(self) -> bool:
        """éªŒè¯ç™»å½•çŠ¶æ€å¹¶è·å–token"""
        try:
            logger.info("ğŸ” éªŒè¯ç™»å½•çŠ¶æ€...")
            
            # è®¿é—®å¾®ä¿¡å…¬ä¼—å¹³å°é¦–é¡µè·å–token
            home_url = "https://mp.weixin.qq.com/"
            
            for attempt in range(self.max_retries):
                try:
                    logger.info(f"ğŸŒ å°è¯•è¿æ¥å¾®ä¿¡å…¬ä¼—å¹³å° (ç¬¬{attempt+1}æ¬¡)...")
                    response = self.session.get(home_url, timeout=30)
                    
                    if response.status_code == 200:
                        # ä»å“åº”ä¸­æå–token
                        content = response.text
                        if 'token=' in content:
                            import re
                            token_match = re.search(r'token=(\d+)', content)
                            if token_match:
                                self.token = token_match.group(1)
                                logger.info(f"âœ… ç™»å½•çŠ¶æ€éªŒè¯æˆåŠŸï¼Œtoken: {self.token[:10]}...")
                                return True
                        
                        logger.warning("âš ï¸ æœªæ‰¾åˆ°tokenï¼Œå¯èƒ½éœ€è¦é‡æ–°ç™»å½•")
                        return False
                    else:
                        logger.warning(f"âš ï¸ è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    logger.warning(f"â° è¿æ¥è¶…æ—¶ (ç¬¬{attempt+1}æ¬¡)")
                except requests.exceptions.RequestException as e:
                    logger.warning(f"ğŸŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸ (ç¬¬{attempt+1}æ¬¡): {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info("â³ ç­‰å¾…5ç§’åé‡è¯•...")
                    time.sleep(5)
            
            logger.error("âŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è¿æ¥å¾®ä¿¡å…¬ä¼—å¹³å°")
            return False
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯ç™»å½•çŠ¶æ€å‡ºé”™: {e}")
            return False
    
    def _get_user_info(self):
        """è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        try:
            user_info_url = "https://mp.weixin.qq.com/cgi-bin/loginpage"
            params = {'token': self.token, 'lang': 'zh_CN'}
            
            response = self.session.get(user_info_url, params=params, timeout=15)
            if response.status_code == 200:
                logger.info("âœ… ç”¨æˆ·ä¿¡æ¯è·å–æˆåŠŸ")
                return response.json()
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        return None
    
    def _wait_for_rate_limit(self):
        """ç­‰å¾…é¿å…é¢‘ç‡é™åˆ¶"""
        time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´ä»¥é¿å…è¢«é™åˆ¶
    
    def search_account(self, keyword: str) -> List[Dict]:
        """æœç´¢å…¬ä¼—å·"""
        try:
            logger.info(f"ğŸ” æœç´¢å…¬ä¼—å·: {keyword}")
            
            if not self.token:
                logger.error("âŒ Tokenæœªè®¾ç½®ï¼Œæ— æ³•æœç´¢")
                return []
            
            search_url = "https://mp.weixin.qq.com/cgi-bin/searchbiz"
            params = {
                'action': 'search_biz',
                'token': self.token,
                'lang': 'zh_CN',
                'f': 'json',
                'ajax': 1,
                'random': str(time.time()),
                'query': keyword,
                'count': 10
            }
            
            for attempt in range(self.max_retries):
                try:
                    self._wait_for_rate_limit()
                    response = self.session.get(search_url, params=params, timeout=15)
                    
                    if response.status_code == 200:
                        result = response.json()
                        
                        if result.get('base_resp', {}).get('ret') == 0:
                            accounts = result.get('list', [])
                            logger.info(f"âœ… æ‰¾åˆ° {len(accounts)} ä¸ªå…¬ä¼—å·")
                            
                            for i, account in enumerate(accounts, 1):
                                nickname = account.get('nickname', 'æœªçŸ¥')
                                alias = account.get('alias', 'æ— åˆ«å')
                                logger.info(f"  {i}. {nickname} ({alias})")
                            
                            return accounts
                        else:
                            error_msg = result.get('base_resp', {}).get('err_msg', 'æœªçŸ¥é”™è¯¯')
                            logger.error(f"âŒ æœç´¢APIè¿”å›é”™è¯¯: {error_msg}")
                    else:
                        logger.warning(f"âš ï¸ æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        
                except requests.exceptions.Timeout:
                    logger.warning(f"â° æœç´¢è¯·æ±‚è¶…æ—¶ (ç¬¬{attempt+1}æ¬¡)")
                except Exception as e:
                    logger.warning(f"ğŸŒ æœç´¢è¯·æ±‚å¼‚å¸¸ (ç¬¬{attempt+1}æ¬¡): {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info("â³ ç­‰å¾…3ç§’åé‡è¯•...")
                    time.sleep(3)
            
            logger.error("âŒ å¤šæ¬¡å°è¯•åæœç´¢ä»ç„¶å¤±è´¥")
            return []
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å…¬ä¼—å·å¤±è´¥: {e}")
            return []
    
    def collect_articles(self, account: Dict, start_date: datetime, end_date: datetime, max_articles: int = 50) -> List[Dict]:
        """æ”¶é›†æŒ‡å®šå…¬ä¼—å·çš„æ–‡ç« 
        
        Args:
            account: å…¬ä¼—å·ä¿¡æ¯
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            max_articles: æœ€å¤§æ–‡ç« æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            fakeid = account.get('fakeid', '')
            if not fakeid:
                logger.error("âŒ æ— æ³•è·å–å…¬ä¼—å·ID")
                return []
            
            account_name = account.get('nickname', 'æœªçŸ¥å…¬ä¼—å·')
            logger.info(f"ğŸ“¥ å¼€å§‹æ”¶é›†æ–‡ç« : {account_name}")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            logger.info(f"ğŸ“Š æœ€å¤§æ•°é‡: {max_articles}")
            
            articles = []
            begin = 0
            page_size = 20
            
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆæ³¨æ„ï¼šend_date è¦åŒ…å«æ•´å¤©ï¼Œæ‰€ä»¥åŠ 1å¤©å†å‡1ç§’ï¼‰
            start_timestamp = int(start_date.timestamp())
            end_timestamp = int((end_date + timedelta(days=1) - timedelta(seconds=1)).timestamp())
            
            logger.info(f"â° æ—¶é—´æˆ³èŒƒå›´: {start_timestamp} - {end_timestamp}")
            
            # ç”¨äºæ”¶é›†ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–‡ç« 
            filtered_articles = []
            
            while len(filtered_articles) < max_articles:
                self._wait_for_rate_limit()
                
                logger.info(f"ğŸ“„ è·å–ç¬¬ {begin//page_size + 1} é¡µï¼Œå·²æ”¶é›† {len(filtered_articles)} ç¯‡")
                
                articles_url = "https://mp.weixin.qq.com/cgi-bin/appmsgpublish"
                params = {
                    'sub': 'list',
                    'search_field': 'null',
                    'begin': begin,
                    'count': page_size,
                    'query': '',
                    'fakeid': fakeid,
                    'type': '101_1',  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„typeå‚æ•°
                    'free_publish_type': 1,
                    'sub_action': 'list_ex',
                    'token': self.token,
                    'lang': 'zh_CN',
                    'f': 'json',
                    'ajax': 1
                }
                
                success = False
                for attempt in range(self.max_retries):
                    try:
                        response = self.session.get(articles_url, params=params, timeout=15)
                        
                        if response.status_code == 200:
                            result = response.json()
                            
                            if result.get('base_resp', {}).get('ret') == 0:
                                success = True
                                break
                            else:
                                error_msg = result.get('base_resp', {}).get('err_msg', 'æœªçŸ¥é”™è¯¯')
                                logger.warning(f"âš ï¸ APIè¿”å›é”™è¯¯ (ç¬¬{attempt+1}æ¬¡): {error_msg}")
                        else:
                            logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {response.status_code}")
                            
                    except requests.exceptions.Timeout:
                        logger.warning(f"â° è¯·æ±‚è¶…æ—¶ (ç¬¬{attempt+1}æ¬¡)")
                    except Exception as e:
                        logger.warning(f"ğŸŒ è¯·æ±‚å¼‚å¸¸ (ç¬¬{attempt+1}æ¬¡): {e}")
                    
                    if attempt < self.max_retries - 1:
                        time.sleep(3)
                
                if not success:
                    logger.error("âŒ å¤šæ¬¡å°è¯•åAPIè¯·æ±‚ä»ç„¶å¤±è´¥")
                    break
                
                # è§£ææ–‡ç« 
                page_articles = self._parse_articles_from_response(result)
                logger.info(f"ğŸ“‹ å½“å‰é¡µè§£æåˆ° {len(page_articles)} ç¯‡æ–‡ç« ")
                
                if not page_articles:
                    logger.info("ğŸ“„ æ²¡æœ‰æ›´å¤šæ–‡ç« ")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡ç« éƒ½æ—©äºèµ·å§‹æ—¶é—´ï¼ˆæŒ‰æ—¶é—´å€’åºï¼Œå¦‚æœç¬¬ä¸€ç¯‡éƒ½æ—©äºèµ·å§‹æ—¶é—´ï¼Œåˆ™å¯ä»¥åœæ­¢ï¼‰
                first_article_time = page_articles[0].get('update_time', 0) if page_articles else 0
                if first_article_time < start_timestamp:
                    logger.info(f"âœ… å·²è¾¾åˆ°èµ·å§‹æ—¥æœŸé™åˆ¶ ({start_date.strftime('%Y-%m-%d')})")
                    break
                
                # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ–‡ç« 
                for article in page_articles:
                    article_time = article.get('update_time', 0)  # ä¿®å¤ï¼šä½¿ç”¨update_timeå­—æ®µ
                    article_date_str = datetime.fromtimestamp(article_time).strftime('%Y-%m-%d %H:%M:%S')
                    
                    # å¦‚æœæ–‡ç« æ—¶é—´æ—©äºèµ·å§‹æ—¶é—´ï¼Œè·³è¿‡è¿™ç¯‡æ–‡ç« 
                    if article_time < start_timestamp:
                        logger.info(f"ğŸ“… è·³è¿‡æ—©äºèµ·å§‹æ—¥æœŸçš„æ–‡ç« : {article.get('title', '')[:30]}... ({article_date_str})")
                        continue
                    
                    # å¦‚æœæ–‡ç« æ—¶é—´æ™šäºç»“æŸæ—¶é—´ï¼Œè·³è¿‡è¿™ç¯‡æ–‡ç« 
                    if article_time > end_timestamp:
                        logger.info(f"ğŸ“… è·³è¿‡æ™šäºç»“æŸæ—¥æœŸçš„æ–‡ç« : {article.get('title', '')[:30]}... ({article_date_str})")
                        continue
                    
                    # æ–‡ç« åœ¨æ—¶é—´èŒƒå›´å†…ï¼Œæ·»åŠ åˆ°è¿‡æ»¤åˆ—è¡¨
                    logger.info(f"âœ… ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–‡ç« : {article.get('title', '')[:30]}... ({article_date_str})")
                    filtered_articles.append(article)
                    
                    # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢æ”¶é›†
                    if len(filtered_articles) >= max_articles:
                        break
                
                # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢æ”¶é›†
                if len(filtered_articles) >= max_articles:
                    logger.info(f"âœ… å·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶: {max_articles}")
                    break
                
                begin += page_size
                
                # é¿å…æ— é™å¾ªç¯
                if begin > 1000:
                    logger.warning("âš ï¸ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶")
                    break
            
            # æŒ‰æ—¶é—´å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
            filtered_articles.sort(key=lambda x: x.get('update_time', 0), reverse=True)
            
            # åº”ç”¨æ•°é‡é™åˆ¶
            result_articles = filtered_articles[:max_articles]
            
            logger.info(f"âœ… æ”¶é›†å®Œæˆï¼Œå…± {len(result_articles)} ç¯‡æ–‡ç« ")
            return result_articles
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æ–‡ç« å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def _parse_articles_from_response(self, result: Dict) -> List[Dict]:
        """ä»APIå“åº”ä¸­è§£ææ–‡ç« åˆ—è¡¨"""
        articles = []
        
        try:
            publish_page_str = result.get('publish_page', '')
            if not publish_page_str:
                logger.warning("âš ï¸ å“åº”ä¸­æ²¡æœ‰publish_pageæ•°æ®")
                return articles
            
            publish_page = json.loads(publish_page_str)
            publish_list = publish_page.get('publish_list', [])
            logger.info(f"ğŸ“‹ å‘å¸ƒåˆ—è¡¨é•¿åº¦: {len(publish_list)}")
            
            for publish_item in publish_list:
                publish_info_str = publish_item.get('publish_info', '')
                if not publish_info_str:
                    continue
                
                publish_info = json.loads(publish_info_str)
                appmsgex_list = publish_info.get('appmsgex', [])
                
                for appmsg in appmsgex_list:
                    link = appmsg.get('link', '').replace('\\/', '/')
                    
                    article = {
                        'title': appmsg.get('title', ''),
                        'url': link,  # ä½¿ç”¨urlå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
                        'link': link,  # ä¿ç•™åŸå­—æ®µå
                        'author_name': appmsg.get('author_name', ''),
                        'digest': appmsg.get('digest', ''),
                        'update_time': appmsg.get('update_time', 0),  # ä¿®å¤ï¼šä½¿ç”¨update_timeå­—æ®µ
                        'create_time': appmsg.get('create_time', 0),
                        'publish_time': datetime.fromtimestamp(appmsg.get('update_time', 0)).strftime('%Y-%m-%d %H:%M:%S') if appmsg.get('update_time') else ''  # ä¿®å¤ï¼šä½¿ç”¨update_timeå­—æ®µ
                    }
                    articles.append(article)
            
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« åˆ—è¡¨å‡ºé”™: {e}")
        
        return articles
    
    def auto_collect_articles(self, account_name: str, days_back: int = 7, max_articles: int = 50) -> List[Dict]:
        """è‡ªåŠ¨æ”¶é›†æŒ‡å®šå…¬ä¼—å·çš„æ–‡ç« 
        
        Args:
            account_name: å…¬ä¼—å·åç§°
            days_back: å‘å‰æ”¶é›†å¤šå°‘å¤©çš„æ–‡ç« 
            max_articles: æœ€å¤§æ–‡ç« æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹è‡ªåŠ¨æ”¶é›†: {account_name} (æœ€è¿‘{days_back}å¤©)")
            
            # 1. åŠ è½½ç™»å½•çŠ¶æ€
            if not self.load_session():
                logger.error("âŒ åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥")
                return []
            
            # 2. æœç´¢å…¬ä¼—å·
            accounts = self.search_account(account_name)
            if not accounts:
                logger.error(f"âŒ æœªæ‰¾åˆ°å…¬ä¼—å·: {account_name}")
                return []
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…¬ä¼—å·
            target_account = accounts[0]
            logger.info(f"âœ… é€‰æ‹©å…¬ä¼—å·: {target_account.get('nickname', '')} ({target_account.get('alias', '')})")
            
            # 3. è®¡ç®—æ—¶é—´èŒƒå›´
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            # 4. æ”¶é›†æ–‡ç« 
            articles = self.collect_articles(target_account, start_date, end_date, max_articles)
            
            logger.info(f"ğŸ‰ è‡ªåŠ¨æ”¶é›†å®Œæˆ: {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ”¶é›†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []


def main():
    """æµ‹è¯•å‡½æ•°"""
    collector = HeadlessWeChatCollector()
    
    # æµ‹è¯•è‡ªåŠ¨æ”¶é›†
    articles = collector.auto_collect_articles('ä»™å¢ƒä¼ è¯´ROæ–°å¯èˆª', days_back=7, max_articles=20)
    
    if articles:
        print(f"\nâœ… æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« :")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title']}")
            print(f"   URL: {article['url']}")
            print(f"   æ—¶é—´: {article['publish_time']}")
            print()
    else:
        print("âŒ æœªæ”¶é›†åˆ°æ–‡ç« ")


if __name__ == "__main__":
    main() 
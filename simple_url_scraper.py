#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple WeChat URL Scraper
ç®€å•å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·

ç›´æ¥å¤„ç†ç”¨æˆ·æä¾›çš„å¾®ä¿¡æ–‡ç« URLï¼Œè·å–å†…å®¹å¹¶ä¿å­˜ä¸ºé£ä¹¦æ ¼å¼
"""

import time
import random
import json
import re
import base64
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement, qn

from typing import List, Dict, Any, Optional
from loguru import logger
from datetime import datetime
import os

from utils import clean_text




class SimpleUrlScraper:
    """ç®€å•URLå¤„ç†å·¥å…·"""
    
    def __init__(self, headless: bool = False):
        """
        åˆå§‹åŒ–å·¥å…· - å¿«é€Ÿæ¨¡å¼ï¼Œä¸ç«‹å³å¯åŠ¨æµè§ˆå™¨
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        self.driver = None
        self.headless = headless
        
        logger.info("ç®€å•URLå¤„ç†å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆæµè§ˆå™¨å°†æ‡’åŠ è½½ï¼‰")
    
    def setup_browser(self) -> bool:
        """
        è®¾ç½®æµè§ˆå™¨ - é«˜æ€§èƒ½ä¼˜åŒ–å¯åŠ¨
        """
        try:
            logger.info("ğŸš€ æ­£åœ¨å¿«é€Ÿåˆå§‹åŒ–æµè§ˆå™¨...")
            
            # Chromeé€‰é¡¹ - æé€Ÿä¼˜åŒ–æ¨¡å¼
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # ğŸ†• æé€Ÿå¯åŠ¨ä¼˜åŒ–å‚æ•°
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-background-timer-throttling')
            chrome_options.add_argument('--disable-backgrounding-occluded-windows')
            chrome_options.add_argument('--disable-renderer-backgrounding')
            chrome_options.add_argument('--disable-features=TranslateUI')
            chrome_options.add_argument('--disable-default-apps')
            
            # ğŸ†• æ–°å¢æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
            chrome_options.add_argument('--disable-gpu')  # ç¦ç”¨GPUåŠ é€Ÿï¼Œå‡å°‘å¯åŠ¨æ—¶é—´
            chrome_options.add_argument('--disable-software-rasterizer')
            chrome_options.add_argument('--disable-background-networking')
            chrome_options.add_argument('--disable-sync')  # ç¦ç”¨åŒæ­¥
            chrome_options.add_argument('--disable-translate')
            chrome_options.add_argument('--disable-ipc-flooding-protection')
            chrome_options.add_argument('--disable-hang-monitor')
            chrome_options.add_argument('--disable-prompt-on-repost')
            chrome_options.add_argument('--disable-domain-reliability')
            chrome_options.add_argument('--disable-component-update')
            chrome_options.add_argument('--disable-background-downloads')
            chrome_options.add_argument('--disable-add-to-shelf')
            
            # å†…å­˜å’Œç½‘ç»œä¼˜åŒ–
            chrome_options.add_argument('--memory-pressure-off')
            chrome_options.add_argument('--disable-client-side-phishing-detection')
            chrome_options.add_argument('--disable-component-extensions-with-background-pages')
            chrome_options.add_argument('--disable-permissions-api')
            chrome_options.add_argument('--disable-notifications')
            
            # ç”¨æˆ·ä»£ç†
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # çª—å£å¤§å°
            chrome_options.add_argument('--window-size=1920,1080')
            
            # ğŸ†• å¯åŠ¨æ—¶é—´ä¼˜åŒ–
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            
            # ä½¿ç”¨ä¿®å¤åçš„Chromeé…ç½®
            chrome_options.binary_location = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            
            # æŸ¥æ‰¾æ­£ç¡®çš„ChromeDriverè·¯å¾„
            import glob
            import os
            driver_path = None
            
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                base_path = ChromeDriverManager().install()
                base_dir = os.path.dirname(base_path)
                
                possible_paths = [
                    os.path.join(base_dir, "chromedriver.exe"),
                    os.path.join(base_dir, "chromedriver-win32", "chromedriver.exe"),
                    base_path if base_path.endswith(".exe") else None
                ]
                
                for path in possible_paths:
                    if path and os.path.exists(path) and path.endswith(".exe"):
                        driver_path = path
                        break
                
                if not driver_path:
                    search_patterns = [
                        os.path.join(os.path.dirname(base_dir), "**", "chromedriver.exe"),
                        os.path.join(base_dir, "**", "chromedriver.exe")
                    ]
                    
                    for pattern in search_patterns:
                        matches = glob.glob(pattern, recursive=True)
                        if matches:
                            driver_path = matches[0]
                            break
            except:
                pass
            
            if not driver_path:
                from webdriver_manager.chrome import ChromeDriverManager
                driver_path = ChromeDriverManager().install()
            
            # åˆ›å»ºWebDriver
            logger.info("âš¡ å¯åŠ¨Chromeæµè§ˆå™¨...")
            service = Service(driver_path)
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(6)  # è¿›ä¸€æ­¥å‡å°‘è¶…æ—¶æ—¶é—´
            self.driver.implicitly_wait(1)  # è¿›ä¸€æ­¥å‡å°‘ç­‰å¾…æ—¶é—´
            
            logger.success("âœ… æµè§ˆå™¨å¿«é€Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def extract_article_info(self, url: str) -> dict:
        """
        å¿«é€Ÿæå–æ–‡ç« ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        try:
            if not self.driver:
                if not self.setup_browser():
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}
            
            logger.info(f"æ­£åœ¨è®¿é—®URL: {url}")
            
            # å¿«é€Ÿè®¿é—®é¡µé¢
            self.driver.get(url)
            
            # å¿«é€Ÿç­‰å¾…ï¼ˆæœ€å¤š2ç§’ï¼‰
            try:
                wait = WebDriverWait(self.driver, 2)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­æå–")
            
            # å¿«é€Ÿæå–æ–‡ç« ä¿¡æ¯
            article_info = self._extract_article_content()
            
            if article_info and 'error' not in article_info:
                return article_info
            else:
                return {"error": "æ–‡ç« ä¿¡æ¯æå–å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def save_as_pdf(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºPDF - å›¾ç‰‡åŠ è½½ä¼˜åŒ–ç‰ˆæœ¬
        ç¡®ä¿æ‰€æœ‰å›¾ç‰‡å®Œå…¨åŠ è½½åå†ç”ŸæˆPDF
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜PDF: {url}")
            
            if not self.driver:
                if not self.setup_browser():
                    return False
            
            # 1. è®¿é—®é¡µé¢
            start_time = time.time()
            self.driver.get(url)
            
            # 2. ç­‰å¾…åŸºæœ¬å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # 3. äººç±»å¼æ»šåŠ¨åŠ è½½ï¼Œç¡®ä¿å›¾ç‰‡å®Œå…¨åŠ è½½
            self._human_like_scroll_and_load()
            
            # 4. å¿«é€Ÿç”ŸæˆPDFï¼ˆçº¦0.5ç§’ï¼‰
            # æ»šåŠ¨å›é¡¶éƒ¨å‡†å¤‡ç”ŸæˆPDF
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(0.2)
            
            # æ³¨å…¥CSSæ ·å¼æ¥æ¶ˆé™¤é¡µé¢è¾¹è·å’Œä¼˜åŒ–æ’ç‰ˆ
            css_style = """
            <style>
                @page {
                    margin: 0 !important;
                    padding: 0 !important;
                    size: A4 !important;
                }
                
                * {
                    box-sizing: border-box !important;
                }
                
                body, html {
                    margin: 0 !important;
                    padding: 0 !important;
                    width: 100% !important;
                    max-width: 100% !important;
                }
                
                /* å¾®ä¿¡æ–‡ç« å†…å®¹åŒºåŸŸä¼˜åŒ– */
                #js_content,
                .rich_media_content,
                .rich_media_area_primary {
                    margin: 0 !important;
                    padding: 5px !important;
                    width: 100% !important;
                    max-width: 100% !important;
                }
                
                /* éšè—ä¸éœ€è¦çš„å…ƒç´  */
                .rich_media_meta_list,
                .rich_media_tool,
                .qr_code_pc_outer,
                .reward_qrcode_area,
                .reward_area,
                #js_pc_qr_code_img,
                .function_mod,
                .profile_container,
                .rich_media_global_msg {
                    display: none !important;
                }
            </style>
            """
            
            # å°†CSSæ ·å¼æ³¨å…¥é¡µé¢
            self.driver.execute_script(f"""
                var style = document.createElement('style');
                style.type = 'text/css';
                style.innerHTML = `{css_style}`;
                document.head.appendChild(style);
            """)
            
            # ç­‰å¾…æ ·å¼ç”Ÿæ•ˆ
            time.sleep(0.3)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¼˜åŒ–çš„PDFé€‰é¡¹ - å¡«æ»¡é¡µé¢ï¼Œæ¶ˆé™¤ç™½è¾¹
            pdf_options = {
                'paperFormat': 'A4',
                'printBackground': True,
                'marginTop': 0,        # å®Œå…¨æ¶ˆé™¤ä¸Šè¾¹è·
                'marginBottom': 0,     # å®Œå…¨æ¶ˆé™¤ä¸‹è¾¹è·  
                'marginLeft': 0,       # å®Œå…¨æ¶ˆé™¤å·¦è¾¹è·
                'marginRight': 0,      # å®Œå…¨æ¶ˆé™¤å³è¾¹è·
                'preferCSSPageSize': True,  # å¯ç”¨CSSé¡µé¢å¤§å°è®¾ç½®
                'displayHeaderFooter': False,
                # è°ƒæ•´ç¼©æ”¾ä»¥æ›´å¥½åœ°å¡«æ»¡é¡µé¢
                'scale': 1.0,  # ä½¿ç”¨100%ç¼©æ”¾ï¼Œé…åˆCSSæ ·å¼
                'landscape': False,
                # æ–°å¢ï¼šä¼˜åŒ–é¡µé¢åˆ©ç”¨ç‡
                'transferMode': 'ReturnAsBase64',
                'generateTaggedPDF': False  # ç®€åŒ–PDFç»“æ„
            }
            
            # ç”ŸæˆPDF
            pdf_data = self.driver.execute_cdp_cmd('Page.printToPDF', pdf_options)
            
            # ä¿å­˜PDFæ–‡ä»¶
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(pdf_data['data']))
            
            total_time = time.time() - start_time
            logger.success(f"PDFä¿å­˜æˆåŠŸ: {output_path}ï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’")
            
            # æç¤ºä¼˜åŒ–åçš„å¤„ç†æ—¶é—´
            if total_time <= 30:
                logger.success("âœ… PDFç”Ÿæˆå®Œæˆï¼Œå›¾ç‰‡åŠ è½½ä¼˜åŒ–ç”Ÿæ•ˆï¼")
            else:
                logger.info(f"â„¹ï¸ è€—æ—¶ {total_time:.1f}ç§’ - ä¸ºç¡®ä¿å›¾ç‰‡å®Œæ•´åŠ è½½è€Œå¢åŠ çš„æ—¶é—´")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜PDFå¤±è´¥: {e}")
            return False
    
    def save_as_docx(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºWordæ–‡æ¡£ - å®Œæ•´å†…å®¹ç‰ˆæœ¬
        ä½¿ç”¨SeleniumåŠ¨æ€åŠ è½½ + BeautifulSoupè§£æç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜Wordæ–‡æ¡£: {url}")
            
            # 1. ä½¿ç”¨SeleniumåŠ è½½å®Œæ•´é¡µé¢å†…å®¹ï¼ˆåŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # 2. åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼ï¼ˆå¾®è½¯é›…é»‘ï¼Œ10.5å·å­—ï¼‰
            try:
                from docx.enum.style import WD_STYLE_TYPE
                style = doc.styles.add_style('DefaultParagraph', WD_STYLE_TYPE.PARAGRAPH)
                font = style.font
                font.name = 'å¾®è½¯é›…é»‘'
                font.size = Pt(10.5)
            except:
                logger.debug("è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼å¤±è´¥ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")
            
            # 3. æ·»åŠ æ ‡é¢˜
            title = article_data.get('title', 'å¾®ä¿¡æ–‡ç« ')
            title_paragraph = doc.add_heading(title, level=1)
            title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 4. æ·»åŠ ä½œè€…å’Œå‘å¸ƒæ—¶é—´
            meta_info = f"ä½œè€…: {article_data.get('author', 'æœªçŸ¥')}\n"
            meta_info += f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_date', 'æœªçŸ¥')}\n"
            meta_info += f"åŸæ–‡é“¾æ¥: {url}\n"
            
            meta_paragraph = doc.add_paragraph(meta_info)
            meta_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 5. æ·»åŠ åˆ†éš”çº¿
            doc.add_paragraph("=" * 50).alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 6. å¤„ç†HTMLå†…å®¹ï¼Œç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½åŒ…å«
            content_soup = article_data.get('content_soup')
            if content_soup:
                self._process_wechat_content_to_docx(doc, content_soup, article_data.get('images', []))
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« å†…å®¹")
                doc.add_paragraph("æœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹")
            
            # 7. ä¿å­˜æ–‡æ¡£
            doc.save(output_path)
            
            logger.success(f"Wordæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
            logger.info(f"åŒ…å« {len(article_data.get('images', []))} å¼ å›¾ç‰‡")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜Wordæ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def _extract_wechat_article_with_selenium(self, url: str) -> dict:
        """
        ä½¿ç”¨Seleniumè·å–å®Œæ•´çš„å¾®ä¿¡æ–‡ç« å†…å®¹ï¼ˆåŒ…æ‹¬åŠ¨æ€åŠ è½½å†…å®¹ï¼‰
        """
        try:
            logger.info(f"ğŸš€ ä½¿ç”¨Seleniumè·å–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            if not self.driver:
                if not self.setup_browser():
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŸºç¡€åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 15)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                logger.debug("âœ… é¡µé¢åŸºç¡€æ¡†æ¶åŠ è½½å®Œæˆ")
            except TimeoutException:
                logger.warning("â° é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # å®Œæ•´æ»šåŠ¨åŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹
            self._human_like_scroll_and_load()
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿æ‰€æœ‰åŠ¨æ€å†…å®¹éƒ½å·²åŠ è½½
            logger.info("â³ ç­‰å¾…åŠ¨æ€å†…å®¹å®Œå…¨åŠ è½½...")
            time.sleep(3)
            
            # è·å–å®Œå…¨æ¸²æŸ“åçš„é¡µé¢HTML
            page_source = self.driver.page_source
            
            # ä½¿ç”¨BeautifulSoupè§£æå®Œæ•´çš„HTML
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è°ƒè¯•ï¼šä¿å­˜å®Œæ•´HTML
            debug_html_path = "debug_selenium_page.html"
            with open(debug_html_path, 'w', encoding='utf-8') as f:
                f.write(page_source)
            logger.debug(f"å®Œæ•´HTMLå·²ä¿å­˜åˆ°: {debug_html_path}")
            
            # æå–æ ‡é¢˜ - ä»æ¸²æŸ“åçš„é¡µé¢
            title = self._extract_title_from_selenium_soup(soup)
            
            # æå–ä½œè€…
            author = self._extract_author_from_selenium_soup(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date_from_selenium_soup(soup)
            
            # æå–æ–‡ç« æ­£æ–‡å†…å®¹ - ä»å®Œå…¨åŠ è½½çš„é¡µé¢
            content_soup = self._extract_content_from_selenium_soup(soup)
            
            if not content_soup:
                return {"error": "æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹"}
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆä½¿ç”¨Seleniumå·²ç»åŠ è½½çš„å›¾ç‰‡ï¼‰
            images = self._download_images_from_selenium_soup(content_soup, url)
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {title[:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ®µè½æ•°={len(content_soup.find_all(['p', 'div']))}, å›¾ç‰‡æ•°é‡={len(images)}")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'content_soup': content_soup,
                'images': images,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"Seleniumæå–æ–‡ç« å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def _extract_title_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ ‡é¢˜"""
        try:
            # å¾®ä¿¡æ–‡ç« æ ‡é¢˜çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            title_selectors = [
                'h1#activity-name',
                'h2.rich_media_title',
                '.rich_media_title',
                'h1.rich_media_title',
                '.rich_media_title h1',
                '#activity-name',
                '.wx_title',
                '[data-role="title"]',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    if len(title) > 3:  # ç¡®ä¿ä¸æ˜¯ç©ºæ ‡é¢˜
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                        return title
            
            # ä»é¡µé¢æ ‡é¢˜æå–
            title_tag = soup.find('title')
            if title_tag and title_tag.get_text(strip=True):
                title = title_tag.get_text(strip=True)
                if 'å¾®ä¿¡å…¬ä¼—å¹³å°' not in title and len(title) > 3:
                    logger.debug(f"ä»titleæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # ä»metaæ ‡ç­¾æå–
            meta_title = soup.find('meta', property='og:title')
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content').strip()
                if title and len(title) > 3:
                    logger.debug(f"ä»metaæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            return "æœªçŸ¥æ ‡é¢˜"
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_author_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–ä½œè€…"""
        try:
            # å¾®ä¿¡æ–‡ç« ä½œè€…çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '[data-role="nickname"]',
                '.account_nickname',
                '.wx_author',
                '#profileBt .nickname'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem and author_elem.get_text(strip=True):
                    author = author_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…å¤±è´¥: {e}")
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '[data-role="publish-time"]',
                '.wx_time',
                '.time'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem and date_elem.get_text(strip=True):
                    date_text = date_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return "æœªçŸ¥æ—¶é—´"
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_content_from_selenium_soup(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ–‡ç« æ­£æ–‡åŒºåŸŸ"""
        try:
            # å¾®ä¿¡æ–‡ç« æ­£æ–‡çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '.appmsg_content_text',
                '[data-role="article-content"]',
                '.rich_media_area_primary',
                '.rich_media_wrp',
                '#js_content_container'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦è¶³å¤Ÿå¤š
                    text_content = content_elem.get_text(strip=True)
                    if text_content and len(text_content) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                        logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                        return content_elem
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å®¹å™¨ï¼Œæ™ºèƒ½æå–
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨æ­£æ–‡å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–")
            
            # æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬å†…å®¹çš„div
            all_divs = soup.find_all('div')
            best_div = None
            max_text_length = 0
            
            for div in all_divs:
                text = div.get_text(strip=True)
                if len(text) > max_text_length and len(text) > 200:  # è‡³å°‘200ä¸ªå­—ç¬¦
                    # æ’é™¤å¯¼èˆªã€å¤´éƒ¨ã€è„šéƒ¨ç­‰åŒºåŸŸ
                    div_class = ' '.join(div.get('class', []))
                    div_id = div.get('id', '')
                    
                    exclude_keywords = ['nav', 'header', 'footer', 'sidebar', 'menu', 'toolbar', 'ad']
                    if not any(keyword in div_class.lower() + div_id.lower() for keyword in exclude_keywords):
                        max_text_length = len(text)
                        best_div = div
            
            if best_div:
                logger.debug(f"æ™ºèƒ½æå–æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ–‡æœ¬é•¿åº¦: {max_text_length}")
                return best_div
            
            logger.warning("ä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            return soup.find('body') or soup
            
        except Exception as e:
            logger.error(f"æå–æ­£æ–‡å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _download_images_from_selenium_soup(self, content_soup: BeautifulSoup, base_url: str) -> list:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­ä¸‹è½½å›¾ç‰‡"""
        try:
            images_info = []
            img_tags = content_soup.find_all('img')
            
            if not img_tags:
                logger.info("ğŸ“· æœªå‘ç°å›¾ç‰‡")
                return images_info
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_tags)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            
            # åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•
            import urllib.parse
            parsed_url = urllib.parse.urlparse(base_url)
            safe_domain = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.netloc)
            img_dir = os.path.join("output", "images", safe_domain)
            os.makedirs(img_dir, exist_ok=True)
            
            for i, img in enumerate(img_tags):
                try:
                    # ä»SeleniumåŠ è½½çš„é¡µé¢ä¸­ï¼Œsrcåº”è¯¥å·²ç»è¢«å®Œå…¨è§£æ
                    img_src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if not img_src:
                        continue
                    
                    # å¤„ç†å®Œæ•´çš„URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = f"https://{parsed_url.netloc}" + img_src
                    elif not img_src.startswith('http'):
                        img_src = urllib.parse.urljoin(base_url, img_src)
                    
                    # è·³è¿‡è¿‡å°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å›¾æ ‡ï¼‰
                    if 'icon' in img_src.lower() or 'logo' in img_src.lower():
                        continue
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"img_{i+1:03d}.jpg"
                    img_path = os.path.join(img_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image_requests(img_src, img_path):
                        images_info.append({
                            'url': img_src,
                            'local_path': img_path,
                            'filename': img_filename
                        })
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_filename}")
                    else:
                        images_info.append({
                            'url': img_src,
                            'local_path': None,
                            'filename': img_filename
                        })
                        logger.warning(f"ğŸ“· ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len([img for img in images_info if img['local_path']])}/{len(img_tags)}")
            return images_info
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
            return []

    def _extract_wechat_article_by_requests(self, url: str) -> dict:
        """
        ä½¿ç”¨Requestsè·å–å¾®ä¿¡æ–‡ç« å†…å®¹
        """
        try:
            logger.info(f"ğŸŒ ä½¿ç”¨Requestsè·å–æ–‡ç« å†…å®¹: {url}")
            
            # è®¾ç½®è¯·æ±‚å¤´ç»•è¿‡åçˆ¬æœºåˆ¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è°ƒè¯•ï¼šä¿å­˜åŸå§‹HTMLåˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
            debug_html_path = "debug_page.html"
            with open(debug_html_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            logger.debug(f"åŸå§‹HTMLå·²ä¿å­˜åˆ°: {debug_html_path}")
            
            # æå–æ ‡é¢˜
            title = self._extract_title_from_soup(soup)
            
            # æå–ä½œè€…
            author = self._extract_author_from_soup(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date_from_soup(soup)
            
            # æå–æ–‡ç« æ­£æ–‡å†…å®¹
            content_soup = self._extract_content_from_soup(soup)
            
            if not content_soup:
                return {"error": "æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹"}
            
            # ä¸‹è½½å›¾ç‰‡
            images = self._download_images_from_soup(content_soup, url)
            
            logger.success(f"âœ… æˆåŠŸæå–æ–‡ç« å†…å®¹: {title[:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ®µè½æ•°={len(content_soup.find_all(['p', 'div']))}, å›¾ç‰‡æ•°é‡={len(images)}")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'content_soup': content_soup,
                'images': images,
                'url': url
            }
            
        except requests.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"è§£æHTMLå¤±è´¥: {e}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}
    
    def _extract_title_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–æ ‡é¢˜"""
        try:
            # å¾®ä¿¡æ–‡ç« æ ‡é¢˜çš„å¸¸è§ä½ç½®
            title_selectors = [
                'h1#activity-name',
                'h2.rich_media_title',
                '.rich_media_title',
                'h1.rich_media_title',
                '.rich_media_title h1',
                '#activity-name',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä»é¡µé¢æºç ä¸­æœç´¢
            page_text = soup.get_text()
            import re
            # å°è¯•ä»metaä¿¡æ¯ä¸­æ‰¾æ ‡é¢˜
            meta_title = soup.find('meta', property='og:title')
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content').strip()
                if title:
                    logger.debug(f"ä»metaæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            return "æœªçŸ¥æ ‡é¢˜"
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_author_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–ä½œè€…"""
        try:
            # å¾®ä¿¡æ–‡ç« ä½œè€…çš„å¸¸è§ä½ç½®
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '[data-role="nickname"]',
                '.account_nickname'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem and author_elem.get_text(strip=True):
                    author = author_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…å¤±è´¥: {e}")
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„å¸¸è§ä½ç½®
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '[data-role="publish-time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem and date_elem.get_text(strip=True):
                    date_text = date_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            # å°è¯•ä»è„šæœ¬ä¸­æå–
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'publish_time' in script.string:
                    import re
                    match = re.search(r'publish_time["\s]*[=:]["\s]*"([^"]+)"', script.string)
                    if match:
                        return match.group(1)
            
            return "æœªçŸ¥æ—¶é—´"
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_content_from_soup(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ä»soupä¸­æå–æ–‡ç« æ­£æ–‡åŒºåŸŸ"""
        try:
            # å¾®ä¿¡æ–‡ç« æ­£æ–‡çš„å¸¸è§å®¹å™¨
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '.appmsg_content_text',
                '[data-role="article-content"]',
                '.rich_media_area_primary',
                '.rich_media_wrp'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦è¶³å¤Ÿå¤š
                    text_content = content_elem.get_text(strip=True)
                    if text_content and len(text_content) > 50:  # è‡³å°‘50ä¸ªå­—ç¬¦
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                        return content_elem
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨æ­£æ–‡å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–")
            
            # æŸ¥æ‰¾åŒ…å«è¾ƒå¤šæ–‡æœ¬çš„div
            all_divs = soup.find_all('div')
            best_div = None
            max_text_length = 0
            
            for div in all_divs:
                text = div.get_text(strip=True)
                if len(text) > max_text_length and len(text) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                    # æ’é™¤ä¸€äº›ä¸ç›¸å…³çš„div
                    div_class = div.get('class', [])
                    div_id = div.get('id', '')
                    if not any(exclude in str(div_class) + div_id for exclude in ['nav', 'header', 'footer', 'sidebar', 'menu']):
                        max_text_length = len(text)
                        best_div = div
            
            if best_div:
                logger.debug(f"æ™ºèƒ½æå–æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ–‡æœ¬é•¿åº¦: {max_text_length}")
                return best_div
            
            logger.warning("ä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            return soup.find('body') or soup
            
        except Exception as e:
            logger.error(f"æå–æ­£æ–‡å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _download_images_from_soup(self, content_soup: BeautifulSoup, base_url: str) -> list:
        """ä»å†…å®¹ä¸­ä¸‹è½½å›¾ç‰‡"""
        try:
            images_info = []
            img_tags = content_soup.find_all('img')
            
            if not img_tags:
                logger.info("ğŸ“· æœªå‘ç°å›¾ç‰‡")
                return images_info
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_tags)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            
            # åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•
            import urllib.parse
            parsed_url = urllib.parse.urlparse(base_url)
            safe_domain = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.netloc)
            img_dir = os.path.join("output", "images", safe_domain)
            os.makedirs(img_dir, exist_ok=True)
            
            for i, img in enumerate(img_tags):
                try:
                    img_src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if not img_src:
                        continue
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = f"https://{parsed_url.netloc}" + img_src
                    elif not img_src.startswith('http'):
                        img_src = urllib.parse.urljoin(base_url, img_src)
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"img_{i+1:03d}.jpg"
                    img_path = os.path.join(img_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image_requests(img_src, img_path):
                        images_info.append({
                            'url': img_src,
                            'local_path': img_path,
                            'filename': img_filename
                        })
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_filename}")
                    else:
                        images_info.append({
                            'url': img_src,
                            'local_path': None,
                            'filename': img_filename
                        })
                        logger.warning(f"ğŸ“· ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len([img for img in images_info if img['local_path']])}/{len(img_tags)}")
            return images_info
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
            return []
    
    def _download_image_requests(self, img_url: str, save_path: str) -> bool:
        """ä½¿ç”¨requestsä¸‹è½½å›¾ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/'
            }
            
            response = requests.get(img_url, headers=headers, timeout=15, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False
    
    def _process_wechat_content_to_docx(self, doc, content_soup: BeautifulSoup, images: list):
        """
        å°†å¾®ä¿¡æ–‡ç« å†…å®¹è½¬æ¢ä¸ºWordæ–‡æ¡£ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            # é€’å½’å¤„ç†æ‰€æœ‰å†…å®¹å…ƒç´ 
            self._process_element_to_docx_recursive(doc, content_soup, images)
                    
        except Exception as e:
            logger.warning(f"å†…å®¹è½¬æ¢å¼‚å¸¸: {e}")
            # é™çº§åˆ°çº¯æ–‡æœ¬å¤„ç†
            try:
                text = content_soup.get_text()
                if text.strip():
                    doc.add_paragraph(text.strip())
            except:
                doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
    
    def _process_element_to_docx_recursive(self, doc, element, images: list):
        """
        é€’å½’å¤„ç†HTMLå…ƒç´ åˆ°Wordæ–‡æ¡£
        """
        try:
            if hasattr(element, 'name'):
                tag_name = element.name.lower() if element.name else None
                
                # å¤„ç†ä¸åŒç±»å‹çš„å…ƒç´ 
                if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    # æ ‡é¢˜å¤„ç†
                    level = int(tag_name[1])
                    text = element.get_text(strip=True)
                    if text:
                        doc.add_heading(text, level=min(level, 3))
                        
                elif tag_name == 'p':
                    # æ®µè½å¤„ç†
                    self._process_paragraph_to_docx(doc, element, images)
                    
                elif tag_name == 'div':
                    # divå®¹å™¨ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                    text = element.get_text(strip=True)
                    if text and len(text) < 200:  # çŸ­æ–‡æœ¬ç›´æ¥ä½œä¸ºæ®µè½
                        paragraph = doc.add_paragraph()
                        self._add_formatted_text_to_paragraph(paragraph, element, images)
                    else:
                        # é•¿å†…å®¹ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                        for child in element.children:
                            self._process_element_to_docx_recursive(doc, child, images)
                    
                elif tag_name == 'img':
                    # å›¾ç‰‡å¤„ç†
                    self._add_image_to_docx_new(doc, element, images)
                    
                elif tag_name == 'blockquote':
                    # å¼•ç”¨å¤„ç†
                    text = element.get_text(strip=True)
                    if text:
                        paragraph = doc.add_paragraph(text)
                        # åº”ç”¨æ–œä½“æ ·å¼
                        for run in paragraph.runs:
                            run.italic = True
                            
                elif tag_name in ['ul', 'ol']:
                    # åˆ—è¡¨å¤„ç†
                    items = element.find_all('li')
                    for item in items:
                        text = item.get_text(strip=True)
                        if text:
                            doc.add_paragraph(f"â€¢ {text}", style='List Bullet')
                            
                elif tag_name == 'br':
                    # æ¢è¡Œ
                    doc.add_paragraph("")
                    
                elif tag_name in ['span', 'strong', 'b', 'em', 'i']:
                    # å†…è”å…ƒç´ ï¼Œåœ¨çˆ¶çº§å¤„ç†
                    pass
                    
                else:
                    # å…¶ä»–å…ƒç´ ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                    if hasattr(element, 'children'):
                        for child in element.children:
                            self._process_element_to_docx_recursive(doc, child, images)
            else:
                # æ–‡æœ¬èŠ‚ç‚¹
                if hasattr(element, 'strip'):
                    text_content = str(element).strip()
                    if text_content and len(text_content) > 0:
                        doc.add_paragraph(text_content)
                    
        except Exception as e:
            logger.debug(f"å¤„ç†å…ƒç´ å¼‚å¸¸: {e}")
    
    def _process_paragraph_to_docx(self, doc, p_element, images: list):
        """å¤„ç†æ®µè½å…ƒç´ """
        try:
            # æ£€æŸ¥æ®µè½æ˜¯å¦ä¸ºç©º
            text_content = p_element.get_text(strip=True)
            if not text_content:
                return
            
            # åˆ›å»ºæ®µè½
            paragraph = doc.add_paragraph()
            
            # æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬
            self._add_formatted_text_to_paragraph(paragraph, p_element, images)
            
        except Exception as e:
            logger.debug(f"å¤„ç†æ®µè½å¼‚å¸¸: {e}")
    
    def _add_formatted_text_to_paragraph(self, paragraph, element, images: list):
        """æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬åˆ°æ®µè½"""
        try:
            if hasattr(element, 'children'):
                for child in element.children:
                    if hasattr(child, 'name') and child.name:
                        tag_name = child.name.lower()
                        
                        if tag_name == 'img':
                            # å›¾ç‰‡æ’å…¥åˆ°æ®µè½ä¸­
                            try:
                                self._add_image_to_docx_new(paragraph._element.getparent().getparent(), child, images, True)
                            except:
                                pass
                                
                        elif tag_name in ['strong', 'b']:
                            # åŠ ç²—æ–‡æœ¬
                            text = child.get_text()
                            if text:
                                run = paragraph.add_run(text)
                                run.bold = True
                                
                        elif tag_name in ['em', 'i']:
                            # æ–œä½“æ–‡æœ¬
                            text = child.get_text()
                            if text:
                                run = paragraph.add_run(text)
                                run.italic = True
                                
                        elif tag_name == 'a':
                            # é“¾æ¥
                            text = child.get_text()
                            href = child.get('href', '')
                            if text:
                                run = paragraph.add_run(f"{text}({href})" if href else text)
                                
                        else:
                            # å…¶ä»–æ ‡ç­¾ï¼Œé€’å½’å¤„ç†
                            self._add_formatted_text_to_paragraph(paragraph, child, images)
                    else:
                        # æ–‡æœ¬èŠ‚ç‚¹
                        text = str(child).strip()
                        if text:
                            paragraph.add_run(text)
            else:
                # å¦‚æœæ²¡æœ‰childrenï¼Œç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
                text = element.get_text() if hasattr(element, 'get_text') else str(element)
                if text.strip():
                    paragraph.add_run(text.strip())
                            
        except Exception as e:
            logger.debug(f"æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬å¼‚å¸¸: {e}")
    
    def _add_image_to_docx_new(self, doc, img_element, images: list, inline: bool = False):
        """æ·»åŠ å›¾ç‰‡åˆ°Wordæ–‡æ¡£ï¼ˆæ–°ç‰ˆæœ¬ï¼‰"""
        try:
            img_src = img_element.get('src') or img_element.get('data-src') or img_element.get('data-original')
            if not img_src:
                return
            
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
            local_image = None
            for img_info in images:
                if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                    local_image = img_info
                    break
            
            if local_image and local_image.get('local_path') and os.path.exists(local_image['local_path']):
                try:
                    if inline:
                        # å†…è”å›¾ç‰‡ï¼Œæ·»åŠ åˆ°å½“å‰æ®µè½
                        doc.add_picture(local_image['local_path'], width=Inches(4))
                    else:
                        # å•ç‹¬æ®µè½å›¾ç‰‡
                        doc.add_picture(local_image['local_path'], width=Inches(5))
                    logger.debug(f"æ·»åŠ å›¾ç‰‡æˆåŠŸ: {local_image['local_path']}")
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡æ·»åŠ å¤±è´¥: {e}")
                    if not inline:
                        doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
            else:
                if not inline:
                    doc.add_paragraph(f"[å›¾ç‰‡é“¾æ¥: {img_src}]")
                
        except Exception as e:
            logger.debug(f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
    
    def _process_html_to_docx(self, doc, html_content: str, images: list):
        """
        å°†HTMLå†…å®¹è½¬æ¢ä¸ºWordæ–‡æ¡£ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é€’å½’å¤„ç†æ¯ä¸ªå…ƒç´ 
            for element in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'img', 'br']):
                self._process_element_to_docx(doc, element, images)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†å…ƒç´ ï¼Œå¤„ç†æ‰€æœ‰æ–‡æœ¬
            if not soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                all_text = soup.get_text(strip=True)
                if all_text:
                    doc.add_paragraph(all_text)
                    
        except Exception as e:
            logger.warning(f"HTMLè½¬æ¢å¼‚å¸¸: {e}")
            # é™çº§åˆ°çº¯æ–‡æœ¬å¤„ç†
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                text = soup.get_text()
                doc.add_paragraph(text)
            except:
                doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
    
    def _process_element_to_docx(self, doc, element, images: list):
        """
        å¤„ç†å•ä¸ªHTMLå…ƒç´ åˆ°Wordæ–‡æ¡£
        """
        try:
            tag_name = element.name.lower()
            
            # æ ‡é¢˜å¤„ç†
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                text = element.get_text(strip=True)
                if text:
                    doc.add_heading(text, level=min(level, 3))
            
            # æ®µè½å¤„ç†
            elif tag_name in ['p', 'div']:
                text = element.get_text(strip=True)
                if text:
                    paragraph = doc.add_paragraph(text)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                    imgs = element.find_all('img')
                    for img in imgs:
                        self._add_image_to_docx(doc, img, images)
            
            # åˆ—è¡¨å¤„ç†
            elif tag_name in ['ul', 'ol']:
                items = element.find_all('li')
                for item in items:
                    text = item.get_text(strip=True)
                    if text:
                        doc.add_paragraph(f"â€¢ {text}", style='List Bullet')
            
            # å›¾ç‰‡å¤„ç†
            elif tag_name == 'img':
                self._add_image_to_docx(doc, element, images)
            
            # æ¢è¡Œå¤„ç†
            elif tag_name == 'br':
                doc.add_paragraph("")
                
        except Exception as e:
            logger.debug(f"å…ƒç´ å¤„ç†å¼‚å¸¸: {e}")
    
    def _add_image_to_docx(self, doc, img_element, images: list):
        """
        æ·»åŠ å›¾ç‰‡åˆ°Wordæ–‡æ¡£
        """
        try:
            img_src = img_element.get('src') or img_element.get('data-src') or img_element.get('data-original')
            if not img_src:
                return
            
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
            local_image = None
            for img_info in images:
                if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                    local_image = img_info
                    break
            
            if local_image and local_image.get('local_path') and os.path.exists(local_image['local_path']):
                try:
                    doc.add_picture(local_image['local_path'], width=Inches(4))
                    logger.debug(f"æ·»åŠ å›¾ç‰‡æˆåŠŸ: {local_image['local_path']}")
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡æ·»åŠ å¤±è´¥: {e}")
                    doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
            else:
                doc.add_paragraph(f"[å›¾ç‰‡é“¾æ¥: {img_src}]")
                
        except Exception as e:
            logger.debug(f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
    
    def _wait_for_basic_page_load(self) -> bool:
        """
        ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        try:
            logger.info("ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½...")
            
            # å‡å°‘ç­‰å¾…æ—¶é—´åˆ°æœ€å¤š5ç§’
            wait = WebDriverWait(self.driver, 5)
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ åŠ è½½
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # ç­‰å¾…æ–‡ç« å†…å®¹åŠ è½½ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 
                    "#js_content, .rich_media_content, .article-content, .content")))
            except:
                logger.debug("æœªæ‰¾åˆ°å¸¸è§æ–‡ç« å†…å®¹å®¹å™¨ï¼Œä½¿ç”¨é€šç”¨æ£€æŸ¥")
            
            # çŸ­æš‚ç­‰å¾…DOMç¨³å®š
            time.sleep(0.5)
            
            logger.success("é¡µé¢åŸºç¡€å†…å®¹åŠ è½½å®Œæˆ")
            return True
            
        except TimeoutException:
            logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å¤„ç†")
            return True
        except Exception as e:
            logger.error(f"ç­‰å¾…é¡µé¢åŠ è½½æ—¶å‡ºé”™: {e}")
            return False
    
    def _human_like_scroll_and_load(self, target_url: str = None) -> bool:
        """
        äººç±»å¼æ»šåŠ¨åŠ è½½å†…å®¹ - ä¼˜åŒ–å›¾ç‰‡åŠ è½½ç‰ˆæœ¬
        ç¡®ä¿å……åˆ†çš„æ»šåŠ¨æ—¶é—´å’Œå›¾ç‰‡åŠ è½½
        """
        try:
            # è·å–é¡µé¢æ€»é«˜åº¦
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            logger.info(f"æ¨¡ä»¿äººç±»é˜…è¯»è¡Œä¸ºï¼Œæ»šåŠ¨åŠ è½½å†…å®¹...")
            
            # å¦‚æœé¡µé¢å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
            if total_height <= 1000:
                logger.debug("é¡µé¢è¾ƒçŸ­ï¼Œæ— éœ€è¯¦ç»†æ»šåŠ¨")
                return True
            
            # è®¡ç®—æ»šåŠ¨æ­¥æ•°ï¼šæ¯600åƒç´ ä¸€ä¸ªä½ç½®ï¼Œæé«˜ä¸‹è½½é€Ÿåº¦
            pixels_per_step = 600  # ä»300å¢åŠ åˆ°600ï¼Œæé«˜ä¸‹è½½é€Ÿåº¦
            scroll_positions = max(5, (total_height // pixels_per_step) + 1)
            
            # é€‚å½“å‡å°‘æ»šåŠ¨å»¶è¿Ÿï¼Œé…åˆæ›´å¤§çš„æ»šåŠ¨æ­¥é•¿
            scroll_delay = 1.5  # ä»2.0ç§’å‡å°‘åˆ°1.5ç§’ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå›¾ç‰‡åŠ è½½
            
            logger.info(f"é¡µé¢é«˜åº¦: {total_height}px, è®¡åˆ’æ»šåŠ¨ {scroll_positions} ä¸ªä½ç½®")
            
            for i in range(scroll_positions):
                # è®¡ç®—æ»šåŠ¨ä½ç½®
                if scroll_positions == 1:
                    scroll_to = total_height
                else:
                    progress = i / (scroll_positions - 1)
                    scroll_to = int(total_height * progress)
                
                logger.debug(f"æ»šåŠ¨åˆ°ä½ç½® {i+1}/{scroll_positions}: {scroll_to}px")
                
                # å¹³æ»‘æ»šåŠ¨åˆ°ç›®æ ‡ä½ç½®
                self.driver.execute_script(f"window.scrollTo(0, {scroll_to});")
                
                # ç­‰å¾…é¡µé¢å†…å®¹å’Œå›¾ç‰‡åŠ è½½
                time.sleep(scroll_delay)
                
                # åœ¨ä¸­é—´ä½ç½®é¢å¤–æ£€æŸ¥å›¾ç‰‡åŠ è½½
                if i % 4 == 0:  # æ¯4ä¸ªä½ç½®æ£€æŸ¥ä¸€æ¬¡ï¼ˆä»æ¯3ä¸ªæ”¹ä¸ºæ¯4ä¸ªï¼Œå‡å°‘æ£€æŸ¥é¢‘ç‡ï¼‰
                    self._trigger_image_loading()
                    time.sleep(0.8)  # ä»1.0ç§’å‡å°‘åˆ°0.8ç§’ï¼ŒåŠ å¿«å¤„ç†é€Ÿåº¦
            
            # æ»šåŠ¨å›é¡¶éƒ¨
            logger.info("æ»šåŠ¨å›é¡¶éƒ¨...")
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1.0)
            
            # æœ€ç»ˆå¿«é€Ÿæµè§ˆï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹å·²åŠ è½½
            logger.info("æœ€ç»ˆå¿«é€Ÿæµè§ˆ...")
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2.0)
            
            # æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰å›¾ç‰‡
            self._final_image_check()
            
            logger.success("äººç±»å¼é˜…è¯»æ»šåŠ¨å®Œæˆ")
            return True
            
        except Exception as e:
            logger.warning(f"æ»šåŠ¨è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")
            return True  # å³ä½¿å‡ºé”™ä¹Ÿç»§ç»­ï¼Œä¸å½±å“PDFç”Ÿæˆ
    
    def _trigger_image_loading(self):
        """è§¦å‘å›¾ç‰‡æ‡’åŠ è½½"""
        try:
            # è§¦å‘å„ç§æ‡’åŠ è½½æœºåˆ¶
            self.driver.execute_script("""
                // è§¦å‘æ‡’åŠ è½½å›¾ç‰‡
                var images = document.querySelectorAll('img[data-src], img[data-lazy-src], img[data-original]');
                images.forEach(function(img) {
                    if (img.dataset.src) img.src = img.dataset.src;
                    if (img.dataset.lazySrc) img.src = img.dataset.lazySrc;
                    if (img.dataset.original) img.src = img.dataset.original;
                    // è§¦å‘onloadäº‹ä»¶
                    img.onload = function() { console.log('Image loaded:', this.src); };
                });
                
                // è§¦å‘æ»šåŠ¨äº‹ä»¶ï¼Œå¯èƒ½æ¿€æ´»æŸäº›æ‡’åŠ è½½è„šæœ¬
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
            """)
            
            logger.debug("è§¦å‘å›¾ç‰‡æ‡’åŠ è½½")
            
        except Exception as e:
            logger.debug(f"è§¦å‘å›¾ç‰‡æ‡’åŠ è½½å¤±è´¥: {e}")
    
    def _final_image_check(self):
        """æœ€ç»ˆå›¾ç‰‡æ£€æŸ¥å’ŒåŠ è½½"""
        try:
            logger.info("æ‰§è¡Œæœ€ç»ˆå¿«é€Ÿæ£€æŸ¥...")
            
            # æœ€åä¸€æ¬¡è§¦å‘æ‰€æœ‰å¯èƒ½çš„æ‡’åŠ è½½
            self._trigger_image_loading()
            
            # ç­‰å¾…å›¾ç‰‡åŠ è½½
            time.sleep(3.0)
            
            # æ£€æŸ¥å›¾ç‰‡åŠ è½½çŠ¶æ€
            image_stats = self.driver.execute_script("""
                var images = document.querySelectorAll('img');
                var total = images.length;
                var loaded = 0;
                var broken = 0;
                var unloaded = 0;
                
                images.forEach(function(img) {
                    if (img.complete) {
                        if (img.naturalHeight !== 0) {
                            loaded++;
                        } else {
                            broken++;
                        }
                    } else {
                        unloaded++;
                    }
                });
                
                return {total: total, loaded: loaded, broken: broken, unloaded: unloaded};
            """)
            
            logger.info(f"å›¾ç‰‡çŠ¶æ€: æ€»æ•°={image_stats['total']}, å·²åŠ è½½={image_stats['loaded']}, æŸå={image_stats['broken']}, æœªåŠ è½½={image_stats['unloaded']}")
            
            # å¦‚æœè¿˜æœ‰æœªåŠ è½½çš„å›¾ç‰‡ï¼Œå†ç­‰å¾…ä¸€ä¼š
            if image_stats['unloaded'] > 0:
                logger.warning(f"è¿˜æœ‰ {image_stats['unloaded']} å¼ å›¾ç‰‡æœªåŠ è½½ï¼Œé¢å¤–ç­‰å¾…...")
                time.sleep(2.0)
            
            logger.success("æœ€ç»ˆæ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            logger.debug(f"æœ€ç»ˆå›¾ç‰‡æ£€æŸ¥å¤±è´¥: {e}")
    
    def _download_image(self, img_url: str, save_path: str) -> bool:
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = 'https://mp.weixin.qq.com' + img_url
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/'
            }
            
            response = requests.get(img_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False
    
    def _get_article_title(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–‡ç« æ ‡é¢˜"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                '#activity-name',
                '.rich_media_title',
                'h1',
                '.article-title',
                '.title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    # æ¸…ç†æ ‡é¢˜
                    title = re.sub(r'\s+', ' ', title)
                    return title[:50]
                    
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
        
        return "unknown_article"
    
    def _extract_article_content(self) -> dict:
        """
        æå–æ–‡ç« å†…å®¹ä¿¡æ¯ - å¿«é€Ÿç‰ˆæœ¬
        """
        try:
            article_info = {}
            
            # å¿«é€Ÿè·å–æ ‡é¢˜
            title_selectors = [
                '#activity-name',
                '.rich_media_title', 
                'h1',
                '.title',
                '[data-role="title"]'
            ]
            
            title = None
            for selector in title_selectors:
                try:
                    title_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_elem and title_elem.text.strip():
                        title = title_elem.text.strip()
                        break
                except:
                    continue
            
            article_info['title'] = title or "æœªçŸ¥æ ‡é¢˜"
            
            # å¿«é€Ÿè·å–ä½œè€…
            author_selectors = [
                '#js_name',
                '.profile_nickname',
                '.author',
                '[data-role="author"]'
            ]
            
            author = None
            for selector in author_selectors:
                try:
                    author_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if author_elem and author_elem.text.strip():
                        author = author_elem.text.strip()
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                        break
                except:
                    continue
            
            article_info['author'] = author or "æœªçŸ¥ä½œè€…"
            logger.info(f"æœ€ç»ˆè¯†åˆ«çš„å…¬ä¼—å·/ä½œè€…: {article_info['author']}")
            
            # æ·»åŠ URL
            article_info['url'] = self.driver.current_url
            
            logger.success(f"æˆåŠŸæå–æ–‡ç« ä¿¡æ¯: {article_info['title'][:30]}...")
            return article_info
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {"error": f"å†…å®¹æå–å¤±è´¥: {str(e)}"}
    
    def process_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†URLåˆ—è¡¨"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURL...")
            
            articles = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="å¤„ç†URL")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        articles.append({
                            'title': f"æ— æ•ˆURL_{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': "æ— æ•ˆçš„URLæ ¼å¼",
                            'url': url
                        })
                        continue
                    
                    # æå–æ–‡ç« ä¿¡æ¯
                    article_info = self.extract_article_info(url)
                    
                    if 'error' not in article_info:
                        articles.append(article_info)
                        if len(article_info.get('content', '').strip()) > 100:
                            success_count += 1
                    else:
                        logger.error(f"å¤„ç†URLå¤±è´¥: {article_info.get('error')}")
                        articles.append({
                            'title': f"é”™è¯¯_æ–‡ç« _{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': f"å¤„ç†å¤±è´¥: {article_info.get('error')}",
                            'url': url
                        })
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    if i < len(urls) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªURL
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
                    articles.append({
                        'title': f"å¼‚å¸¸_æ–‡ç« _{i+1}",
                        'author': "æœªçŸ¥",
                        'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'content': f"å¤„ç†å¼‚å¸¸: {str(e)}",
                        'url': url
                    })
            
            logger.success(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(articles)}")
            return articles
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return []
    
    def process_urls_as_pdf(self, urls: List[str], output_dir: str = "output/pdf") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºPDFæ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºPDF...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆPDF")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆPDFæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    pdf_filename = f"{safe_title}.pdf"
                    pdf_path = os.path.join(output_dir, pdf_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = pdf_path
                    while os.path.exists(pdf_path):
                        name_without_ext = safe_title
                        pdf_filename = f"{name_without_ext}_{counter}.pdf"
                        pdf_path = os.path.join(output_dir, pdf_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºPDF
                    if self.save_as_pdf(url, pdf_path):
                        saved_files.append(pdf_path)
                        success_count += 1
                        logger.success(f"PDFå·²ä¿å­˜: {pdf_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"PDFç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡PDFç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def process_urls_as_docx(self, urls: List[str], output_dir: str = "output/docx") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£æ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆWordæ–‡æ¡£")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆWordæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    docx_filename = f"{safe_title}.docx"
                    docx_path = os.path.join(output_dir, docx_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = docx_path
                    while os.path.exists(docx_path):
                        name_without_ext = safe_title
                        docx_filename = f"{name_without_ext}_{counter}.docx"
                        docx_path = os.path.join(output_dir, docx_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºWordæ–‡æ¡£
                    if self.save_as_docx(url, docx_path):
                        saved_files.append(docx_path)
                        success_count += 1
                        logger.success(f"Wordæ–‡æ¡£å·²ä¿å­˜: {docx_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"Wordæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡Wordæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _is_valid_wechat_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¾®ä¿¡æ–‡ç« URL"""
        try:
            # æ£€æŸ¥åŸºæœ¬æ ¼å¼
            if not url or not url.startswith('http'):
                return False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡æ–‡ç« URL
            if 'mp.weixin.qq.com/s' not in url:
                return False
            
            return True
            
        except Exception:
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
                logger.debug("æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
        finally:
            self.driver = None
    
    def save_complete_html(self, url: str, output_path: str) -> bool:
        """ä¿å­˜å®Œæ•´çš„HTMLæ–‡ä»¶ï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œæ ·å¼"""
        try:
            if not self.driver:
                if not self.setup_browser():
                    return False
            
            logger.info(f"æ­£åœ¨ä¿å­˜å®Œæ•´HTML: {url}")
            
            # è®¿é—®URL
            self.driver.get(url)
            time.sleep(2)  # å‡å°‘ç­‰å¾…æ—¶é—´
            
            # ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # å¿«é€Ÿæ»šåŠ¨åŠ è½½
            self._human_like_scroll_and_load()
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # è§£æHTMLå¹¶ä¸‹è½½å›¾ç‰‡
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è·å–æ–‡ç« æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å¤¹å
            title = self._get_article_title(soup)
            base_dir = os.path.dirname(output_path)
            article_dir = os.path.join(base_dir, f"images_{title[:20]}")
            os.makedirs(article_dir, exist_ok=True)
            
            # ä¸‹è½½å¹¶æ›¿æ¢å›¾ç‰‡é“¾æ¥
            img_count = 0
            for img in soup.find_all('img'):
                img_src = img.get('src') or img.get('data-src')
                if img_src:
                    try:
                        # ä¸‹è½½å›¾ç‰‡
                        img_filename = f"image_{img_count:03d}.jpg"
                        img_path = os.path.join(article_dir, img_filename)
                        
                        if self._download_image(img_src, img_path):
                            # æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
                            relative_path = f"images_{title[:20]}/{img_filename}"
                            img['src'] = relative_path
                            img_count += 1
                        
                    except Exception as e:
                        logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            
            # æ·»åŠ CSSæ ·å¼ä¿æŒåŸå§‹æ ¼å¼
            style_tag = soup.new_tag('style')
            style_tag.string = """
                body { max-width: 100%; margin: 0 auto; padding: 20px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }
                img { max-width: 100%; height: auto; }
                .rich_media_content { max-width: 100%; }
            """
            soup.head.append(style_tag)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            
            logger.success(f"å®Œæ•´HTMLå·²ä¿å­˜: {output_path}")
            logger.info(f"å›¾ç‰‡æ–‡ä»¶å¤¹: {article_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å®Œæ•´HTMLå¤±è´¥: {e}")
            return False
    
    def extract_full_article_content(self, url: str, download_images: bool = True) -> dict:
        """
        æå–å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬HTMLå†…å®¹å’Œå›¾ç‰‡
        
        Args:
            url: æ–‡ç« URL
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„å­—å…¸
        """
        try:
            if not self.driver:
                if not self.setup_browser():
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}

            logger.info(f"ğŸš€ å¼€å§‹æå–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 10)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­æå–")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # æ»šåŠ¨åŠ è½½å®Œæ•´å†…å®¹
            self._human_like_scroll_and_load()
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = self._extract_article_content()
            if 'error' in basic_info:
                return basic_info
            
            # æå–å®Œæ•´HTMLå†…å®¹
            html_content = self._extract_html_content()
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date()
            
            # å¤„ç†å›¾ç‰‡
            images_info = []
            if download_images:
                images_info = self._extract_and_download_images()
            
            # æ„å»ºå®Œæ•´çš„æ–‡ç« ä¿¡æ¯
            full_article = {
                'title': basic_info.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                'author': basic_info.get('author', 'æœªçŸ¥ä½œè€…'),
                'url': basic_info.get('url', url),
                'publish_date': publish_date,
                'html_content': html_content,
                'text_content': self._html_to_text(html_content),
                'images': images_info,
                'extraction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {full_article['title'][:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ–‡å­—é•¿åº¦={len(full_article['text_content'])}, å›¾ç‰‡æ•°é‡={len(images_info)}")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ æå–å®Œæ•´æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def _extract_html_content(self) -> str:
        """æå–æ–‡ç« çš„HTMLå†…å®¹"""
        try:
            # å¾®ä¿¡æ–‡ç« å†…å®¹çš„å¸¸è§é€‰æ‹©å™¨
            content_selectors = [
                '#js_content',
                '.rich_media_content', 
                '.appmsg_content_text',
                '.article_content',
                '[data-role="article-content"]'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if content_elem:
                        # è·å–å…ƒç´ çš„HTMLå†…å®¹
                        html_content = content_elem.get_attribute('innerHTML')
                        if html_content and len(html_content.strip()) > 100:
                            logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                            return html_content
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•è·å–æ•´ä¸ªbody
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨å†…å®¹å®¹å™¨ï¼Œå°è¯•æå–bodyå†…å®¹")
            body_elem = self.driver.find_element(By.TAG_NAME, "body")
            return body_elem.get_attribute('innerHTML') or ""
            
        except Exception as e:
            logger.error(f"æå–HTMLå†…å®¹å¤±è´¥: {e}")
            return ""
    
    def _extract_publish_date(self) -> str:
        """æå–æ–‡ç« å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„å¸¸è§é€‰æ‹©å™¨
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '.publish_time',
                '[data-role="publish-time"]',
                '.time'
            ]
            
            for selector in date_selectors:
                try:
                    date_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if date_elem and date_elem.text.strip():
                        date_text = date_elem.text.strip()
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                        return date_text
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»é¡µé¢æºç ä¸­æå–
            page_source = self.driver.page_source
            import re
            
            # å°è¯•åŒ¹é…å¸¸è§çš„æ—¶é—´æ ¼å¼
            date_patterns = [
                r'publish_time["\s]*[=:]["\s]*(\d{4}-\d{1,2}-\d{1,2}[^\'"]*)',
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{1,2}æœˆ\d{1,2}æ—¥)'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_source)
                if match:
                    date_text = match.group(1)
                    logger.debug(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def _extract_and_download_images(self) -> List[Dict[str, str]]:
        """æå–å¹¶ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡"""
        try:
            images_info = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_elements)} å¼ å›¾ç‰‡")
            
            for i, img_elem in enumerate(img_elements):
                try:
                    # è·å–å›¾ç‰‡URL
                    img_src = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
                    
                    if not img_src or img_src.startswith('data:'):
                        continue  # è·³è¿‡base64å›¾ç‰‡å’Œæ— æ•ˆå›¾ç‰‡
                    
                    # è·å–å›¾ç‰‡çš„altå±æ€§ä½œä¸ºæè¿°
                    img_alt = img_elem.get_attribute('alt') or f"å›¾ç‰‡_{i+1}"
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"image_{i+1:03d}.jpg"
                    
                    # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
                    images_dir = os.path.join("output", "images", 
                                            datetime.now().strftime('%Y%m%d'))
                    os.makedirs(images_dir, exist_ok=True)
                    
                    # æœ¬åœ°ä¿å­˜è·¯å¾„
                    local_path = os.path.join(images_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image(img_src, local_path):
                        images_info.append({
                            'original_url': img_src,
                            'local_path': local_path,
                            'filename': img_filename,
                            'alt': img_alt,
                            'index': i + 1
                        })
                        logger.debug(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {img_filename}")
                    else:
                        logger.warning(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(images_info)}/{len(img_elements)}")
            return images_info
            
        except Exception as e:
            logger.error(f"æå–å’Œä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return []
    
    def _html_to_text(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
        try:
            from bs4 import BeautifulSoup
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for element in soup(["script", "style"]):
                element.decompose()
            
            # è·å–çº¯æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            clean_text = '\n'.join(lines)
            
            return clean_text
            
        except Exception as e:
            logger.error(f"HTMLè½¬æ–‡æœ¬å¤±è´¥: {e}")
            return ""




def main():
    """æµ‹è¯•å·¥å…·"""
    # ç¤ºä¾‹URL
    test_url = "https://mp.weixin.qq.com/s/LiS8ytwKsKP9yAHGp96G_Q"
    
    scraper = SimpleUrlScraper(headless=False)  # ä½¿ç”¨å¯è§æ¨¡å¼ä¾¿äºå¤„ç†éªŒè¯ç 
    
    try:
        # æµ‹è¯•PDFç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_path = f"output/pdf/test_article_{timestamp}.pdf"
        
        if scraper.save_as_pdf(test_url, pdf_path):
            logger.success(f"PDFæµ‹è¯•æˆåŠŸ: {pdf_path}")
        
        # æµ‹è¯•å®Œæ•´HTMLä¿å­˜
        html_path = f"output/html/test_article_{timestamp}.html"
        
        if scraper.save_complete_html(test_url, html_path):
            logger.success(f"å®Œæ•´HTMLæµ‹è¯•æˆåŠŸ: {html_path}")
        
        # æµ‹è¯•åŸºæœ¬æ–‡ç« ä¿¡æ¯æå–
        article = scraper.extract_article_info(test_url)
        
        if 'error' not in article:
            logger.success(f"æ–‡ç« ä¿¡æ¯æå–æˆåŠŸ: {article.get('title')}")
            
        else:
            logger.error(f"å¤„ç†å¤±è´¥: {article.get('error')}")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        scraper.cleanup()


if __name__ == "__main__":
    main() 
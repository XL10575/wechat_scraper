#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple WeChat URL Scraper
ç®€å•å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·

ç›´æ¥å¤„ç†ç”¨æˆ·æä¾›çš„å¾®ä¿¡æ–‡ç« URLï¼Œè·å–å†…å®¹å¹¶ä¿å­˜ä¸ºé£ä¹¦æ ¼å¼
"""

import time
import random
import json
import re
import base64
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement, qn

from typing import List, Dict, Any, Optional
from loguru import logger
from datetime import datetime
import os

from utils import clean_text




class SimpleUrlScraper:
    """ç®€å•URLå¤„ç†å·¥å…·"""
    
    def __init__(self, headless: bool = False):
        """
        åˆå§‹åŒ–å·¥å…· - å¿«é€Ÿæ¨¡å¼ï¼Œä¸ç«‹å³å¯åŠ¨æµè§ˆå™¨
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        self.driver = None
        self.headless = headless
        
        logger.info("ç®€å•URLå¤„ç†å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆæµè§ˆå™¨å°†æ‡’åŠ è½½ï¼‰")
    
    def setup_browser(self, headless: bool = True) -> Optional[webdriver.Chrome]:
        """è®¾ç½®Chromeæµè§ˆå™¨ - é’ˆå¯¹exeç¯å¢ƒä¼˜åŒ–"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆexeä¼˜åŒ–æ¨¡å¼ï¼‰...")
            
            # Chromeé€‰é¡¹é…ç½® - é’ˆå¯¹exeç¯å¢ƒä¼˜åŒ–
            chrome_options = Options()
            
            # åŸºç¡€é€‰é¡¹
            if headless:
                chrome_options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„headlessæ¨¡å¼
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # é‡è¦ï¼šä¸ç¦ç”¨JavaScriptå’ŒCSSï¼ŒéªŒè¯ç éœ€è¦è¿™äº›
            # chrome_options.add_argument("--disable-javascript")  # æ³¨é‡Šæ‰
            # chrome_options.add_argument("--disable-css")  # æ³¨é‡Šæ‰
            
            # åæ£€æµ‹é…ç½® - é’ˆå¯¹exeç¯å¢ƒ
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç¯å¢ƒ
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor")
            
            # exeç¯å¢ƒç‰¹æ®Šé…ç½®
            import sys
            if getattr(sys, 'frozen', False):  # æ£€æµ‹æ˜¯å¦ä¸ºexeç¯å¢ƒ
                logger.info("ğŸ”§ æ£€æµ‹åˆ°exeç¯å¢ƒï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                chrome_options.add_argument("--disable-logging")
                chrome_options.add_argument("--disable-gpu-logging")
                chrome_options.add_argument("--silent")
                chrome_options.add_argument("--no-first-run")
                chrome_options.add_argument("--no-default-browser-check")
            
            # GitHub Actionsç‰¹æ®Šé…ç½®
            if os.getenv('GITHUB_ACTIONS'):
                logger.info("ğŸ”§ æ£€æµ‹åˆ°GitHub Actionsç¯å¢ƒï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                chrome_options.add_argument("--no-first-run")
                chrome_options.add_argument("--no-default-browser-check")
                chrome_options.add_argument("--disable-default-apps")
                chrome_options.add_argument("--disable-background-timer-throttling")
                chrome_options.add_argument("--disable-backgrounding-occluded-windows")
                chrome_options.add_argument("--disable-renderer-backgrounding")
                chrome_options.add_argument("--disable-features=TranslateUI")
                chrome_options.add_argument("--disable-ipc-flooding-protection")
                chrome_options.add_argument("--single-process")
                chrome_options.add_argument("--remote-debugging-port=9222")
                
                # è®¾ç½®ç”¨æˆ·æ•°æ®ç›®å½•
                user_data_dir = "/tmp/chrome-user-data"
                os.makedirs(user_data_dir, exist_ok=True)
                chrome_options.add_argument(f"--user-data-dir={user_data_dir}")
            
            # ç”¨æˆ·ä»£ç† - ä½¿ç”¨Windows Chrome
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            chrome_options.add_argument(f"--user-agent={user_agent}")
            
            # é¡µé¢åŠ è½½ç­–ç•¥
            chrome_options.page_load_strategy = 'normal'  # æ”¹ä¸ºnormalä»¥ç¡®ä¿éªŒè¯ç åŠ è½½
            
            # è¶…æ—¶è®¾ç½®
            chrome_options.add_argument("--timeout=60000")  # å»¶é•¿è¶…æ—¶æ—¶é—´
            
            # æ—¥å¿—é…ç½®
            chrome_options.add_argument("--log-level=3")
            
            # æ€§èƒ½ä¼˜åŒ– - ä½†ä¸ç¦ç”¨å›¾ç‰‡ï¼ˆéªŒè¯ç éœ€è¦ï¼‰
            prefs = {
                "profile.default_content_setting_values": {
                    "plugins": 2,
                    "popups": 2,
                    "geolocation": 2,
                    "notifications": 2,
                    "media_stream": 2,
                },
                # ä¸ç¦ç”¨å›¾ç‰‡ï¼ŒéªŒè¯ç éœ€è¦å›¾ç‰‡åŠ è½½
                "profile.managed_default_content_settings": {
                    # "images": 2  # æ³¨é‡Šæ‰ï¼Œå…è®¸å›¾ç‰‡åŠ è½½
                }
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            # WebDriver Manageré…ç½®
            service = None
            try:
                logger.info("âš¡ å¯åŠ¨Chromeæµè§ˆå™¨...")
                
                # åœ¨GitHub Actionsç¯å¢ƒä¸­ï¼Œå°è¯•ä½¿ç”¨ç³»ç»ŸChrome
                if os.getenv('GITHUB_ACTIONS'):
                    # æ£€æŸ¥ç³»ç»ŸChromeè·¯å¾„
                    chrome_paths = [
                        "/usr/bin/google-chrome",
                        "/usr/bin/google-chrome-stable",
                        "/usr/bin/chromium-browser",
                        "/snap/bin/chromium"
                    ]
                    
                    chrome_binary = None
                    for path in chrome_paths:
                        if os.path.exists(path):
                            chrome_binary = path
                            logger.info(f"ğŸ” æ‰¾åˆ°ChromeäºŒè¿›åˆ¶æ–‡ä»¶: {chrome_binary}")
                            break
                    
                    if chrome_binary:
                        chrome_options.binary_location = chrome_binary
                    
                    # å°è¯•ä½¿ç”¨ç³»ç»Ÿchromedriver
                    chromedriver_paths = [
                        "/usr/bin/chromedriver",
                        "/usr/local/bin/chromedriver"
                    ]
                    
                    chromedriver_path = None
                    for path in chromedriver_paths:
                        if os.path.exists(path):
                            chromedriver_path = path
                            logger.info(f"ğŸ” æ‰¾åˆ°ChromeDriver: {chromedriver_path}")
                            break
                    
                    if chromedriver_path:
                        service = Service(chromedriver_path)
                    else:
                        # ä½¿ç”¨WebDriverManagerä½œä¸ºåå¤‡
                        from webdriver_manager.chrome import ChromeDriverManager
                        service = Service(ChromeDriverManager().install())
                else:
                    # æœ¬åœ°ç¯å¢ƒä½¿ç”¨WebDriverManager
                    from webdriver_manager.chrome import ChromeDriverManager
                    service = Service(ChromeDriverManager().install())
                
                # åˆ›å»ºWebDriverå®ä¾‹
                if service:
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                else:
                    driver = webdriver.Chrome(options=chrome_options)
                
                # è®¾ç½®è¶…æ—¶
                driver.set_page_load_timeout(60)  # å»¶é•¿é¡µé¢åŠ è½½è¶…æ—¶
                driver.implicitly_wait(15)  # å»¶é•¿éšå¼ç­‰å¾…
                
                # åæ£€æµ‹è„šæœ¬æ³¨å…¥
                driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
                driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']})")
                
                logger.info("âœ… Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆå·²ä¼˜åŒ–éªŒè¯ç æ”¯æŒï¼‰")
                return driver
                
            except Exception as e:
                logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                
                # å°è¯•é™çº§æ–¹æ¡ˆ
                if not os.getenv('GITHUB_ACTIONS'):
                    logger.info("ğŸ”„ å°è¯•é™çº§æ–¹æ¡ˆ...")
                    try:
                        # ç§»é™¤ä¸€äº›å¯èƒ½æœ‰é—®é¢˜çš„é€‰é¡¹
                        chrome_options = Options()
                        chrome_options.add_argument("--headless")
                        chrome_options.add_argument("--no-sandbox")
                        chrome_options.add_argument("--disable-dev-shm-usage")
                        
                        driver = webdriver.Chrome(options=chrome_options)
                        driver.set_page_load_timeout(30)
                        logger.info("âœ… é™çº§æ–¹æ¡ˆæˆåŠŸ")
                        return driver
                    except Exception as e2:
                        logger.error(f"é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                
                return None
                
        except Exception as e:
            logger.error(f"è®¾ç½®æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_article_info(self, url: str) -> dict:
        """
        æå–æ–‡ç« ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¼˜å…ˆä½¿ç”¨ç®€åŒ–è®¿é—®
        """
        try:
            logger.info(f"ğŸ“„ å¼€å§‹æå–æ–‡ç« ä¿¡æ¯: {url}")
            
            # é¦–å…ˆå°è¯•ç®€åŒ–è®¿é—®
            if self.simple_article_access(url):
                logger.info("âœ… ä½¿ç”¨ç®€åŒ–æ–¹å¼æˆåŠŸè®¿é—®æ–‡ç« ")
            else:
                logger.warning("âš ï¸ ç®€åŒ–è®¿é—®å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ–¹å¼")
                # æ ‡å‡†è®¿é—®æµç¨‹
                if not self.driver:
                    self.driver = self.setup_browser(self.headless)
                
                self.driver.get(url)
                self._wait_for_basic_page_load()
            
            # å¿«é€Ÿæå–æ–‡ç« ä¿¡æ¯
            try:
                wait = WebDriverWait(self.driver, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢å…ƒç´ åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­")
            
            # è·å–é¡µé¢æºä»£ç 
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æå–ä¿¡æ¯
            title = self._extract_title_from_soup(soup)
            author = self._extract_author_from_soup(soup)
            publish_date = self._extract_publish_date_from_soup(soup)
            
            logger.info(f"ğŸ“Š æå–å®Œæˆ - æ ‡é¢˜: {title[:30]}...")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'title': 'æå–å¤±è´¥',
                'author': 'æœªçŸ¥',
                'publish_date': 'æœªçŸ¥',
                'url': url,
                'error': str(e)
            }
    
    def _extract_title_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–‡ç« æ ‡é¢˜"""
        try:
            # 1. ä¼˜å…ˆå°è¯•ä¸»è¦æ ‡é¢˜é€‰æ‹©å™¨
            title_selectors = [
                'h1.rich_media_title',
                'h1[id*="title"]',
                'h1.weui-msg__title',
                '.rich_media_title',
                '#activity-name',
                '.activity_title',
                'title'
            ]
            
            for selector in title_selectors:
                try:
                    title_element = soup.select_one(selector)
                    if title_element and title_element.get_text(strip=True):
                        title = title_element.get_text(strip=True)
                        # æ¸…ç†æ ‡é¢˜
                        title = title.replace('\n', ' ').replace('\r', ' ')
                        title = ' '.join(title.split())  # å»é™¤å¤šä½™ç©ºæ ¼
                        if len(title) > 3:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                            logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æå–åˆ°æ ‡é¢˜: {title[:50]}...")
                            return title
                except Exception as e:
                    logger.debug(f"é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # 2. å°è¯•ä»é¡µé¢titleä¸­æå–
            try:
                page_title = soup.title
                if page_title and page_title.string:
                    title = page_title.string.strip()
                    if title and len(title) > 3:
                        logger.debug(f"âœ… ä»é¡µé¢titleæå–åˆ°æ ‡é¢˜: {title[:50]}...")
                        return title
            except Exception as e:
                logger.debug(f"ä»é¡µé¢titleæå–å¤±è´¥: {e}")
            
            # 3. å°è¯•ä»metaæ ‡ç­¾ä¸­æå–
            try:
                meta_title = soup.find('meta', {'property': 'og:title'})
                if meta_title and meta_title.get('content'):
                    title = meta_title.get('content').strip()
                    if title and len(title) > 3:
                        logger.debug(f"âœ… ä»metaæ ‡ç­¾æå–åˆ°æ ‡é¢˜: {title[:50]}...")
                        return title
            except Exception as e:
                logger.debug(f"ä»metaæ ‡ç­¾æå–å¤±è´¥: {e}")
            
            logger.warning("æ— æ³•æå–æ–‡ç« æ ‡é¢˜")
            return "æœªèƒ½è·å–æ–‡ç« æ ‡é¢˜"
            
        except Exception as e:
            logger.error(f"æå–æ ‡é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "æ ‡é¢˜æå–å¤±è´¥"
    
    def _extract_author_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ä½œè€…ä¿¡æ¯"""
        try:
            # ä½œè€…ä¿¡æ¯é€‰æ‹©å™¨
            author_selectors = [
                '.rich_media_meta_text',
                '.profile_nickname',
                '.account_nickname',
                '.wx_follow_nickname',
                '#profileBt a',
                '.rich_media_meta .rich_media_meta_text',
                '[data-author]'
            ]
            
            for selector in author_selectors:
                try:
                    author_element = soup.select_one(selector)
                    if author_element:
                        author = author_element.get_text(strip=True)
                        if author and len(author) > 0:
                            logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æå–åˆ°ä½œè€…: {author}")
                            return author
                except Exception as e:
                    logger.debug(f"ä½œè€…é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # å°è¯•ä»metaæ ‡ç­¾æå–
            try:
                meta_author = soup.find('meta', {'name': 'author'})
                if meta_author and meta_author.get('content'):
                    author = meta_author.get('content').strip()
                    if author:
                        logger.debug(f"âœ… ä»metaæ ‡ç­¾æå–åˆ°ä½œè€…: {author}")
                        return author
            except Exception as e:
                logger.debug(f"ä»metaæ ‡ç­¾æå–ä½œè€…å¤±è´¥: {e}")
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.error(f"æå–ä½œè€…æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "ä½œè€…æå–å¤±è´¥"
    
    def _extract_publish_date_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–å‘å¸ƒæ—¥æœŸ"""
        try:
            # å‘å¸ƒæ—¥æœŸé€‰æ‹©å™¨
            date_selectors = [
                '.rich_media_meta_text',
                '.publish_time',
                '.time',
                '#publish_time',
                '.rich_media_meta .rich_media_meta_text',
                '[data-time]',
                '.ct_time'
            ]
            
            for selector in date_selectors:
                try:
                    date_elements = soup.select(selector)
                    for element in date_elements:
                        text = element.get_text(strip=True)
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼
                        if text and any(char in text for char in ['å¹´', 'æœˆ', 'æ—¥', '-', '/', ':', '2024', '2023', '2025']):
                            logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æå–åˆ°æ—¥æœŸ: {text}")
                            return text
                except Exception as e:
                    logger.debug(f"æ—¥æœŸé€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # å°è¯•ä»metaæ ‡ç­¾æå–
            try:
                meta_date = soup.find('meta', {'property': 'article:published_time'})
                if meta_date and meta_date.get('content'):
                    date = meta_date.get('content').strip()
                    if date:
                        logger.debug(f"âœ… ä»metaæ ‡ç­¾æå–åˆ°æ—¥æœŸ: {date}")
                        return date
            except Exception as e:
                logger.debug(f"ä»metaæ ‡ç­¾æå–æ—¥æœŸå¤±è´¥: {e}")
            
            return "æœªçŸ¥æ—¥æœŸ"
            
        except Exception as e:
            logger.error(f"æå–å‘å¸ƒæ—¥æœŸæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "æ—¥æœŸæå–å¤±è´¥"
    
    def save_as_pdf(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºPDFæ–‡ä»¶ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜PDF: {url}")
            # 1. ä½¿ç”¨SeleniumåŠ è½½å®Œæ•´é¡µé¢å†…å®¹
            if not self.driver:
                self.driver = self.setup_browser(headless=True)
            self.driver.get(url)
            # å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œä»…ç­‰å¾…åŸºæœ¬åŠ è½½
            self._wait_for_basic_page_load()
            self._human_like_scroll_and_load()
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(0.2)
            # æ³¨å…¥CSSæ ·å¼
            css_style = """
            <style>
                @page { margin: 0 !important; padding: 0 !important; size: A4 !important; }
                * { box-sizing: border-box !important; }
                body, html { margin: 0 !important; padding: 0 !important; width: 100% !important; max-width: 100% !important; }
                #js_content, .rich_media_content, .rich_media_area_primary { margin: 0 !important; padding: 5px !important; width: 100% !important; max-width: 100% !important; }
                .rich_media_meta_list, .rich_media_tool, .qr_code_pc_outer, .reward_qrcode_area, .reward_area, #js_pc_qr_code_img, .function_mod, .profile_container, .rich_media_global_msg { display: none !important; }
            </style>
            """
            self.driver.execute_script(f"""
                var style = document.createElement('style');
                style.type = 'text/css';
                style.innerHTML = `{css_style}`;
                document.head.appendChild(style);
            """)
            time.sleep(0.3)
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            pdf_options = {
                'paperFormat': 'A4',
                'printBackground': True,
                'marginTop': 0,
                'marginBottom': 0,
                'marginLeft': 0,
                'marginRight': 0,
                'preferCSSPageSize': True,
                'displayHeaderFooter': False,
                'scale': 1.0,
                'landscape': False,
                'transferMode': 'ReturnAsBase64',
                'generateTaggedPDF': False
            }
            pdf_data = self.driver.execute_cdp_cmd('Page.printToPDF', pdf_options)
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(pdf_data['data']))
            logger.success(f"PDFä¿å­˜æˆåŠŸ: {output_path}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜PDFå¤±è´¥: {e}")
            return False
    
    def save_as_docx(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºWordæ–‡æ¡£ - å®Œæ•´å†…å®¹ç‰ˆæœ¬
        ä½¿ç”¨SeleniumåŠ¨æ€åŠ è½½ + BeautifulSoupè§£æç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜Wordæ–‡æ¡£: {url}")
            
            # 1. ä½¿ç”¨SeleniumåŠ è½½å®Œæ•´é¡µé¢å†…å®¹ï¼ˆåŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # 2. åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼ï¼ˆå¾®è½¯é›…é»‘ï¼Œ10.5å·å­—ï¼‰
            try:
                from docx.enum.style import WD_STYLE_TYPE
                style = doc.styles.add_style('DefaultParagraph', WD_STYLE_TYPE.PARAGRAPH)
                font = style.font
                font.name = 'å¾®è½¯é›…é»‘'
                font.size = Pt(10.5)
            except:
                logger.debug("è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼å¤±è´¥ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")
            
            # 3. æ·»åŠ æ ‡é¢˜
            title = article_data.get('title', 'å¾®ä¿¡æ–‡ç« ')
            title_paragraph = doc.add_heading(title, level=1)
            title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 4. æ·»åŠ ä½œè€…å’Œå‘å¸ƒæ—¶é—´
            meta_info = f"ä½œè€…: {article_data.get('author', 'æœªçŸ¥')}\n"
            meta_info += f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_date', 'æœªçŸ¥')}\n"
            meta_info += f"åŸæ–‡é“¾æ¥: {url}\n"
            
            meta_paragraph = doc.add_paragraph(meta_info)
            meta_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 5. æ·»åŠ åˆ†éš”çº¿
            doc.add_paragraph("=" * 50).alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 6. å¤„ç†HTMLå†…å®¹ï¼Œç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½åŒ…å«
            content_soup = article_data.get('content_soup')
            images = article_data.get('images', [])
            
            if content_soup:
                logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å« {len(images)} å¼ å›¾ç‰‡")
                try:
                    # æ·»åŠ è¶…æ—¶ä¿æŠ¤çš„å†…å®¹å¤„ç†
                    import signal
                    
                    def timeout_handler(signum, frame):
                        raise TimeoutError("æ–‡æ¡£å¤„ç†è¶…æ—¶")
                    
                    # è®¾ç½®30ç§’è¶…æ—¶
                    if hasattr(signal, 'SIGALRM'):  # Unixç³»ç»Ÿ
                        signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(30)
                    
                    self._process_wechat_content_to_docx(doc, content_soup, images)
                    
                    if hasattr(signal, 'SIGALRM'):
                        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                        
                except TimeoutError:
                    logger.warning("â° æ–‡æ¡£å¤„ç†è¶…æ—¶ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†")
                    # é™çº§åˆ°ç®€å•æ–‡æœ¬å¤„ç†
                    text_content = content_soup.get_text(strip=True)
                    if text_content:
                        doc.add_paragraph(text_content[:5000])  # é™åˆ¶é•¿åº¦
                except Exception as e:
                    logger.warning(f"âš ï¸ å†…å®¹å¤„ç†å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†")
                    # é™çº§åˆ°ç®€å•æ–‡æœ¬å¤„ç†
                    try:
                        text_content = content_soup.get_text(strip=True)
                        if text_content:
                            doc.add_paragraph(text_content[:5000])  # é™åˆ¶é•¿åº¦
                    except:
                        doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« å†…å®¹")
                doc.add_paragraph("æœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹")
            
            # 7. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # 8. ä¿å­˜æ–‡æ¡£
            try:
                doc.save(output_path)
                logger.success(f"Wordæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
                logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å›¾ç‰‡={len(images)}å¼ , æ–‡ä»¶å¤§å°={os.path.getsize(output_path)/1024:.1f}KB")
                return True
            except Exception as e:
                logger.error(f"ä¿å­˜Wordæ–‡æ¡£æ–‡ä»¶å¤±è´¥: {e}")
                return False
            
        except Exception as e:
            logger.error(f"ä¿å­˜Wordæ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def _extract_wechat_article_with_selenium(self, url: str) -> dict:
        """
        ä½¿ç”¨Seleniumè·å–å®Œæ•´çš„å¾®ä¿¡æ–‡ç« å†…å®¹ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        try:
            logger.info(f"ğŸš€ ä½¿ç”¨Seleniumè·å–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŸºç¡€åŠ è½½ - å‡å°‘è¶…æ—¶æ—¶é—´
            try:
                wait = WebDriverWait(self.driver, 8)  # ä»15ç§’å‡å°‘åˆ°8ç§’
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                logger.debug("âœ… é¡µé¢åŸºç¡€æ¡†æ¶åŠ è½½å®Œæˆ")
            except TimeoutException:
                logger.warning("â° é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­")
            
            # å¤„ç†éªŒè¯ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰- å·²ç¦ç”¨ï¼Œç›´æ¥è·³è¿‡
            if not self._handle_captcha_if_present():
                logger.debug("éªŒè¯ç æ£€æµ‹è·³è¿‡")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # å®Œæ•´æ»šåŠ¨åŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹
            self._human_like_scroll_and_load()
            
            # å‡å°‘åŠ¨æ€å†…å®¹ç­‰å¾…æ—¶é—´ï¼Œä»3ç§’å‡å°‘åˆ°1ç§’
            logger.info("â³ å¿«é€Ÿç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½...")
            time.sleep(1)
            
            # è·å–å®Œå…¨æ¸²æŸ“åçš„é¡µé¢HTML
            page_source = self.driver.page_source
            
            # ä½¿ç”¨BeautifulSoupè§£æå®Œæ•´çš„HTML
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è°ƒè¯•ï¼šä¿å­˜å®Œæ•´HTMLï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
            try:
                # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹å¼æ£€æŸ¥æ—¥å¿—çº§åˆ«
                debug_html_path = "debug_selenium_page.html"
                with open(debug_html_path, 'w', encoding='utf-8') as f:
                    f.write(page_source)
                logger.debug(f"å®Œæ•´HTMLå·²ä¿å­˜åˆ°: {debug_html_path}")
            except Exception as e:
                logger.debug(f"ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {e}")
            
            # æå–æ ‡é¢˜ - ä»æ¸²æŸ“åçš„é¡µé¢
            title = self._extract_title_from_selenium_soup(soup)
            
            # æå–ä½œè€…
            author = self._extract_author_from_selenium_soup(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date_from_selenium_soup(soup)
            
            # æå–æ–‡ç« æ­£æ–‡å†…å®¹ - ä»å®Œå…¨åŠ è½½çš„é¡µé¢
            content_soup = self._extract_content_from_selenium_soup(soup)
            
            if not content_soup:
                return {"error": "æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹"}
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆä½¿ç”¨Seleniumå·²ç»åŠ è½½çš„å›¾ç‰‡ï¼‰
            images = self._download_images_from_selenium_soup(content_soup, url)
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {title[:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ®µè½æ•°={len(content_soup.find_all(['p', 'div']))}, å›¾ç‰‡æ•°é‡={len(images)}")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'content_soup': content_soup,
                'images': images,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"Seleniumæå–æ–‡ç« å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def _extract_title_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ ‡é¢˜"""
        try:
            # å¾®ä¿¡æ–‡ç« æ ‡é¢˜çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            title_selectors = [
                'h1#activity-name',
                'h2.rich_media_title',
                '.rich_media_title',
                'h1.rich_media_title',
                '.rich_media_title h1',
                '#activity-name',
                '.wx_title',
                '[data-role="title"]',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    if len(title) > 3:  # ç¡®ä¿ä¸æ˜¯ç©ºæ ‡é¢˜
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                        return title
            
            # ä»é¡µé¢æ ‡é¢˜æå–
            title_tag = soup.find('title')
            if title_tag and title_tag.get_text(strip=True):
                title = title_tag.get_text(strip=True)
                if 'å¾®ä¿¡å…¬ä¼—å¹³å°' not in title and len(title) > 3:
                    logger.debug(f"ä»titleæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # ä»metaæ ‡ç­¾æå–
            meta_title = soup.find('meta', property='og:title')
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content').strip()
                if title and len(title) > 3:
                    logger.debug(f"ä»metaæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            return "æœªçŸ¥æ ‡é¢˜"
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_author_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–ä½œè€…"""
        try:
            # å¾®ä¿¡æ–‡ç« ä½œè€…çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '[data-role="nickname"]',
                '.account_nickname',
                '.wx_author',
                '#profileBt .nickname'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem and author_elem.get_text(strip=True):
                    author = author_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…å¤±è´¥: {e}")
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '[data-role="publish-time"]',
                '.wx_time',
                '.time'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem and date_elem.get_text(strip=True):
                    date_text = date_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return "æœªçŸ¥æ—¶é—´"
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_content_from_selenium_soup(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ–‡ç« æ­£æ–‡åŒºåŸŸ"""
        try:
            # å¾®ä¿¡æ–‡ç« æ­£æ–‡çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '.appmsg_content_text',
                '[data-role="article-content"]',
                '.rich_media_area_primary',
                '.rich_media_wrp',
                '#js_content_container'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦è¶³å¤Ÿå¤š
                    text_content = content_elem.get_text(strip=True)
                    if text_content and len(text_content) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                        logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                        return content_elem
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å®¹å™¨ï¼Œæ™ºèƒ½æå–
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨æ­£æ–‡å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–")
            
            # æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬å†…å®¹çš„div
            all_divs = soup.find_all('div')
            best_div = None
            max_text_length = 0
            
            for div in all_divs:
                text = div.get_text(strip=True)
                if len(text) > max_text_length and len(text) > 200:  # è‡³å°‘200ä¸ªå­—ç¬¦
                    # æ’é™¤å¯¼èˆªã€å¤´éƒ¨ã€è„šéƒ¨ç­‰åŒºåŸŸ
                    div_class = ' '.join(div.get('class', []))
                    div_id = div.get('id', '')
                    
                    exclude_keywords = ['nav', 'header', 'footer', 'sidebar', 'menu', 'toolbar', 'ad']
                    if not any(keyword in div_class.lower() + div_id.lower() for keyword in exclude_keywords):
                        max_text_length = len(text)
                        best_div = div
            
            if best_div:
                logger.debug(f"æ™ºèƒ½æå–æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ–‡æœ¬é•¿åº¦: {max_text_length}")
                return best_div
            
            logger.warning("ä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            return soup.find('body') or soup
            
        except Exception as e:
            logger.error(f"æå–æ­£æ–‡å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _download_images_from_selenium_soup(self, content_soup: BeautifulSoup, base_url: str) -> list:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­ä¸‹è½½å›¾ç‰‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            images_info = []
            img_tags = content_soup.find_all('img')
            
            if not img_tags:
                logger.info("ğŸ“· æœªå‘ç°å›¾ç‰‡")
                return images_info
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_tags)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            
            # åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•
            import urllib.parse
            parsed_url = urllib.parse.urlparse(base_url)
            safe_domain = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.netloc)
            img_dir = os.path.join("output", "images", safe_domain)
            os.makedirs(img_dir, exist_ok=True)
            
            # é™åˆ¶åŒæ—¶ä¸‹è½½çš„å›¾ç‰‡æ•°é‡ï¼Œé¿å…å¡ä½
            max_images = min(len(img_tags), 20)  # æœ€å¤šä¸‹è½½20å¼ å›¾ç‰‡
            successful_downloads = 0
            
            for i, img in enumerate(img_tags[:max_images]):
                try:
                    # ä»SeleniumåŠ è½½çš„é¡µé¢ä¸­ï¼Œsrcåº”è¯¥å·²ç»è¢«å®Œå…¨è§£æ
                    img_src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if not img_src:
                        continue
                    
                    # å¤„ç†å®Œæ•´çš„URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = f"https://{parsed_url.netloc}" + img_src
                    elif not img_src.startswith('http'):
                        img_src = urllib.parse.urljoin(base_url, img_src)
                    
                    # è·³è¿‡è¿‡å°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å›¾æ ‡ï¼‰æˆ–base64å›¾ç‰‡
                    if ('icon' in img_src.lower() or 
                        'logo' in img_src.lower() or 
                        img_src.startswith('data:') or
                        len(img_src) > 1000):  # è·³è¿‡è¶…é•¿URL
                        continue
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"img_{i+1:03d}.jpg"
                    img_path = os.path.join(img_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
                    download_success = self._download_image_with_timeout(img_src, img_path, timeout=10)
                    
                    if download_success:
                        images_info.append({
                            'url': img_src,
                            'local_path': img_path,
                            'filename': img_filename
                        })
                        successful_downloads += 1
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_filename}")
                    else:
                        # å³ä½¿ä¸‹è½½å¤±è´¥ï¼Œä¹Ÿè®°å½•å›¾ç‰‡ä¿¡æ¯ï¼Œä½†local_pathä¸ºNone
                        images_info.append({
                            'url': img_src,
                            'local_path': None,
                            'filename': img_filename
                        })
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_src[:50]}...")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {successful_downloads}/{max_images} å¼ æˆåŠŸ")
            return images_info
            
        except Exception as e:
            logger.warning(f"å›¾ç‰‡ä¸‹è½½è¿‡ç¨‹å¼‚å¸¸: {e}")
            return []

    def _download_image_with_timeout(self, img_url: str, save_path: str, timeout: int = 10) -> bool:
        """å¸¦è¶…æ—¶æ§åˆ¶çš„å›¾ç‰‡ä¸‹è½½"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
            }
            
            # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            response = requests.get(img_url, headers=headers, timeout=timeout, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png', 'gif', 'webp']):
                logger.debug(f"éå›¾ç‰‡å†…å®¹ç±»å‹: {content_type}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½è¿‡å¤§çš„æ–‡ä»¶
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MBé™åˆ¶
                logger.debug(f"å›¾ç‰‡æ–‡ä»¶è¿‡å¤§: {content_length} bytes")
                return False
            
            # å†™å…¥æ–‡ä»¶
            with open(save_path, 'wb') as f:
                downloaded_size = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        # é™åˆ¶ä¸‹è½½å¤§å°
                        if downloaded_size > 10 * 1024 * 1024:  # 10MBé™åˆ¶
                            logger.debug("ä¸‹è½½æ–‡ä»¶è¿‡å¤§ï¼Œåœæ­¢ä¸‹è½½")
                            return False
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½
            if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                return True
            else:
                return False
            
        except requests.exceptions.Timeout:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½è¶…æ—¶: {img_url}")
            return False
        except requests.exceptions.RequestException as e:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½ç½‘ç»œé”™è¯¯: {e}")
            return False
        except Exception as e:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def _handle_captcha_if_present(self) -> bool:
        """å¤„ç†éªŒè¯ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰- å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›True"""
        try:
            # ç”¨æˆ·è¦æ±‚ç¦ç”¨éªŒè¯ç æ£€æµ‹ï¼Œç›´æ¥è¿”å›Trueç»§ç»­æ‰§è¡Œ
            logger.debug("ğŸ“‹ éªŒè¯ç æ£€æµ‹å·²ç¦ç”¨ï¼Œç›´æ¥ç»§ç»­...")
            return True
                
        except Exception as e:
            logger.debug(f"éªŒè¯ç å¤„ç†è·³è¿‡: {e}")
            return True  # ç»§ç»­æ‰§è¡Œ
    
    def extract_full_article_content(self, url: str, download_images: bool = True) -> dict:
        """
        æå–å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬HTMLå†…å®¹å’Œå›¾ç‰‡
        
        Args:
            url: æ–‡ç« URL
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„å­—å…¸
        """
        try:
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}

            logger.info(f"ğŸš€ å¼€å§‹æå–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 10)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­æå–")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # æ»šåŠ¨åŠ è½½å®Œæ•´å†…å®¹
            self._human_like_scroll_and_load()
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = self._extract_article_content()
            if 'error' in basic_info:
                return basic_info
            
            # æå–å®Œæ•´HTMLå†…å®¹
            html_content = self._extract_html_content()
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date()
            
            # å¤„ç†å›¾ç‰‡
            images_info = []
            if download_images:
                images_info = self._extract_and_download_images()
            
            # æ„å»ºå®Œæ•´çš„æ–‡ç« ä¿¡æ¯
            full_article = {
                'title': basic_info.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                'author': basic_info.get('author', 'æœªçŸ¥ä½œè€…'),
                'url': basic_info.get('url', url),
                'publish_date': publish_date,
                'html_content': html_content,
                'text_content': self._html_to_text(html_content),
                'images': images_info,
                'extraction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {full_article['title'][:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ–‡å­—é•¿åº¦={len(full_article['text_content'])}, å›¾ç‰‡æ•°é‡={len(images_info)}")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ æå–å®Œæ•´æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}

    def _wait_for_basic_page_load(self) -> bool:
        """
        ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆç§»é™¤ä¸å¿…è¦ç­‰å¾…ï¼‰
        """
        try:
            logger.info("ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½...")
            
            # å‡å°‘ç­‰å¾…æ—¶é—´åˆ°æœ€å¤š5ç§’
            wait = WebDriverWait(self.driver, 5)
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ åŠ è½½
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # ç­‰å¾…æ–‡ç« å†…å®¹åŠ è½½ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 
                    "#js_content, .rich_media_content, .article-content, .content")))
            except:
                logger.debug("æœªæ‰¾åˆ°å¸¸è§æ–‡ç« å†…å®¹å®¹å™¨ï¼Œä½¿ç”¨é€šç”¨æ£€æŸ¥")
            
            # ç§»é™¤ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´ - é¡µé¢åŠ è½½å®Œæˆåç«‹å³ç»§ç»­
            logger.success("é¡µé¢åŸºç¡€å†…å®¹åŠ è½½å®Œæˆ")
            return True
            
        except TimeoutException:
            logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å¤„ç†")
            return True
        except Exception as e:
            logger.error(f"ç­‰å¾…é¡µé¢åŠ è½½æ—¶å‡ºé”™: {e}")
            return False

    def _human_like_scroll_and_load(self, target_url: str = None) -> bool:
        """
        äººç±»å¼æ»šåŠ¨åŠ è½½å†…å®¹ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå‡å°‘é‡å¤æ»šåŠ¨ï¼‰
        """
        try:
            # è·å–é¡µé¢æ€»é«˜åº¦
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            logger.info(f"å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½ï¼Œé¡µé¢é«˜åº¦: {total_height}px")
            
            # å¦‚æœé¡µé¢å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
            if total_height <= 1000:
                logger.debug("é¡µé¢è¾ƒçŸ­ï¼Œæ— éœ€æ»šåŠ¨")
                return True
            
            # æ ¹æ®é¡µé¢é«˜åº¦åŠ¨æ€è°ƒæ•´æ»šåŠ¨ç­–ç•¥
            if total_height <= 3000:
                # çŸ­é¡µé¢ï¼šå¿«é€Ÿæ»šåŠ¨
                pixels_per_step = 800
                scroll_delay = 1.0
                check_frequency = 3
            elif total_height <= 8000:
                # ä¸­ç­‰é¡µé¢ï¼šå¹³è¡¡æ»šåŠ¨
                pixels_per_step = 1000
                scroll_delay = 1.2
                check_frequency = 4
            else:
                # é•¿é¡µé¢ï¼šå¤§æ­¥æ»šåŠ¨
                pixels_per_step = 1500
                scroll_delay = 1.5
                check_frequency = 5
            
            # è®¡ç®—æ»šåŠ¨æ­¥æ•°
            scroll_positions = max(3, (total_height // pixels_per_step) + 1)
            scroll_positions = min(scroll_positions, 8)  # æœ€å¤š8ä¸ªä½ç½®ï¼Œé¿å…è¿‡åº¦æ»šåŠ¨
            
            logger.info(f"æ»šåŠ¨ç­–ç•¥: {scroll_positions}æ­¥, æ¯æ­¥{pixels_per_step}px, å»¶è¿Ÿ{scroll_delay}s")
            
            # æ‰§è¡Œæ»šåŠ¨
            for i in range(scroll_positions):
                # è®¡ç®—æ»šåŠ¨ä½ç½®
                if scroll_positions == 1:
                    scroll_to = total_height
                else:
                    progress = i / (scroll_positions - 1)
                    scroll_to = int(total_height * progress)
                
                logger.debug(f"æ»šåŠ¨åˆ°ä½ç½® {i+1}/{scroll_positions}: {scroll_to}px")
                
                # å¹³æ»‘æ»šåŠ¨åˆ°ç›®æ ‡ä½ç½®
                self.driver.execute_script(f"window.scrollTo({{top: {scroll_to}, behavior: 'smooth'}});")
                
                # ç­‰å¾…å†…å®¹åŠ è½½
                time.sleep(scroll_delay)
                
                # å‡å°‘å›¾ç‰‡æ£€æŸ¥é¢‘ç‡
                if i % check_frequency == 0:
                    self._trigger_image_loading()
                    time.sleep(0.5)
            
            # æœ€ç»ˆå¤„ç†ï¼šå¿«é€Ÿæ»šåŠ¨åˆ°åº•éƒ¨
            logger.info("æœ€ç»ˆå¤„ç†...")
            self.driver.execute_script("window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});")
            time.sleep(1.5)
            
            # æœ€åä¸€æ¬¡å›¾ç‰‡æ£€æŸ¥
            self._trigger_image_loading()
            time.sleep(1.0)
            
            # å›åˆ°é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo({top: 0, behavior: 'smooth'});")
            time.sleep(0.5)
            
            logger.success("æ™ºèƒ½æ»šåŠ¨å®Œæˆ")
            return True
            
        except Exception as e:
            logger.warning(f"æ»šåŠ¨è¿‡ç¨‹å¼‚å¸¸: {e}")
            return True  # å³ä½¿å‡ºé”™ä¹Ÿç»§ç»­ï¼Œä¸å½±å“å†…å®¹æå–

    def _trigger_image_loading(self):
        """è§¦å‘å›¾ç‰‡æ‡’åŠ è½½"""
        try:
            # è§¦å‘å„ç§æ‡’åŠ è½½æœºåˆ¶
            self.driver.execute_script("""
                // è§¦å‘æ‡’åŠ è½½å›¾ç‰‡
                var images = document.querySelectorAll('img[data-src], img[data-lazy-src], img[data-original]');
                images.forEach(function(img) {
                    if (img.dataset.src) img.src = img.dataset.src;
                    if (img.dataset.lazySrc) img.src = img.dataset.lazySrc;
                    if (img.dataset.original) img.src = img.dataset.original;
                    // è§¦å‘onloadäº‹ä»¶
                    img.onload = function() { console.log('Image loaded:', this.src); };
                });
                
                // è§¦å‘æ»šåŠ¨äº‹ä»¶ï¼Œå¯èƒ½æ¿€æ´»æŸäº›æ‡’åŠ è½½è„šæœ¬
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
            """)
            
            logger.debug("è§¦å‘å›¾ç‰‡æ‡’åŠ è½½")
            
        except Exception as e:
            logger.debug(f"è§¦å‘å›¾ç‰‡æ‡’åŠ è½½å¤±è´¥: {e}")

    def _extract_html_content(self) -> str:
        """æå–æ–‡ç« çš„HTMLå†…å®¹"""
        try:
            # å¾®ä¿¡æ–‡ç« å†…å®¹çš„å¸¸è§é€‰æ‹©å™¨
            content_selectors = [
                '#js_content',
                '.rich_media_content', 
                '.appmsg_content_text',
                '.article_content',
                '[data-role="article-content"]'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if content_elem:
                        # è·å–å…ƒç´ çš„HTMLå†…å®¹
                        html_content = content_elem.get_attribute('innerHTML')
                        if html_content and len(html_content.strip()) > 100:
                            logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                            return html_content
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•è·å–æ•´ä¸ªbody
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨å†…å®¹å®¹å™¨ï¼Œå°è¯•æå–bodyå†…å®¹")
            body_elem = self.driver.find_element(By.TAG_NAME, "body")
            return body_elem.get_attribute('innerHTML') or ""
            
        except Exception as e:
            logger.error(f"æå–HTMLå†…å®¹å¤±è´¥: {e}")
            return ""

    def _extract_publish_date(self) -> str:
        """æå–æ–‡ç« å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„å¸¸è§é€‰æ‹©å™¨
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '.publish_time',
                '[data-role="publish-time"]',
                '.time'
            ]
            
            for selector in date_selectors:
                try:
                    date_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if date_elem and date_elem.text.strip():
                        date_text = date_elem.text.strip()
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                        return date_text
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»é¡µé¢æºç ä¸­æå–
            page_source = self.driver.page_source
            import re
            
            # å°è¯•åŒ¹é…å¸¸è§çš„æ—¶é—´æ ¼å¼
            date_patterns = [
                r'publish_time["\s]*[=:]["\s]*("[^"]+)"',
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{1,2}æœˆ\d{1,2}æ—¥)'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_source)
                if match:
                    date_text = match.group(1)
                    logger.debug(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return datetime.now().strftime('%Y-%m-%d')

    def _extract_and_download_images(self) -> List[Dict[str, str]]:
        """æå–å¹¶ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡"""
        try:
            images_info = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_elements)} å¼ å›¾ç‰‡")
            
            for i, img_elem in enumerate(img_elements):
                try:
                    # è·å–å›¾ç‰‡URL
                    img_src = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
                    
                    if not img_src or img_src.startswith('data:'):
                        continue  # è·³è¿‡base64å›¾ç‰‡å’Œæ— æ•ˆå›¾ç‰‡
                    
                    # è·å–å›¾ç‰‡çš„altå±æ€§ä½œä¸ºæè¿°
                    img_alt = img_elem.get_attribute('alt') or f"å›¾ç‰‡_{i+1}"
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"image_{i+1:03d}.jpg"
                    
                    # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
                    images_dir = os.path.join("output", "images", 
                                            datetime.now().strftime('%Y%m%d'))
                    os.makedirs(images_dir, exist_ok=True)
                    
                    # æœ¬åœ°ä¿å­˜è·¯å¾„
                    local_path = os.path.join(images_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image(img_src, local_path):
                        images_info.append({
                            'original_url': img_src,
                            'local_path': local_path,
                            'filename': img_filename,
                            'alt': img_alt,
                            'index': i + 1
                        })
                        logger.debug(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {img_filename}")
                    else:
                        logger.warning(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(images_info)}/{len(img_elements)}")
            return images_info
            
        except Exception as e:
            logger.error(f"æå–å’Œä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return []

    def _download_image(self, img_url: str, save_path: str) -> bool:
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = 'https://mp.weixin.qq.com' + img_url
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/'
            }
            
            response = requests.get(img_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False

    def _html_to_text(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
        try:
            from bs4 import BeautifulSoup
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for element in soup(["script", "style"]):
                element.decompose()
            
            # è·å–çº¯æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            clean_text = '\n'.join(lines)
            
            return clean_text
            
        except Exception as e:
            logger.error(f"HTMLè½¬æ–‡æœ¬å¤±è´¥: {e}")
            return ""

    def save_as_json(self, url: str, output_path: str) -> bool:
        """ä¿å­˜URLä¸ºJSONæ ¼å¼"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜JSONæ–‡æ¡£: {url}")
            
            # è·å–æ–‡ç« å†…å®¹
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # æ„å»ºJSONæ•°æ®
            content_soup = article_data.get('content_soup')
            content_text = ""
            content_html = ""
            
            if content_soup:
                content_text = content_soup.get_text(strip=True)
                content_html = str(content_soup)
            
            json_data = {
                "title": article_data.get('title', ''),
                "author": article_data.get('author', ''),
                "publish_date": article_data.get('publish_date', ''),
                "url": url,
                "content_text": content_text,
                "content_html": content_html,
                "images": article_data.get('images', []),
                "extracted_at": datetime.now().isoformat(),
                "word_count": len(content_text) if content_text else 0
            }
            
            # ä¿å­˜æ–‡ä»¶
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.success(f"JSONæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡æ¡£å¤±è´¥: {e}")
            return False

    def save_as_markdown(self, url: str, output_path: str) -> bool:
        """ä¿å­˜URLä¸ºMarkdownæ ¼å¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜Markdownæ–‡æ¡£: {url}")
            
            # ä½¿ç”¨å·²æœ‰çš„æ–‡ç« æå–æ–¹æ³•ï¼Œé¿å…é‡å¤æ»šåŠ¨
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # ç”ŸæˆMarkdownå†…å®¹
            markdown_content = []
            
            # æ·»åŠ æ ‡é¢˜
            title = article_data.get('title', 'å¾®ä¿¡æ–‡ç« ')
            markdown_content.append(f"# {title}\n")
            
            # æ·»åŠ å…ƒä¿¡æ¯
            author = article_data.get('author', 'æœªçŸ¥')
            publish_date = article_data.get('publish_date', 'æœªçŸ¥')
            markdown_content.append(f"**ä½œè€…**: {author}")
            markdown_content.append(f"**å‘å¸ƒæ—¶é—´**: {publish_date}")
            markdown_content.append(f"**åŸæ–‡é“¾æ¥**: {url}")
            markdown_content.append("\n---\n")
            
            # è½¬æ¢HTMLå†…å®¹ä¸ºMarkdown
            content_soup = article_data.get('content_soup')
            if content_soup:
                markdown_text = self._convert_soup_to_markdown(content_soup)
                markdown_content.append(markdown_text)
            else:
                markdown_content.append("æ— æ³•æå–æ–‡ç« å†…å®¹")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜Markdownæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(markdown_content))
            
            logger.success(f"Markdownæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜Markdownæ–‡æ¡£å¤±è´¥: {e}")
            return False

    def _convert_soup_to_markdown(self, soup: BeautifulSoup) -> str:
        """å°†BeautifulSoupå¯¹è±¡è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        try:
            markdown_lines = []
            
            for element in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'img', 'strong', 'b', 'em', 'i']):
                if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    level = int(element.name[1])
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"{'#' * level} {text}\n")
                        
                elif element.name == 'img':
                    src = element.get('src', '')
                    alt = element.get('alt', 'å›¾ç‰‡')
                    if src:
                        markdown_lines.append(f"![{alt}]({src})\n")
                        
                elif element.name in ['strong', 'b']:
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"**{text}**")
                        
                elif element.name in ['em', 'i']:
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"*{text}*")
                        
                elif element.name in ['p', 'div']:
                    text = element.get_text(strip=True)
                    if text and len(text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        markdown_lines.append(f"{text}\n")
            
            return '\n'.join(markdown_lines)
            
        except Exception as e:
            logger.error(f"è½¬æ¢Markdownå¤±è´¥: {e}")
            return soup.get_text(strip=True) if soup else ""

    def save_complete_html(self, url: str, output_path: str) -> bool:
        """ä¿å­˜å®Œæ•´çš„HTMLæ–‡ä»¶ï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œæ ·å¼ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜å®Œæ•´HTML: {url}")
            
            # ä½¿ç”¨å·²æœ‰çš„æ–‡ç« æå–æ–¹æ³•ï¼Œé¿å…é‡å¤æ»šåŠ¨
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # è·å–æ–‡ç« ä¿¡æ¯
            title = article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
            author = article_data.get('author', 'æœªçŸ¥ä½œè€…')
            publish_date = article_data.get('publish_date', 'æœªçŸ¥æ—¶é—´')
            content_soup = article_data.get('content_soup')
            images = article_data.get('images', [])
            
            if not content_soup:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« å†…å®¹")
                return False
            
            # åˆ›å»ºå®Œæ•´çš„HTMLæ–‡æ¡£
            html_doc = BeautifulSoup('<!DOCTYPE html><html><head></head><body></body></html>', 'html.parser')
            
            # è®¾ç½®HTMLå¤´éƒ¨
            html_doc.head.append(html_doc.new_tag('meta', charset='utf-8'))
            html_doc.head.append(html_doc.new_tag('meta', attrs={'name': 'viewport', 'content': 'width=device-width, initial-scale=1.0'}))
            
            title_tag = html_doc.new_tag('title')
            title_tag.string = title
            html_doc.head.append(title_tag)
            
            # æ·»åŠ CSSæ ·å¼
            style_tag = html_doc.new_tag('style')
            style_tag.string = """
                body { 
                    max-width: 800px; 
                    margin: 0 auto; 
                    padding: 20px; 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; 
                    line-height: 1.6;
                    color: #333;
                }
                .article-header {
                    text-align: center;
                    margin-bottom: 30px;
                    padding-bottom: 20px;
                    border-bottom: 1px solid #eee;
                }
                .article-title {
                    font-size: 24px;
                    font-weight: bold;
                    margin-bottom: 10px;
                    color: #2c3e50;
                }
                .article-meta {
                    color: #666;
                    font-size: 14px;
                }
                .article-content {
                    font-size: 16px;
                    line-height: 1.8;
                }
                img { 
                    max-width: 100%; 
                    height: auto; 
                    display: block;
                    margin: 15px auto;
                    border-radius: 4px;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                }
                p { margin: 15px 0; }
                blockquote {
                    border-left: 4px solid #3498db;
                    margin: 20px 0;
                    padding-left: 15px;
                    color: #666;
                    font-style: italic;
                }
                .image-placeholder {
                    background: #f8f9fa;
                    border: 2px dashed #dee2e6;
                    padding: 20px;
                    text-align: center;
                    color: #6c757d;
                    margin: 15px 0;
                    border-radius: 4px;
                }
            """
            html_doc.head.append(style_tag)
            
            # åˆ›å»ºæ–‡ç« å¤´éƒ¨
            header_div = html_doc.new_tag('div', class_='article-header')
            
            title_h1 = html_doc.new_tag('h1', class_='article-title')
            title_h1.string = title
            header_div.append(title_h1)
            
            meta_div = html_doc.new_tag('div', class_='article-meta')
            meta_div.string = f"ä½œè€…: {author} | å‘å¸ƒæ—¶é—´: {publish_date} | åŸæ–‡é“¾æ¥: {url}"
            header_div.append(meta_div)
            
            html_doc.body.append(header_div)
            
            # åˆ›å»ºæ–‡ç« å†…å®¹å®¹å™¨
            content_div = html_doc.new_tag('div', class_='article-content')
            
            # å¤„ç†å›¾ç‰‡å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
            base_dir = os.path.dirname(output_path)
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:20]
            images_dir = os.path.join(base_dir, f"images_{safe_title}")
            os.makedirs(images_dir, exist_ok=True)
            
            # æ›¿æ¢å›¾ç‰‡é“¾æ¥
            img_count = 0
            for img_tag in content_soup.find_all('img'):
                img_src = img_tag.get('src') or img_tag.get('data-src')
                if img_src:
                    # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
                    local_img_path = None
                    for img_info in images:
                        if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                            if img_info.get('local_path') and os.path.exists(img_info['local_path']):
                                # å¤åˆ¶å›¾ç‰‡åˆ°HTMLç›®å½•
                                img_filename = f"img_{img_count:03d}.jpg"
                                new_img_path = os.path.join(images_dir, img_filename)
                                try:
                                    import shutil
                                    shutil.copy2(img_info['local_path'], new_img_path)
                                    local_img_path = f"images_{safe_title}/{img_filename}"
                                    img_count += 1
                                except Exception as e:
                                    logger.debug(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {e}")
                                break
                    
                    if local_img_path:
                        img_tag['src'] = local_img_path
                        # æ¸…ç†å…¶ä»–å±æ€§
                        for attr in ['data-src', 'data-original', 'data-lazy-src']:
                            if img_tag.get(attr):
                                del img_tag[attr]
                    else:
                        # åˆ›å»ºå›¾ç‰‡å ä½ç¬¦
                        placeholder_div = html_doc.new_tag('div', class_='image-placeholder')
                        placeholder_div.string = f"[å›¾ç‰‡: {img_src}]"
                        img_tag.replace_with(placeholder_div)
            
            # å°†å¤„ç†åçš„å†…å®¹æ·»åŠ åˆ°HTMLæ–‡æ¡£
            for element in content_soup.children:
                if hasattr(element, 'name'):
                    content_div.append(element)
            
            html_doc.body.append(content_div)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(str(html_doc.prettify()))
            
            logger.success(f"å®Œæ•´HTMLå·²ä¿å­˜: {output_path}")
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å›¾ç‰‡={img_count}å¼ , å›¾ç‰‡ç›®å½•={images_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å®Œæ•´HTMLå¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False

    def process_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†URLåˆ—è¡¨"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURL...")
            
            articles = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="å¤„ç†URL")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        articles.append({
                            'title': f"æ— æ•ˆURL_{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': "æ— æ•ˆçš„URLæ ¼å¼",
                            'url': url
                        })
                        continue
                    
                    # æå–æ–‡ç« ä¿¡æ¯
                    article_info = self.extract_article_info(url)
                    
                    if 'error' not in article_info:
                        articles.append(article_info)
                        if len(article_info.get('content', '').strip()) > 100:
                            success_count += 1
                    else:
                        logger.error(f"å¤„ç†URLå¤±è´¥: {article_info.get('error')}")
                        articles.append({
                            'title': f"é”™è¯¯_æ–‡ç« _{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': f"å¤„ç†å¤±è´¥: {article_info.get('error')}",
                            'url': url
                        })
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    if i < len(urls) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªURL
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
                    articles.append({
                        'title': f"å¼‚å¸¸_æ–‡ç« _{i+1}",
                        'author': "æœªçŸ¥",
                        'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'content': f"å¤„ç†å¼‚å¸¸: {str(e)}",
                        'url': url
                    })
            
            logger.success(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(articles)}")
            return articles
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return []

    def process_urls_as_pdf(self, urls: List[str], output_dir: str = "output/pdf") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºPDFæ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºPDF...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆPDF")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆPDFæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    pdf_filename = f"{safe_title}.pdf"
                    pdf_path = os.path.join(output_dir, pdf_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = pdf_path
                    while os.path.exists(pdf_path):
                        name_without_ext = safe_title
                        pdf_filename = f"{name_without_ext}_{counter}.pdf"
                        pdf_path = os.path.join(output_dir, pdf_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºPDF
                    if self.save_as_pdf(url, pdf_path):
                        saved_files.append(pdf_path)
                        success_count += 1
                        logger.success(f"PDFå·²ä¿å­˜: {pdf_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"PDFç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡PDFç”Ÿæˆå¤±è´¥: {e}")
            return []

    def process_urls_as_docx(self, urls: List[str], output_dir: str = "output/docx") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£æ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆWordæ–‡æ¡£")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆWordæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    docx_filename = f"{safe_title}.docx"
                    docx_path = os.path.join(output_dir, docx_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = docx_path
                    while os.path.exists(docx_path):
                        name_without_ext = safe_title
                        docx_filename = f"{name_without_ext}_{counter}.docx"
                        docx_path = os.path.join(output_dir, docx_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºWordæ–‡æ¡£
                    if self.save_as_docx(url, docx_path):
                        saved_files.append(docx_path)
                        success_count += 1
                        logger.success(f"Wordæ–‡æ¡£å·²ä¿å­˜: {docx_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"Wordæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡Wordæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            return []

    def _is_valid_wechat_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¾®ä¿¡æ–‡ç« URL"""
        try:
            # æ£€æŸ¥åŸºæœ¬æ ¼å¼
            if not url or not url.startswith('http'):
                return False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡æ–‡ç« URL
            if 'mp.weixin.qq.com/s' not in url:
                return False
            
            return True
            
        except Exception:
            return False

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
                logger.debug("æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
        finally:
            self.driver = None

    def quick_access_check(self, url: str) -> bool:
        """
        å¿«é€Ÿæ£€æŸ¥æ˜¯å¦èƒ½æ­£å¸¸è®¿é—®URLï¼Œé¿å…å¤æ‚ç™»å½•æµç¨‹
        """
        try:
            logger.info(f"ğŸ” å¿«é€Ÿæ£€æŸ¥URLè®¿é—®: {url}")
            
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return False
            
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            original_timeout = self.driver.implicitly_wait(5)
            
            try:
                self.driver.get(url)
                
                # å¿«é€Ÿæ£€æŸ¥é¡µé¢æ˜¯å¦æˆåŠŸåŠ è½½
                wait = WebDriverWait(self.driver, 5)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„æ–‡ç« å†…å®¹
                article_indicators = [
                    "js_content",
                    "rich_media_content", 
                    "activity-name",
                    "rich_media_title"
                ]
                
                page_source = self.driver.page_source.lower()
                has_content = any(indicator in page_source for indicator in article_indicators)
                
                if has_content:
                    logger.info("âœ… URLè®¿é—®æ­£å¸¸ï¼ŒåŒ…å«æ–‡ç« å†…å®¹")
                    return True
                else:
                    logger.warning("âš ï¸ URLå¯è®¿é—®ä½†å†…å®¹ä¸å®Œæ•´ï¼Œå¯èƒ½éœ€è¦ç™»å½•")
                    return False
                    
            except TimeoutException:
                logger.warning("â° URLè®¿é—®è¶…æ—¶")
                return False
            finally:
                # æ¢å¤åŸå§‹è¶…æ—¶è®¾ç½®
                self.driver.implicitly_wait(original_timeout)
                
        except Exception as e:
            logger.error(f"URLè®¿é—®æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def simple_article_access(self, url: str) -> bool:
        """
        ç®€åŒ–çš„æ–‡ç« è®¿é—®æ–¹æ³•ï¼Œé¿å…å¤æ‚ç™»å½•æµç¨‹
        """
        try:
            logger.info(f"ğŸš€ å°è¯•ç®€åŒ–è®¿é—®: {url}")
            
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return False
            
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶
            self.driver.set_page_load_timeout(10)
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç 
            try:
                captcha_elements = self.driver.find_elements(By.CLASS_NAME, "weui-mask")
                if captcha_elements:
                    logger.warning("ğŸ”’ æ£€æµ‹åˆ°å¯èƒ½çš„éªŒè¯ç ï¼Œå°è¯•å…³é—­")
                    for elem in captcha_elements:
                        try:
                            if elem.is_displayed():
                                # å°è¯•ç‚¹å‡»å…³é—­æŒ‰é’®
                                close_btn = elem.find_element(By.TAG_NAME, "a")
                                if close_btn:
                                    close_btn.click()
                                    time.sleep(0.5)
                        except:
                            continue
            except:
                pass
            
            # å¿«é€Ÿæ£€æŸ¥é¡µé¢æ˜¯å¦åŠ è½½æˆåŠŸ
            try:
                wait = WebDriverWait(self.driver, 3)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ç« å†…å®¹æ ‡è¯†
                content_indicators = [
                    By.ID, "js_content",
                    By.CLASS_NAME, "rich_media_content",
                    By.CLASS_NAME, "rich_media_title"
                ]
                
                has_content = False
                for by_type, selector in zip([By.ID, By.CLASS_NAME, By.CLASS_NAME], 
                                           ["js_content", "rich_media_content", "rich_media_title"]):
                    try:
                        elements = self.driver.find_elements(by_type, selector)
                        if elements and any(elem.is_displayed() for elem in elements):
                            has_content = True
                            break
                    except:
                        continue
                
                if has_content:
                    logger.info("âœ… æ–‡ç« å†…å®¹è®¿é—®æˆåŠŸ")
                    return True
                else:
                    logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°æ–‡ç« å†…å®¹ï¼Œå¯èƒ½éœ€è¦ç™»å½•")
                    return False
                    
            except TimeoutException:
                logger.warning("â° é¡µé¢åŠ è½½è¶…æ—¶")
                return False
                
        except Exception as e:
            logger.error(f"ç®€åŒ–è®¿é—®å¤±è´¥: {e}")
            return False

    def _extract_article_content(self) -> dict:
        """æå–æ–‡ç« çš„åŸºæœ¬å†…å®¹ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜
            title_selectors = [
                'h1.rich_media_title',
                '#activity-name',
                '.appmsg_title',
                'h1',
                '.title'
            ]
            
            title = "æœªçŸ¥æ ‡é¢˜"
            for selector in title_selectors:
                try:
                    title_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_elem and title_elem.text.strip():
                        title = title_elem.text.strip()
                        break
                except:
                    continue
            
            # æå–ä½œè€…
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '.author',
                '#js_author_name'
            ]
            
            author = "æœªçŸ¥ä½œè€…"
            for selector in author_selectors:
                try:
                    author_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if author_elem and author_elem.text.strip():
                        author = author_elem.text.strip()
                        break
                except:
                    continue
            
            # è·å–å½“å‰URL
            current_url = self.driver.current_url
            
            return {
                'title': title,
                'author': author,
                'url': current_url
            }
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}


def main():
    """æµ‹è¯•å·¥å…·"""
    # ç¤ºä¾‹URL
    test_url = "https://mp.weixin.qq.com/s/LiS8ytwKsKP9yAHGp96G_Q"
    
    scraper = SimpleUrlScraper(headless=False)  # ä½¿ç”¨å¯è§æ¨¡å¼ä¾¿äºå¤„ç†éªŒè¯ç 
    
    try:
        # æµ‹è¯•PDFç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_path = f"output/pdf/test_article_{timestamp}.pdf"
        
        if scraper.save_as_pdf(test_url, pdf_path):
            logger.success(f"PDFæµ‹è¯•æˆåŠŸ: {pdf_path}")
        
        # æµ‹è¯•å®Œæ•´HTMLä¿å­˜
        html_path = f"output/html/test_article_{timestamp}.html"
        
        if scraper.save_complete_html(test_url, html_path):
            logger.success(f"å®Œæ•´HTMLæµ‹è¯•æˆåŠŸ: {html_path}")
        
        # æµ‹è¯•åŸºæœ¬æ–‡ç« ä¿¡æ¯æå–
        article = scraper.extract_article_info(test_url)
        
        if 'error' not in article:
            logger.success(f"æ–‡ç« ä¿¡æ¯æå–æˆåŠŸ: {article.get('title')}")
            
        else:
            logger.error(f"å¤„ç†å¤±è´¥: {article.get('error')}")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        scraper.cleanup()


if __name__ == "__main__":
    main()
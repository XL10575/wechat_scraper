#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple WeChat URL Scraper
ç®€å•å¾®ä¿¡æ–‡ç« URLå¤„ç†å·¥å…·

ç›´æ¥å¤„ç†ç”¨æˆ·æä¾›çš„å¾®ä¿¡æ–‡ç« URLï¼Œè·å–å†…å®¹å¹¶ä¿å­˜ä¸ºé£ä¹¦æ ¼å¼
"""

import time
import random
import json
import re
import base64
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement, qn

from typing import List, Dict, Any, Optional
from loguru import logger
from datetime import datetime
import os

from utils import clean_text




class SimpleUrlScraper:
    """ç®€å•URLå¤„ç†å·¥å…·"""
    
    def __init__(self, headless: bool = False):
        """
        åˆå§‹åŒ–å·¥å…· - å¿«é€Ÿæ¨¡å¼ï¼Œä¸ç«‹å³å¯åŠ¨æµè§ˆå™¨
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        self.driver = None
        self.headless = headless
        
        logger.info("ç®€å•URLå¤„ç†å·¥å…·åˆå§‹åŒ–å®Œæˆï¼ˆæµè§ˆå™¨å°†æ‡’åŠ è½½ï¼‰")
    
    def setup_browser(self, headless: bool = True) -> Optional[webdriver.Chrome]:
        """è®¾ç½®Chromeæµè§ˆå™¨ - é’ˆå¯¹exeç¯å¢ƒä¼˜åŒ–"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆexeä¼˜åŒ–æ¨¡å¼ï¼‰...")
            
            # Chromeé€‰é¡¹é…ç½® - é’ˆå¯¹exeç¯å¢ƒä¼˜åŒ–
            chrome_options = Options()
            
            # åŸºç¡€é€‰é¡¹
            if headless:
                chrome_options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„headlessæ¨¡å¼
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # é‡è¦ï¼šä¸ç¦ç”¨JavaScriptå’ŒCSSï¼ŒéªŒè¯ç éœ€è¦è¿™äº›
            # chrome_options.add_argument("--disable-javascript")  # æ³¨é‡Šæ‰
            # chrome_options.add_argument("--disable-css")  # æ³¨é‡Šæ‰
            
            # åæ£€æµ‹é…ç½® - é’ˆå¯¹exeç¯å¢ƒ
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç¯å¢ƒ
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--allow-running-insecure-content")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor")
            
            # exeç¯å¢ƒç‰¹æ®Šé…ç½®
            import sys
            if getattr(sys, 'frozen', False):  # æ£€æµ‹æ˜¯å¦ä¸ºexeç¯å¢ƒ
                logger.info("ğŸ”§ æ£€æµ‹åˆ°exeç¯å¢ƒï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                chrome_options.add_argument("--disable-logging")
                chrome_options.add_argument("--disable-gpu-logging")
                chrome_options.add_argument("--silent")
                chrome_options.add_argument("--no-first-run")
                chrome_options.add_argument("--no-default-browser-check")
            
            # GitHub Actionsç‰¹æ®Šé…ç½®
            if os.getenv('GITHUB_ACTIONS'):
                logger.info("ğŸ”§ æ£€æµ‹åˆ°GitHub Actionsç¯å¢ƒï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                chrome_options.add_argument("--no-first-run")
                chrome_options.add_argument("--no-default-browser-check")
                chrome_options.add_argument("--disable-default-apps")
                chrome_options.add_argument("--disable-background-timer-throttling")
                chrome_options.add_argument("--disable-backgrounding-occluded-windows")
                chrome_options.add_argument("--disable-renderer-backgrounding")
                chrome_options.add_argument("--disable-features=TranslateUI")
                chrome_options.add_argument("--disable-ipc-flooding-protection")
                chrome_options.add_argument("--single-process")
                chrome_options.add_argument("--remote-debugging-port=9222")
                
                # è®¾ç½®ç”¨æˆ·æ•°æ®ç›®å½•
                user_data_dir = "/tmp/chrome-user-data"
                os.makedirs(user_data_dir, exist_ok=True)
                chrome_options.add_argument(f"--user-data-dir={user_data_dir}")
            
            # ç”¨æˆ·ä»£ç† - ä½¿ç”¨Windows Chrome
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            chrome_options.add_argument(f"--user-agent={user_agent}")
            
            # é¡µé¢åŠ è½½ç­–ç•¥
            chrome_options.page_load_strategy = 'normal'  # æ”¹ä¸ºnormalä»¥ç¡®ä¿éªŒè¯ç åŠ è½½
            
            # è¶…æ—¶è®¾ç½®
            chrome_options.add_argument("--timeout=60000")  # å»¶é•¿è¶…æ—¶æ—¶é—´
            
            # æ—¥å¿—é…ç½®
            chrome_options.add_argument("--log-level=3")
            
            # æ€§èƒ½ä¼˜åŒ– - ä½†ä¸ç¦ç”¨å›¾ç‰‡ï¼ˆéªŒè¯ç éœ€è¦ï¼‰
            prefs = {
                "profile.default_content_setting_values": {
                    "plugins": 2,
                    "popups": 2,
                    "geolocation": 2,
                    "notifications": 2,
                    "media_stream": 2,
                },
                # ä¸ç¦ç”¨å›¾ç‰‡ï¼ŒéªŒè¯ç éœ€è¦å›¾ç‰‡åŠ è½½
                "profile.managed_default_content_settings": {
                    # "images": 2  # æ³¨é‡Šæ‰ï¼Œå…è®¸å›¾ç‰‡åŠ è½½
                }
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            # WebDriver Manageré…ç½®
            service = None
            try:
                logger.info("âš¡ å¯åŠ¨Chromeæµè§ˆå™¨...")
                
                # åœ¨GitHub Actionsç¯å¢ƒä¸­ï¼Œå°è¯•ä½¿ç”¨ç³»ç»ŸChrome
                if os.getenv('GITHUB_ACTIONS'):
                    # æ£€æŸ¥ç³»ç»ŸChromeè·¯å¾„
                    chrome_paths = [
                        "/usr/bin/google-chrome",
                        "/usr/bin/google-chrome-stable",
                        "/usr/bin/chromium-browser",
                        "/snap/bin/chromium"
                    ]
                    
                    chrome_binary = None
                    for path in chrome_paths:
                        if os.path.exists(path):
                            chrome_binary = path
                            logger.info(f"ğŸ” æ‰¾åˆ°ChromeäºŒè¿›åˆ¶æ–‡ä»¶: {chrome_binary}")
                            break
                    
                    if chrome_binary:
                        chrome_options.binary_location = chrome_binary
                    
                    # å°è¯•ä½¿ç”¨ç³»ç»Ÿchromedriver
                    chromedriver_paths = [
                        "/usr/bin/chromedriver",
                        "/usr/local/bin/chromedriver"
                    ]
                    
                    chromedriver_path = None
                    for path in chromedriver_paths:
                        if os.path.exists(path):
                            chromedriver_path = path
                            logger.info(f"ğŸ” æ‰¾åˆ°ChromeDriver: {chromedriver_path}")
                            break
                    
                    if chromedriver_path:
                        service = Service(chromedriver_path)
                    else:
                        # ä½¿ç”¨WebDriverManagerä½œä¸ºåå¤‡
                        from webdriver_manager.chrome import ChromeDriverManager
                        service = Service(ChromeDriverManager().install())
                else:
                    # æœ¬åœ°ç¯å¢ƒä½¿ç”¨WebDriverManager
                    from webdriver_manager.chrome import ChromeDriverManager
                    service = Service(ChromeDriverManager().install())
                
                # åˆ›å»ºWebDriverå®ä¾‹
                if service:
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                else:
                    driver = webdriver.Chrome(options=chrome_options)
                
                # è®¾ç½®è¶…æ—¶
                driver.set_page_load_timeout(60)  # å»¶é•¿é¡µé¢åŠ è½½è¶…æ—¶
                driver.implicitly_wait(15)  # å»¶é•¿éšå¼ç­‰å¾…
                
                # åæ£€æµ‹è„šæœ¬æ³¨å…¥
                driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
                driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh', 'en']})")
                
                logger.info("âœ… Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆå·²ä¼˜åŒ–éªŒè¯ç æ”¯æŒï¼‰")
                return driver
                
            except Exception as e:
                logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                
                # å°è¯•é™çº§æ–¹æ¡ˆ
                if not os.getenv('GITHUB_ACTIONS'):
                    logger.info("ğŸ”„ å°è¯•é™çº§æ–¹æ¡ˆ...")
                    try:
                        # ç§»é™¤ä¸€äº›å¯èƒ½æœ‰é—®é¢˜çš„é€‰é¡¹
                        chrome_options = Options()
                        chrome_options.add_argument("--headless")
                        chrome_options.add_argument("--no-sandbox")
                        chrome_options.add_argument("--disable-dev-shm-usage")
                        
                        driver = webdriver.Chrome(options=chrome_options)
                        driver.set_page_load_timeout(30)
                        logger.info("âœ… é™çº§æ–¹æ¡ˆæˆåŠŸ")
                        return driver
                    except Exception as e2:
                        logger.error(f"é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                
                return None
                
        except Exception as e:
            logger.error(f"è®¾ç½®æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_article_info(self, url: str) -> dict:
        """
        å¿«é€Ÿæå–æ–‡ç« ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        try:
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}
            
            logger.info(f"æ­£åœ¨è®¿é—®URL: {url}")
            
            # å¿«é€Ÿè®¿é—®é¡µé¢
            self.driver.get(url)
            
            # å¿«é€Ÿç­‰å¾…ï¼ˆæœ€å¤š2ç§’ï¼‰
            try:
                wait = WebDriverWait(self.driver, 2)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­æå–")
            
            # å¿«é€Ÿæå–æ–‡ç« ä¿¡æ¯
            article_info = self._extract_article_content()
            
            if article_info and 'error' not in article_info:
                return article_info
            else:
                return {"error": "æ–‡ç« ä¿¡æ¯æå–å¤±è´¥"}
                
        except Exception as e:
            logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def save_as_pdf(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºPDFæ–‡ä»¶ï¼ˆåŸå§‹å®ç°ï¼Œæ— é‡è¯•æœºåˆ¶ï¼‰
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜PDF: {url}")
            # 1. ä½¿ç”¨SeleniumåŠ è½½å®Œæ•´é¡µé¢å†…å®¹
            if not self.driver:
                self.driver = self.setup_browser(headless=True)
            self.driver.get(url)
            time.sleep(2)
            # ç­‰å¾…é¡µé¢åŠ è½½
            self._wait_for_basic_page_load()
            self._human_like_scroll_and_load()
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(0.2)
            # æ³¨å…¥CSSæ ·å¼
            css_style = """
            <style>
                @page { margin: 0 !important; padding: 0 !important; size: A4 !important; }
                * { box-sizing: border-box !important; }
                body, html { margin: 0 !important; padding: 0 !important; width: 100% !important; max-width: 100% !important; }
                #js_content, .rich_media_content, .rich_media_area_primary { margin: 0 !important; padding: 5px !important; width: 100% !important; max-width: 100% !important; }
                .rich_media_meta_list, .rich_media_tool, .qr_code_pc_outer, .reward_qrcode_area, .reward_area, #js_pc_qr_code_img, .function_mod, .profile_container, .rich_media_global_msg { display: none !important; }
            </style>
            """
            self.driver.execute_script(f"""
                var style = document.createElement('style');
                style.type = 'text/css';
                style.innerHTML = `{css_style}`;
                document.head.appendChild(style);
            """)
            time.sleep(0.3)
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            pdf_options = {
                'paperFormat': 'A4',
                'printBackground': True,
                'marginTop': 0,
                'marginBottom': 0,
                'marginLeft': 0,
                'marginRight': 0,
                'preferCSSPageSize': True,
                'displayHeaderFooter': False,
                'scale': 1.0,
                'landscape': False,
                'transferMode': 'ReturnAsBase64',
                'generateTaggedPDF': False
            }
            pdf_data = self.driver.execute_cdp_cmd('Page.printToPDF', pdf_options)
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(pdf_data['data']))
            logger.success(f"PDFä¿å­˜æˆåŠŸ: {output_path}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜PDFå¤±è´¥: {e}")
            return False
    
    def save_as_docx(self, url: str, output_path: str) -> bool:
        """
        ä¿å­˜URLä¸ºWordæ–‡æ¡£ - å®Œæ•´å†…å®¹ç‰ˆæœ¬
        ä½¿ç”¨SeleniumåŠ¨æ€åŠ è½½ + BeautifulSoupè§£æç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜Wordæ–‡æ¡£: {url}")
            
            # 1. ä½¿ç”¨SeleniumåŠ è½½å®Œæ•´é¡µé¢å†…å®¹ï¼ˆåŒ…æ‹¬åŠ¨æ€å†…å®¹ï¼‰
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # 2. åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼ï¼ˆå¾®è½¯é›…é»‘ï¼Œ10.5å·å­—ï¼‰
            try:
                from docx.enum.style import WD_STYLE_TYPE
                style = doc.styles.add_style('DefaultParagraph', WD_STYLE_TYPE.PARAGRAPH)
                font = style.font
                font.name = 'å¾®è½¯é›…é»‘'
                font.size = Pt(10.5)
            except:
                logger.debug("è®¾ç½®é»˜è®¤å­—ä½“æ ·å¼å¤±è´¥ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")
            
            # 3. æ·»åŠ æ ‡é¢˜
            title = article_data.get('title', 'å¾®ä¿¡æ–‡ç« ')
            title_paragraph = doc.add_heading(title, level=1)
            title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 4. æ·»åŠ ä½œè€…å’Œå‘å¸ƒæ—¶é—´
            meta_info = f"ä½œè€…: {article_data.get('author', 'æœªçŸ¥')}\n"
            meta_info += f"å‘å¸ƒæ—¶é—´: {article_data.get('publish_date', 'æœªçŸ¥')}\n"
            meta_info += f"åŸæ–‡é“¾æ¥: {url}\n"
            
            meta_paragraph = doc.add_paragraph(meta_info)
            meta_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 5. æ·»åŠ åˆ†éš”çº¿
            doc.add_paragraph("=" * 50).alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # 6. å¤„ç†HTMLå†…å®¹ï¼Œç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½åŒ…å«
            content_soup = article_data.get('content_soup')
            images = article_data.get('images', [])
            
            if content_soup:
                logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å« {len(images)} å¼ å›¾ç‰‡")
                try:
                    # æ·»åŠ è¶…æ—¶ä¿æŠ¤çš„å†…å®¹å¤„ç†
                    import signal
                    
                    def timeout_handler(signum, frame):
                        raise TimeoutError("æ–‡æ¡£å¤„ç†è¶…æ—¶")
                    
                    # è®¾ç½®30ç§’è¶…æ—¶
                    if hasattr(signal, 'SIGALRM'):  # Unixç³»ç»Ÿ
                        signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(30)
                    
                    self._process_wechat_content_to_docx(doc, content_soup, images)
                    
                    if hasattr(signal, 'SIGALRM'):
                        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                        
                except TimeoutError:
                    logger.warning("â° æ–‡æ¡£å¤„ç†è¶…æ—¶ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†")
                    # é™çº§åˆ°ç®€å•æ–‡æœ¬å¤„ç†
                    text_content = content_soup.get_text(strip=True)
                    if text_content:
                        doc.add_paragraph(text_content[:5000])  # é™åˆ¶é•¿åº¦
                except Exception as e:
                    logger.warning(f"âš ï¸ å†…å®¹å¤„ç†å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†")
                    # é™çº§åˆ°ç®€å•æ–‡æœ¬å¤„ç†
                    try:
                        text_content = content_soup.get_text(strip=True)
                        if text_content:
                            doc.add_paragraph(text_content[:5000])  # é™åˆ¶é•¿åº¦
                    except:
                        doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« å†…å®¹")
                doc.add_paragraph("æœªèƒ½æå–åˆ°æ–‡ç« å†…å®¹")
            
            # 7. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # 8. ä¿å­˜æ–‡æ¡£
            try:
                doc.save(output_path)
                logger.success(f"Wordæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
                logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å›¾ç‰‡={len(images)}å¼ , æ–‡ä»¶å¤§å°={os.path.getsize(output_path)/1024:.1f}KB")
                return True
            except Exception as e:
                logger.error(f"ä¿å­˜Wordæ–‡æ¡£æ–‡ä»¶å¤±è´¥: {e}")
                return False
            
        except Exception as e:
            logger.error(f"ä¿å­˜Wordæ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def _extract_wechat_article_with_selenium(self, url: str) -> dict:
        """
        ä½¿ç”¨Seleniumè·å–å®Œæ•´çš„å¾®ä¿¡æ–‡ç« å†…å®¹ï¼ˆåŒ…æ‹¬åŠ¨æ€åŠ è½½å†…å®¹ï¼‰
        """
        try:
            logger.info(f"ğŸš€ ä½¿ç”¨Seleniumè·å–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # ç¡®ä¿æµè§ˆå™¨å·²åˆå§‹åŒ–
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŸºç¡€åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 15)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                logger.debug("âœ… é¡µé¢åŸºç¡€æ¡†æ¶åŠ è½½å®Œæˆ")
            except TimeoutException:
                logger.warning("â° é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­")
            
            # å¤„ç†éªŒè¯ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if not self._handle_captcha_if_present():
                logger.error("âŒ éªŒè¯ç å¤„ç†å¤±è´¥")
                return {"error": "éªŒè¯ç å¤„ç†å¤±è´¥"}
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # å®Œæ•´æ»šåŠ¨åŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹
            self._human_like_scroll_and_load()
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿æ‰€æœ‰åŠ¨æ€å†…å®¹éƒ½å·²åŠ è½½
            logger.info("â³ ç­‰å¾…åŠ¨æ€å†…å®¹å®Œå…¨åŠ è½½...")
            time.sleep(3)
            
            # è·å–å®Œå…¨æ¸²æŸ“åçš„é¡µé¢HTML
            page_source = self.driver.page_source
            
            # ä½¿ç”¨BeautifulSoupè§£æå®Œæ•´çš„HTML
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è°ƒè¯•ï¼šä¿å­˜å®Œæ•´HTML
            debug_html_path = "debug_selenium_page.html"
            with open(debug_html_path, 'w', encoding='utf-8') as f:
                f.write(page_source)
            logger.debug(f"å®Œæ•´HTMLå·²ä¿å­˜åˆ°: {debug_html_path}")
            
            # æå–æ ‡é¢˜ - ä»æ¸²æŸ“åçš„é¡µé¢
            title = self._extract_title_from_selenium_soup(soup)
            
            # æå–ä½œè€…
            author = self._extract_author_from_selenium_soup(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date_from_selenium_soup(soup)
            
            # æå–æ–‡ç« æ­£æ–‡å†…å®¹ - ä»å®Œå…¨åŠ è½½çš„é¡µé¢
            content_soup = self._extract_content_from_selenium_soup(soup)
            
            if not content_soup:
                return {"error": "æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹"}
            
            # ä¸‹è½½å›¾ç‰‡ï¼ˆä½¿ç”¨Seleniumå·²ç»åŠ è½½çš„å›¾ç‰‡ï¼‰
            images = self._download_images_from_selenium_soup(content_soup, url)
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {title[:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ®µè½æ•°={len(content_soup.find_all(['p', 'div']))}, å›¾ç‰‡æ•°é‡={len(images)}")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'content_soup': content_soup,
                'images': images,
                'url': url
            }
            
        except Exception as e:
            logger.error(f"Seleniumæå–æ–‡ç« å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def _extract_title_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ ‡é¢˜"""
        try:
            # å¾®ä¿¡æ–‡ç« æ ‡é¢˜çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            title_selectors = [
                'h1#activity-name',
                'h2.rich_media_title',
                '.rich_media_title',
                'h1.rich_media_title',
                '.rich_media_title h1',
                '#activity-name',
                '.wx_title',
                '[data-role="title"]',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    if len(title) > 3:  # ç¡®ä¿ä¸æ˜¯ç©ºæ ‡é¢˜
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                        return title
            
            # ä»é¡µé¢æ ‡é¢˜æå–
            title_tag = soup.find('title')
            if title_tag and title_tag.get_text(strip=True):
                title = title_tag.get_text(strip=True)
                if 'å¾®ä¿¡å…¬ä¼—å¹³å°' not in title and len(title) > 3:
                    logger.debug(f"ä»titleæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # ä»metaæ ‡ç­¾æå–
            meta_title = soup.find('meta', property='og:title')
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content').strip()
                if title and len(title) > 3:
                    logger.debug(f"ä»metaæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            return "æœªçŸ¥æ ‡é¢˜"
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_author_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–ä½œè€…"""
        try:
            # å¾®ä¿¡æ–‡ç« ä½œè€…çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '[data-role="nickname"]',
                '.account_nickname',
                '.wx_author',
                '#profileBt .nickname'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem and author_elem.get_text(strip=True):
                    author = author_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…å¤±è´¥: {e}")
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date_from_selenium_soup(self, soup: BeautifulSoup) -> str:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '[data-role="publish-time"]',
                '.wx_time',
                '.time'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem and date_elem.get_text(strip=True):
                    date_text = date_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return "æœªçŸ¥æ—¶é—´"
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_content_from_selenium_soup(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­æå–æ–‡ç« æ­£æ–‡åŒºåŸŸ"""
        try:
            # å¾®ä¿¡æ–‡ç« æ­£æ–‡çš„é€‰æ‹©å™¨ï¼ˆæ¸²æŸ“åï¼‰
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '.appmsg_content_text',
                '[data-role="article-content"]',
                '.rich_media_area_primary',
                '.rich_media_wrp',
                '#js_content_container'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦è¶³å¤Ÿå¤š
                    text_content = content_elem.get_text(strip=True)
                    if text_content and len(text_content) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                        logger.debug(f"âœ… é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                        return content_elem
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å®¹å™¨ï¼Œæ™ºèƒ½æå–
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨æ­£æ–‡å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–")
            
            # æŸ¥æ‰¾åŒ…å«æœ€å¤šæ–‡æœ¬å†…å®¹çš„div
            all_divs = soup.find_all('div')
            best_div = None
            max_text_length = 0
            
            for div in all_divs:
                text = div.get_text(strip=True)
                if len(text) > max_text_length and len(text) > 200:  # è‡³å°‘200ä¸ªå­—ç¬¦
                    # æ’é™¤å¯¼èˆªã€å¤´éƒ¨ã€è„šéƒ¨ç­‰åŒºåŸŸ
                    div_class = ' '.join(div.get('class', []))
                    div_id = div.get('id', '')
                    
                    exclude_keywords = ['nav', 'header', 'footer', 'sidebar', 'menu', 'toolbar', 'ad']
                    if not any(keyword in div_class.lower() + div_id.lower() for keyword in exclude_keywords):
                        max_text_length = len(text)
                        best_div = div
            
            if best_div:
                logger.debug(f"æ™ºèƒ½æå–æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ–‡æœ¬é•¿åº¦: {max_text_length}")
                return best_div
            
            logger.warning("ä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            return soup.find('body') or soup
            
        except Exception as e:
            logger.error(f"æå–æ­£æ–‡å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _download_images_from_selenium_soup(self, content_soup: BeautifulSoup, base_url: str) -> list:
        """ä»Seleniumæ¸²æŸ“çš„é¡µé¢ä¸­ä¸‹è½½å›¾ç‰‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            images_info = []
            img_tags = content_soup.find_all('img')
            
            if not img_tags:
                logger.info("ğŸ“· æœªå‘ç°å›¾ç‰‡")
                return images_info
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_tags)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            
            # åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•
            import urllib.parse
            parsed_url = urllib.parse.urlparse(base_url)
            safe_domain = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.netloc)
            img_dir = os.path.join("output", "images", safe_domain)
            os.makedirs(img_dir, exist_ok=True)
            
            # é™åˆ¶åŒæ—¶ä¸‹è½½çš„å›¾ç‰‡æ•°é‡ï¼Œé¿å…å¡ä½
            max_images = min(len(img_tags), 20)  # æœ€å¤šä¸‹è½½20å¼ å›¾ç‰‡
            successful_downloads = 0
            
            for i, img in enumerate(img_tags[:max_images]):
                try:
                    # ä»SeleniumåŠ è½½çš„é¡µé¢ä¸­ï¼Œsrcåº”è¯¥å·²ç»è¢«å®Œå…¨è§£æ
                    img_src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if not img_src:
                        continue
                    
                    # å¤„ç†å®Œæ•´çš„URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = f"https://{parsed_url.netloc}" + img_src
                    elif not img_src.startswith('http'):
                        img_src = urllib.parse.urljoin(base_url, img_src)
                    
                    # è·³è¿‡è¿‡å°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å›¾æ ‡ï¼‰æˆ–base64å›¾ç‰‡
                    if ('icon' in img_src.lower() or 
                        'logo' in img_src.lower() or 
                        img_src.startswith('data:') or
                        len(img_src) > 1000):  # è·³è¿‡è¶…é•¿URL
                        continue
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"img_{i+1:03d}.jpg"
                    img_path = os.path.join(img_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
                    download_success = self._download_image_with_timeout(img_src, img_path, timeout=10)
                    
                    if download_success:
                        images_info.append({
                            'url': img_src,
                            'local_path': img_path,
                            'filename': img_filename
                        })
                        successful_downloads += 1
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_filename}")
                    else:
                        # å³ä½¿ä¸‹è½½å¤±è´¥ï¼Œä¹Ÿè®°å½•å›¾ç‰‡ä¿¡æ¯ï¼Œä½†local_pathä¸ºNone
                        images_info.append({
                            'url': img_src,
                            'local_path': None,
                            'filename': img_filename
                        })
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_src[:50]}...")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {successful_downloads}/{max_images} å¼ æˆåŠŸ")
            return images_info
            
        except Exception as e:
            logger.warning(f"å›¾ç‰‡ä¸‹è½½è¿‡ç¨‹å¼‚å¸¸: {e}")
            return []

    def _download_image_with_timeout(self, img_url: str, save_path: str, timeout: int = 10) -> bool:
        """å¸¦è¶…æ—¶æ§åˆ¶çš„å›¾ç‰‡ä¸‹è½½"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
            }
            
            # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            response = requests.get(img_url, headers=headers, timeout=timeout, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png', 'gif', 'webp']):
                logger.debug(f"éå›¾ç‰‡å†…å®¹ç±»å‹: {content_type}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½è¿‡å¤§çš„æ–‡ä»¶
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MBé™åˆ¶
                logger.debug(f"å›¾ç‰‡æ–‡ä»¶è¿‡å¤§: {content_length} bytes")
                return False
            
            # å†™å…¥æ–‡ä»¶
            with open(save_path, 'wb') as f:
                downloaded_size = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        # é™åˆ¶ä¸‹è½½å¤§å°
                        if downloaded_size > 10 * 1024 * 1024:  # 10MBé™åˆ¶
                            logger.debug("ä¸‹è½½æ–‡ä»¶è¿‡å¤§ï¼Œåœæ­¢ä¸‹è½½")
                            return False
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½
            if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                return True
            else:
                return False
            
        except requests.exceptions.Timeout:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½è¶…æ—¶: {img_url}")
            return False
        except requests.exceptions.RequestException as e:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½ç½‘ç»œé”™è¯¯: {e}")
            return False
        except Exception as e:
            logger.debug(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def _extract_wechat_article_by_requests(self, url: str) -> dict:
        """
        ä½¿ç”¨Requestsè·å–å¾®ä¿¡æ–‡ç« å†…å®¹
        """
        try:
            logger.info(f"ğŸŒ ä½¿ç”¨Requestsè·å–æ–‡ç« å†…å®¹: {url}")
            
            # è®¾ç½®è¯·æ±‚å¤´ç»•è¿‡åçˆ¬æœºåˆ¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è°ƒè¯•ï¼šä¿å­˜åŸå§‹HTMLåˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
            debug_html_path = "debug_page.html"
            with open(debug_html_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            logger.debug(f"åŸå§‹HTMLå·²ä¿å­˜åˆ°: {debug_html_path}")
            
            # æå–æ ‡é¢˜
            title = self._extract_title_from_soup(soup)
            
            # æå–ä½œè€…
            author = self._extract_author_from_soup(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date_from_soup(soup)
            
            # æå–æ–‡ç« æ­£æ–‡å†…å®¹
            content_soup = self._extract_content_from_soup(soup)
            
            if not content_soup:
                return {"error": "æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹"}
            
            # ä¸‹è½½å›¾ç‰‡
            images = self._download_images_from_soup(content_soup, url)
            
            logger.success(f"âœ… æˆåŠŸæå–æ–‡ç« å†…å®¹: {title[:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ®µè½æ•°={len(content_soup.find_all(['p', 'div']))}, å›¾ç‰‡æ•°é‡={len(images)}")
            
            return {
                'title': title,
                'author': author,
                'publish_date': publish_date,
                'content_soup': content_soup,
                'images': images,
                'url': url
            }
            
        except requests.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return {"error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            logger.error(f"è§£æHTMLå¤±è´¥: {e}")
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}
    
    def _extract_title_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–æ ‡é¢˜"""
        try:
            # å¾®ä¿¡æ–‡ç« æ ‡é¢˜çš„å¸¸è§ä½ç½®
            title_selectors = [
                'h1#activity-name',
                'h2.rich_media_title',
                '.rich_media_title',
                'h1.rich_media_title',
                '.rich_media_title h1',
                '#activity-name',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä»é¡µé¢æºç ä¸­æœç´¢
            page_text = soup.get_text()
            import re
            # å°è¯•ä»metaä¿¡æ¯ä¸­æ‰¾æ ‡é¢˜
            meta_title = soup.find('meta', property='og:title')
            if meta_title and meta_title.get('content'):
                title = meta_title.get('content').strip()
                if title:
                    logger.debug(f"ä»metaæ ‡ç­¾æ‰¾åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            return "æœªçŸ¥æ ‡é¢˜"
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _extract_author_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–ä½œè€…"""
        try:
            # å¾®ä¿¡æ–‡ç« ä½œè€…çš„å¸¸è§ä½ç½®
            author_selectors = [
                '.rich_media_meta_nickname',
                '.profile_nickname',
                '[data-role="nickname"]',
                '.account_nickname'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem and author_elem.get_text(strip=True):
                    author = author_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…å¤±è´¥: {e}")
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_publish_date_from_soup(self, soup: BeautifulSoup) -> str:
        """ä»soupä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„å¸¸è§ä½ç½®
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '[data-role="publish-time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem and date_elem.get_text(strip=True):
                    date_text = date_elem.get_text(strip=True)
                    logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            # å°è¯•ä»è„šæœ¬ä¸­æå–
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'publish_time' in script.string:
                    import re
                    match = re.search(r'publish_time["\s]*[=:]["\s]*("[^"]+)"', script.string)
                    if match:
                        return match.group(1)
            
            return "æœªçŸ¥æ—¶é—´"
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return "æœªçŸ¥æ—¶é—´"
    
    def _extract_content_from_soup(self, soup: BeautifulSoup) -> BeautifulSoup:
        """ä»soupä¸­æå–æ–‡ç« æ­£æ–‡åŒºåŸŸ"""
        try:
            # å¾®ä¿¡æ–‡ç« æ­£æ–‡çš„å¸¸è§å®¹å™¨
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '.appmsg_content_text',
                '[data-role="article-content"]',
                '.rich_media_area_primary',
                '.rich_media_wrp'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦è¶³å¤Ÿå¤š
                    text_content = content_elem.get_text(strip=True)
                    if text_content and len(text_content) > 50:  # è‡³å°‘50ä¸ªå­—ç¬¦
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                        return content_elem
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨æ­£æ–‡å®¹å™¨ï¼Œå°è¯•æ™ºèƒ½æå–")
            
            # æŸ¥æ‰¾åŒ…å«è¾ƒå¤šæ–‡æœ¬çš„div
            all_divs = soup.find_all('div')
            best_div = None
            max_text_length = 0
            
            for div in all_divs:
                text = div.get_text(strip=True)
                if len(text) > max_text_length and len(text) > 100:  # è‡³å°‘100ä¸ªå­—ç¬¦
                    # æ’é™¤ä¸€äº›ä¸ç›¸å…³çš„div
                    div_class = div.get('class', [])
                    div_id = div.get('id', '')
                    if not any(exclude in str(div_class) + div_id for exclude in ['nav', 'header', 'footer', 'sidebar', 'menu']):
                        max_text_length = len(text)
                        best_div = div
            
            if best_div:
                logger.debug(f"æ™ºèƒ½æå–æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ–‡æœ¬é•¿åº¦: {max_text_length}")
                return best_div
            
            logger.warning("ä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            return soup.find('body') or soup
            
        except Exception as e:
            logger.error(f"æå–æ­£æ–‡å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _download_images_from_soup(self, content_soup: BeautifulSoup, base_url: str) -> list:
        """ä»å†…å®¹ä¸­ä¸‹è½½å›¾ç‰‡"""
        try:
            images_info = []
            img_tags = content_soup.find_all('img')
            
            if not img_tags:
                logger.info("ğŸ“· æœªå‘ç°å›¾ç‰‡")
                return images_info
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_tags)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
            
            # åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•
            import urllib.parse
            parsed_url = urllib.parse.urlparse(base_url)
            safe_domain = re.sub(r'[<>:"/\\|?*]', '_', parsed_url.netloc)
            img_dir = os.path.join("output", "images", safe_domain)
            os.makedirs(img_dir, exist_ok=True)
            
            for i, img in enumerate(img_tags):
                try:
                    img_src = img.get('src') or img.get('data-src') or img.get('data-original')
                    if not img_src:
                        continue
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = f"https://{parsed_url.netloc}" + img_src
                    elif not img_src.startswith('http'):
                        img_src = urllib.parse.urljoin(base_url, img_src)
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"img_{i+1:03d}.jpg"
                    img_path = os.path.join(img_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image_requests(img_src, img_path):
                        images_info.append({
                            'url': img_src,
                            'local_path': img_path,
                            'filename': img_filename
                        })
                        logger.debug(f"ğŸ“· ä¸‹è½½å›¾ç‰‡æˆåŠŸ: {img_filename}")
                    else:
                        images_info.append({
                            'url': img_src,
                            'local_path': None,
                            'filename': img_filename
                        })
                        logger.warning(f"ğŸ“· ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.warning(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len([img for img in images_info if img['local_path']])}/{len(img_tags)}")
            return images_info
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
            return []
    
    def _download_image_requests(self, img_url: str, save_path: str) -> bool:
        """ä½¿ç”¨requestsä¸‹è½½å›¾ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/'
            }
            
            response = requests.get(img_url, headers=headers, timeout=15, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False
    
    def _process_wechat_content_to_docx(self, doc, content_soup: BeautifulSoup, images: list):
        """
        å°†å¾®ä¿¡æ–‡ç« å†…å®¹è½¬æ¢ä¸ºWordæ–‡æ¡£ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            # é€’å½’å¤„ç†æ‰€æœ‰å†…å®¹å…ƒç´ ï¼Œæ·»åŠ æ·±åº¦é™åˆ¶
            self._process_element_to_docx_recursive(doc, content_soup, images, depth=0, max_depth=10)
                    
        except Exception as e:
            logger.warning(f"å†…å®¹è½¬æ¢å¼‚å¸¸: {e}")
            # é™çº§åˆ°çº¯æ–‡æœ¬å¤„ç†
            try:
                text = content_soup.get_text()
                if text.strip():
                    doc.add_paragraph(text.strip())
            except:
                doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
    
    def _process_element_to_docx_recursive(self, doc, element, images: list, depth: int = 0, max_depth: int = 10):
        """
        é€’å½’å¤„ç†HTMLå…ƒç´ åˆ°Wordæ–‡æ¡£ï¼ˆå¸¦æ·±åº¦é™åˆ¶ï¼‰
        """
        try:
            # é˜²æ­¢æ— é™é€’å½’
            if depth > max_depth:
                logger.warning(f"é€’å½’æ·±åº¦è¶…é™ ({depth})ï¼Œåœæ­¢å¤„ç†")
                return
                
            if hasattr(element, 'name'):
                tag_name = element.name.lower() if element.name else None
                
                # å¤„ç†ä¸åŒç±»å‹çš„å…ƒç´ 
                if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    # æ ‡é¢˜å¤„ç†
                    level = int(tag_name[1])
                    text = element.get_text(strip=True)
                    if text:
                        doc.add_heading(text, level=min(level, 3))
                        
                elif tag_name == 'p':
                    # æ®µè½å¤„ç†
                    self._process_paragraph_to_docx(doc, element, images)
                    
                elif tag_name == 'div':
                    # divå®¹å™¨ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                    text = element.get_text(strip=True)
                    if text and len(text) < 200:  # çŸ­æ–‡æœ¬ç›´æ¥ä½œä¸ºæ®µè½
                        paragraph = doc.add_paragraph()
                        self._add_formatted_text_to_paragraph(paragraph, element, images)
                    else:
                        # é•¿å†…å®¹ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                        if hasattr(element, 'children'):
                            child_count = 0
                            for child in element.children:
                                child_count += 1
                                if child_count > 50:  # é™åˆ¶å­å…ƒç´ æ•°é‡
                                    logger.warning("å­å…ƒç´ è¿‡å¤šï¼Œåœæ­¢å¤„ç†")
                                    break
                                self._process_element_to_docx_recursive(doc, child, images, depth + 1, max_depth)
                    
                elif tag_name == 'img':
                    # å›¾ç‰‡å¤„ç†
                    self._add_image_to_docx_new(doc, element, images)
                    
                elif tag_name == 'blockquote':
                    # å¼•ç”¨å¤„ç†
                    text = element.get_text(strip=True)
                    if text:
                        paragraph = doc.add_paragraph(text)
                        # åº”ç”¨æ–œä½“æ ·å¼
                        for run in paragraph.runs:
                            run.italic = True
                            
                elif tag_name in ['ul', 'ol']:
                    # åˆ—è¡¨å¤„ç†
                    items = element.find_all('li')[:20]  # é™åˆ¶åˆ—è¡¨é¡¹æ•°é‡
                    for item in items:
                        text = item.get_text(strip=True)
                        if text:
                            doc.add_paragraph(f"â€¢ {text}", style='List Bullet')
                            
                elif tag_name == 'br':
                    # æ¢è¡Œ
                    doc.add_paragraph("")
                    
                elif tag_name in ['span', 'strong', 'b', 'em', 'i']:
                    # å†…è”å…ƒç´ ï¼Œåœ¨çˆ¶çº§å¤„ç†
                    pass
                    
                else:
                    # å…¶ä»–å…ƒç´ ï¼Œé€’å½’å¤„ç†å­å…ƒç´ 
                    if hasattr(element, 'children'):
                        child_count = 0
                        for child in element.children:
                            child_count += 1
                            if child_count > 50:  # é™åˆ¶å­å…ƒç´ æ•°é‡
                                logger.warning("å­å…ƒç´ è¿‡å¤šï¼Œåœæ­¢å¤„ç†")
                                break
                            self._process_element_to_docx_recursive(doc, child, images, depth + 1, max_depth)
            else:
                # æ–‡æœ¬èŠ‚ç‚¹
                if hasattr(element, 'strip'):
                    text_content = str(element).strip()
                    if text_content and len(text_content) > 0 and len(text_content) < 1000:  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                        doc.add_paragraph(text_content)
                    
        except Exception as e:
            logger.debug(f"å¤„ç†å…ƒç´ å¼‚å¸¸ (æ·±åº¦{depth}): {e}")
            # å¼‚å¸¸æ—¶æ·»åŠ ç®€å•æ–‡æœ¬å†…å®¹
            try:
                if hasattr(element, 'get_text'):
                    text = element.get_text(strip=True)
                    if text and len(text) < 500:
                        doc.add_paragraph(text)
            except:
                pass
    
    def _process_paragraph_to_docx(self, doc, p_element, images: list):
        """å¤„ç†æ®µè½å…ƒç´ """
        try:
            # æ£€æŸ¥æ®µè½æ˜¯å¦ä¸ºç©º
            text_content = p_element.get_text(strip=True)
            if not text_content:
                return
            
            # åˆ›å»ºæ®µè½
            paragraph = doc.add_paragraph()
            
            # æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬
            self._add_formatted_text_to_paragraph(paragraph, p_element, images)
            
        except Exception as e:
            logger.debug(f"å¤„ç†æ®µè½å¼‚å¸¸: {e}")
    
    def _add_formatted_text_to_paragraph(self, paragraph, element, images: list):
        """æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬åˆ°æ®µè½"""
        try:
            if hasattr(element, 'children'):
                for child in element.children:
                    if hasattr(child, 'name') and child.name:
                        tag_name = child.name.lower()
                        
                        if tag_name == 'img':
                            # å›¾ç‰‡æ’å…¥åˆ°æ®µè½ä¸­
                            try:
                                self._add_image_to_docx_new(paragraph._element.getparent().getparent(), child, images, True)
                            except:
                                pass
                                
                        elif tag_name in ['strong', 'b']:
                            # åŠ ç²—æ–‡æœ¬
                            text = child.get_text()
                            if text:
                                run = paragraph.add_run(text)
                                run.bold = True
                                
                        elif tag_name in ['em', 'i']:
                            # æ–œä½“æ–‡æœ¬
                            text = child.get_text()
                            if text:
                                run = paragraph.add_run(text)
                                run.italic = True
                                
                        elif tag_name == 'a':
                            # é“¾æ¥
                            text = child.get_text()
                            href = child.get('href', '')
                            if text:
                                run = paragraph.add_run(f"{text}({href})" if href else text)
                                
                        else:
                            # å…¶ä»–æ ‡ç­¾ï¼Œé€’å½’å¤„ç†
                            self._add_formatted_text_to_paragraph(paragraph, child, images)
                    else:
                        # æ–‡æœ¬èŠ‚ç‚¹
                        text = str(child).strip()
                        if text:
                            paragraph.add_run(text)
            else:
                # å¦‚æœæ²¡æœ‰childrenï¼Œç›´æ¥å¤„ç†æ–‡æœ¬å†…å®¹
                text = element.get_text() if hasattr(element, 'get_text') else str(element)
                if text.strip():
                    paragraph.add_run(text.strip())
                            
        except Exception as e:
            logger.debug(f"æ·»åŠ æ ¼å¼åŒ–æ–‡æœ¬å¼‚å¸¸: {e}")
    
    def _add_image_to_docx_new(self, doc, img_element, images: list, inline: bool = False):
        """æ·»åŠ å›¾ç‰‡åˆ°Wordæ–‡æ¡£ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            img_src = img_element.get('src') or img_element.get('data-src') or img_element.get('data-original')
            if not img_src:
                return
            
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
            local_image = None
            for img_info in images:
                if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                    local_image = img_info
                    break
            
            if local_image and local_image.get('local_path'):
                local_path = local_image['local_path']
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
                if not os.path.exists(local_path):
                    logger.debug(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
                    if not inline:
                        doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
                    return
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                try:
                    file_size = os.path.getsize(local_path)
                    if file_size == 0:
                        logger.debug(f"å›¾ç‰‡æ–‡ä»¶ä¸ºç©º: {local_path}")
                        if not inline:
                            doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
                        return
                    elif file_size > 5 * 1024 * 1024:  # 5MBé™åˆ¶
                        logger.debug(f"å›¾ç‰‡æ–‡ä»¶è¿‡å¤§: {file_size} bytes")
                        if not inline:
                            doc.add_paragraph(f"[å›¾ç‰‡è¿‡å¤§: {img_src}]")
                        return
                except OSError:
                    logger.debug(f"æ— æ³•è·å–å›¾ç‰‡æ–‡ä»¶å¤§å°: {local_path}")
                    if not inline:
                        doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
                    return
                
                # å°è¯•æ’å…¥å›¾ç‰‡
                try:
                    # éªŒè¯å›¾ç‰‡æ ¼å¼
                    valid_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp')
                    if not any(local_path.lower().endswith(ext) for ext in valid_extensions):
                        # å°è¯•é‡å‘½åä¸º.jpg
                        new_path = local_path.rsplit('.', 1)[0] + '.jpg'
                        if os.path.exists(local_path):
                            try:
                                os.rename(local_path, new_path)
                                local_path = new_path
                            except:
                                pass
                    
                    # è®¾ç½®å›¾ç‰‡å¤§å°
                    if inline:
                        # å†…è”å›¾ç‰‡ï¼Œè¾ƒå°å°ºå¯¸
                        doc.add_picture(local_path, width=Inches(3))
                    else:
                        # ç‹¬ç«‹æ®µè½å›¾ç‰‡ï¼Œè¾ƒå¤§å°ºå¯¸
                        doc.add_picture(local_path, width=Inches(4.5))
                    
                    logger.debug(f"âœ… å›¾ç‰‡æ’å…¥æˆåŠŸ: {os.path.basename(local_path)}")
                    
                except Exception as img_error:
                    logger.debug(f"å›¾ç‰‡æ’å…¥å¤±è´¥ {local_path}: {img_error}")
                    # é™çº§å¤„ç†ï¼šæ·»åŠ å›¾ç‰‡é“¾æ¥æ–‡æœ¬
                    if not inline:
                        doc.add_paragraph(f"[å›¾ç‰‡é“¾æ¥: {img_src}]")
            else:
                # æ²¡æœ‰æœ¬åœ°å›¾ç‰‡æ–‡ä»¶ï¼Œæ·»åŠ é“¾æ¥æ–‡æœ¬
                if not inline:
                    doc.add_paragraph(f"[å›¾ç‰‡é“¾æ¥: {img_src}]")
                
        except Exception as e:
            logger.debug(f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“æ•´ä½“æ–‡æ¡£ç”Ÿæˆ
    
    def _process_html_to_docx(self, doc, html_content: str, images: list):
        """
        å°†HTMLå†…å®¹è½¬æ¢ä¸ºWordæ–‡æ¡£ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # é€’å½’å¤„ç†æ¯ä¸ªå…ƒç´ 
            for element in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'img', 'br']):
                self._process_element_to_docx(doc, element, images)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†å…ƒç´ ï¼Œå¤„ç†æ‰€æœ‰æ–‡æœ¬
            if not soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                all_text = soup.get_text(strip=True)
                if all_text:
                    doc.add_paragraph(all_text)
                    
        except Exception as e:
            logger.warning(f"HTMLè½¬æ¢å¼‚å¸¸: {e}")
            # é™çº§åˆ°çº¯æ–‡æœ¬å¤„ç†
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                text = soup.get_text()
                doc.add_paragraph(text)
            except:
                doc.add_paragraph("å†…å®¹æå–å¤±è´¥")
    
    def _process_element_to_docx(self, doc, element, images: list):
        """
        å¤„ç†å•ä¸ªHTMLå…ƒç´ åˆ°Wordæ–‡æ¡£
        """
        try:
            tag_name = element.name.lower()
            
            # æ ‡é¢˜å¤„ç†
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                text = element.get_text(strip=True)
                if text:
                    doc.add_heading(text, level=min(level, 3))
            
            # æ®µè½å¤„ç†
            elif tag_name in ['p', 'div']:
                text = element.get_text(strip=True)
                if text:
                    paragraph = doc.add_paragraph(text)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                    imgs = element.find_all('img')
                    for img in imgs:
                        self._add_image_to_docx(doc, img, images)
            
            # åˆ—è¡¨å¤„ç†
            elif tag_name in ['ul', 'ol']:
                items = element.find_all('li')
                for item in items:
                    text = item.get_text(strip=True)
                    if text:
                        doc.add_paragraph(f"â€¢ {text}", style='List Bullet')
            
            # å›¾ç‰‡å¤„ç†
            elif tag_name == 'img':
                self._add_image_to_docx(doc, element, images)
            
            # æ¢è¡Œå¤„ç†
            elif tag_name == 'br':
                doc.add_paragraph("")
                
        except Exception as e:
            logger.debug(f"å…ƒç´ å¤„ç†å¼‚å¸¸: {e}")
    
    def _add_image_to_docx(self, doc, img_element, images: list):
        """
        æ·»åŠ å›¾ç‰‡åˆ°Wordæ–‡æ¡£
        """
        try:
            img_src = img_element.get('src') or img_element.get('data-src') or img_element.get('data-original')
            if not img_src:
                return
            
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
            local_image = None
            for img_info in images:
                if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                    local_image = img_info
                    break
            
            if local_image and local_image.get('local_path') and os.path.exists(local_image['local_path']):
                try:
                    doc.add_picture(local_image['local_path'], width=Inches(4))
                    logger.debug(f"æ·»åŠ å›¾ç‰‡æˆåŠŸ: {local_image['local_path']}")
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡æ·»åŠ å¤±è´¥: {e}")
                    doc.add_paragraph(f"[å›¾ç‰‡: {img_src}]")
            else:
                doc.add_paragraph(f"[å›¾ç‰‡é“¾æ¥: {img_src}]")
                
        except Exception as e:
            logger.debug(f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
    
    def _handle_captcha_if_present(self) -> bool:
        """å¤„ç†éªŒè¯ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        try:
            logger.info("ğŸ” æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...")
            
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(3)
            
            # æ£€æŸ¥å¸¸è§çš„éªŒè¯ç å…ƒç´ 
            captcha_selectors = [
                "iframe[src*='captcha']",
                "div[class*='captcha']",
                "div[id*='captcha']",
                ".captcha-container",
                "#captcha",
                "iframe[src*='verify']",
                "div[class*='verify']",
                "canvas[id*='captcha']",
                "img[src*='captcha']"
            ]
            
            captcha_found = False
            for selector in captcha_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        logger.warning(f"ğŸ¯ å‘ç°éªŒè¯ç å…ƒç´ : {selector}")
                        captcha_found = True
                        break
                except:
                    continue
            
            if captcha_found:
                logger.warning("âš ï¸ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œåˆ‡æ¢åˆ°éheadlessæ¨¡å¼ä»¥ä¾¿æ‰‹åŠ¨å¤„ç†")
                
                # å¦‚æœæ˜¯headlessæ¨¡å¼ï¼Œæç¤ºç”¨æˆ·
                if self.headless:
                    logger.error("âŒ æ£€æµ‹åˆ°éªŒè¯ç ä½†å½“å‰ä¸ºheadlessæ¨¡å¼ï¼Œè¯·ä½¿ç”¨GUIæ¨¡å¼è¿è¡Œç¨‹åº")
                    return False
                
                # ç­‰å¾…ç”¨æˆ·å¤„ç†éªŒè¯ç 
                logger.info("ğŸ–±ï¸ è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯ç éªŒè¯...")
                logger.info("â³ ç¨‹åºå°†ç­‰å¾…60ç§’ä¾›æ‚¨å®ŒæˆéªŒè¯...")
                
                # ç­‰å¾…éªŒè¯ç æ¶ˆå¤±æˆ–é¡µé¢è·³è½¬
                for i in range(60):
                    time.sleep(1)
                    try:
                        # æ£€æŸ¥éªŒè¯ç æ˜¯å¦è¿˜å­˜åœ¨
                        still_has_captcha = False
                        for selector in captcha_selectors:
                            try:
                                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                                if elements and elements[0].is_displayed():
                                    still_has_captcha = True
                                    break
                            except:
                                continue
                        
                        if not still_has_captcha:
                            logger.info("âœ… éªŒè¯ç å·²å®Œæˆï¼Œç»§ç»­å¤„ç†...")
                            return True
                            
                        # æ£€æŸ¥æ˜¯å¦å·²ç»è·³è½¬åˆ°æ–‡ç« é¡µé¢
                        if "mp.weixin.qq.com/s" in self.driver.current_url:
                            article_title = self.driver.find_elements(By.CSS_SELECTOR, "#activity-name, .rich_media_title")
                            if article_title:
                                logger.info("âœ… å·²æˆåŠŸè·³è½¬åˆ°æ–‡ç« é¡µé¢")
                                return True
                                
                    except Exception as e:
                        logger.debug(f"éªŒè¯ç æ£€æŸ¥å¼‚å¸¸: {e}")
                        continue
                
                logger.error("â° éªŒè¯ç å¤„ç†è¶…æ—¶ï¼Œè¯·é‡è¯•")
                return False
            else:
                logger.info("âœ… æœªæ£€æµ‹åˆ°éªŒè¯ç ")
                return True
                
        except Exception as e:
            logger.error(f"éªŒè¯ç å¤„ç†å¼‚å¸¸: {e}")
            return True  # ç»§ç»­æ‰§è¡Œï¼Œå¯èƒ½æ²¡æœ‰éªŒè¯ç 

    def _wait_for_basic_page_load(self) -> bool:
        """
        ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½ - ä¼˜åŒ–ç‰ˆæœ¬
        """
        try:
            logger.info("ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½...")
            
            # å‡å°‘ç­‰å¾…æ—¶é—´åˆ°æœ€å¤š5ç§’
            wait = WebDriverWait(self.driver, 5)
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ åŠ è½½
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # ç­‰å¾…æ–‡ç« å†…å®¹åŠ è½½ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 
                    "#js_content, .rich_media_content, .article-content, .content")))
            except:
                logger.debug("æœªæ‰¾åˆ°å¸¸è§æ–‡ç« å†…å®¹å®¹å™¨ï¼Œä½¿ç”¨é€šç”¨æ£€æŸ¥")
            
            # çŸ­æš‚ç­‰å¾…DOMç¨³å®š
            time.sleep(0.5)
            
            logger.success("é¡µé¢åŸºç¡€å†…å®¹åŠ è½½å®Œæˆ")
            return True
            
        except TimeoutException:
            logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å¤„ç†")
            return True
        except Exception as e:
            logger.error(f"ç­‰å¾…é¡µé¢åŠ è½½æ—¶å‡ºé”™: {e}")
            return False
    
    def _human_like_scroll_and_load(self, target_url: str = None) -> bool:
        """
        äººç±»å¼æ»šåŠ¨åŠ è½½å†…å®¹ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå‡å°‘é‡å¤æ»šåŠ¨ï¼‰
        """
        try:
            # è·å–é¡µé¢æ€»é«˜åº¦
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            logger.info(f"å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½ï¼Œé¡µé¢é«˜åº¦: {total_height}px")
            
            # å¦‚æœé¡µé¢å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
            if total_height <= 1000:
                logger.debug("é¡µé¢è¾ƒçŸ­ï¼Œæ— éœ€æ»šåŠ¨")
                return True
            
            # æ ¹æ®é¡µé¢é«˜åº¦åŠ¨æ€è°ƒæ•´æ»šåŠ¨ç­–ç•¥
            if total_height <= 3000:
                # çŸ­é¡µé¢ï¼šå¿«é€Ÿæ»šåŠ¨
                pixels_per_step = 800
                scroll_delay = 1.0
                check_frequency = 3
            elif total_height <= 8000:
                # ä¸­ç­‰é¡µé¢ï¼šå¹³è¡¡æ»šåŠ¨
                pixels_per_step = 1000
                scroll_delay = 1.2
                check_frequency = 4
            else:
                # é•¿é¡µé¢ï¼šå¤§æ­¥æ»šåŠ¨
                pixels_per_step = 1500
                scroll_delay = 1.5
                check_frequency = 5
            
            # è®¡ç®—æ»šåŠ¨æ­¥æ•°
            scroll_positions = max(3, (total_height // pixels_per_step) + 1)
            scroll_positions = min(scroll_positions, 8)  # æœ€å¤š8ä¸ªä½ç½®ï¼Œé¿å…è¿‡åº¦æ»šåŠ¨
            
            logger.info(f"æ»šåŠ¨ç­–ç•¥: {scroll_positions}æ­¥, æ¯æ­¥{pixels_per_step}px, å»¶è¿Ÿ{scroll_delay}s")
            
            # æ‰§è¡Œæ»šåŠ¨
            for i in range(scroll_positions):
                # è®¡ç®—æ»šåŠ¨ä½ç½®
                if scroll_positions == 1:
                    scroll_to = total_height
                else:
                    progress = i / (scroll_positions - 1)
                    scroll_to = int(total_height * progress)
                
                logger.debug(f"æ»šåŠ¨åˆ°ä½ç½® {i+1}/{scroll_positions}: {scroll_to}px")
                
                # å¹³æ»‘æ»šåŠ¨åˆ°ç›®æ ‡ä½ç½®
                self.driver.execute_script(f"window.scrollTo({{top: {scroll_to}, behavior: 'smooth'}});")
                
                # ç­‰å¾…å†…å®¹åŠ è½½
                time.sleep(scroll_delay)
                
                # å‡å°‘å›¾ç‰‡æ£€æŸ¥é¢‘ç‡
                if i % check_frequency == 0:
                    self._trigger_image_loading()
                    time.sleep(0.5)
            
            # æœ€ç»ˆå¤„ç†ï¼šå¿«é€Ÿæ»šåŠ¨åˆ°åº•éƒ¨
            logger.info("æœ€ç»ˆå¤„ç†...")
            self.driver.execute_script("window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});")
            time.sleep(1.5)
            
            # æœ€åä¸€æ¬¡å›¾ç‰‡æ£€æŸ¥
            self._trigger_image_loading()
            time.sleep(1.0)
            
            # å›åˆ°é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo({top: 0, behavior: 'smooth'});")
            time.sleep(0.5)
            
            logger.success("æ™ºèƒ½æ»šåŠ¨å®Œæˆ")
            return True
            
        except Exception as e:
            logger.warning(f"æ»šåŠ¨è¿‡ç¨‹å¼‚å¸¸: {e}")
            return True  # å³ä½¿å‡ºé”™ä¹Ÿç»§ç»­ï¼Œä¸å½±å“å†…å®¹æå–
    
    def _trigger_image_loading(self):
        """è§¦å‘å›¾ç‰‡æ‡’åŠ è½½"""
        try:
            # è§¦å‘å„ç§æ‡’åŠ è½½æœºåˆ¶
            self.driver.execute_script("""
                // è§¦å‘æ‡’åŠ è½½å›¾ç‰‡
                var images = document.querySelectorAll('img[data-src], img[data-lazy-src], img[data-original]');
                images.forEach(function(img) {
                    if (img.dataset.src) img.src = img.dataset.src;
                    if (img.dataset.lazySrc) img.src = img.dataset.lazySrc;
                    if (img.dataset.original) img.src = img.dataset.original;
                    // è§¦å‘onloadäº‹ä»¶
                    img.onload = function() { console.log('Image loaded:', this.src); };
                });
                
                // è§¦å‘æ»šåŠ¨äº‹ä»¶ï¼Œå¯èƒ½æ¿€æ´»æŸäº›æ‡’åŠ è½½è„šæœ¬
                window.dispatchEvent(new Event('scroll'));
                window.dispatchEvent(new Event('resize'));
            """)
            
            logger.debug("è§¦å‘å›¾ç‰‡æ‡’åŠ è½½")
            
        except Exception as e:
            logger.debug(f"è§¦å‘å›¾ç‰‡æ‡’åŠ è½½å¤±è´¥: {e}")
    
    def _final_image_check(self):
        """æœ€ç»ˆå›¾ç‰‡æ£€æŸ¥å’ŒåŠ è½½"""
        try:
            logger.info("æ‰§è¡Œæœ€ç»ˆå¿«é€Ÿæ£€æŸ¥...")
            
            # æœ€åä¸€æ¬¡è§¦å‘æ‰€æœ‰å¯èƒ½çš„æ‡’åŠ è½½
            self._trigger_image_loading()
            
            # ç­‰å¾…å›¾ç‰‡åŠ è½½
            time.sleep(3.0)
            
            # æ£€æŸ¥å›¾ç‰‡åŠ è½½çŠ¶æ€
            image_stats = self.driver.execute_script("""
                var images = document.querySelectorAll('img');
                var total = images.length;
                var loaded = 0;
                var broken = 0;
                var unloaded = 0;
                
                images.forEach(function(img) {
                    if (img.complete) {
                        if (img.naturalHeight !== 0) {
                            loaded++;
                        } else {
                            broken++;
                        }
                    } else {
                        unloaded++;
                    }
                });
                
                return {total: total, loaded: loaded, broken: broken, unloaded: unloaded};
            """)
            
            logger.info(f"å›¾ç‰‡çŠ¶æ€: æ€»æ•°={image_stats['total']}, å·²åŠ è½½={image_stats['loaded']}, æŸå={image_stats['broken']}, æœªåŠ è½½={image_stats['unloaded']}")
            
            # å¦‚æœè¿˜æœ‰æœªåŠ è½½çš„å›¾ç‰‡ï¼Œå†ç­‰å¾…ä¸€ä¼š
            if image_stats['unloaded'] > 0:
                logger.warning(f"è¿˜æœ‰ {image_stats['unloaded']} å¼ å›¾ç‰‡æœªåŠ è½½ï¼Œé¢å¤–ç­‰å¾…...")
                time.sleep(2.0)
            
            logger.success("æœ€ç»ˆæ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            logger.debug(f"æœ€ç»ˆå›¾ç‰‡æ£€æŸ¥å¤±è´¥: {e}")
    
    def _download_image(self, img_url: str, save_path: str) -> bool:
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = 'https://mp.weixin.qq.com' + img_url
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://mp.weixin.qq.com/'
            }
            
            response = requests.get(img_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            return True
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False
    
    def _get_article_title(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–‡ç« æ ‡é¢˜"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                '#activity-name',
                '.rich_media_title',
                'h1',
                '.article-title',
                '.title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    # æ¸…ç†æ ‡é¢˜
                    title = re.sub(r'\s+', ' ', title)
                    return title[:50]
                    
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜å¤±è´¥: {e}")
        
        return "unknown_article"
    
    def _extract_article_content(self) -> dict:
        """
        æå–æ–‡ç« å†…å®¹ä¿¡æ¯ - å¿«é€Ÿç‰ˆæœ¬
        """
        try:
            article_info = {}
            
            # å¿«é€Ÿè·å–æ ‡é¢˜
            title_selectors = [
                '#activity-name',
                '.rich_media_title', 
                'h1',
                '.title',
                '[data-role="title"]'
            ]
            
            title = None
            for selector in title_selectors:
                try:
                    title_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_elem and title_elem.text.strip():
                        title = title_elem.text.strip()
                        break
                except:
                    continue
            
            article_info['title'] = title or "æœªçŸ¥æ ‡é¢˜"
            
            # å¿«é€Ÿè·å–ä½œè€…
            author_selectors = [
                '#js_name',
                '.profile_nickname',
                '.author',
                '[data-role="author"]'
            ]
            
            author = None
            for selector in author_selectors:
                try:
                    author_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if author_elem and author_elem.text.strip():
                        author = author_elem.text.strip()
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°ä½œè€…: {author}")
                        break
                except:
                    continue
            
            article_info['author'] = author or "æœªçŸ¥ä½œè€…"
            logger.info(f"æœ€ç»ˆè¯†åˆ«çš„å…¬ä¼—å·/ä½œè€…: {article_info['author']}")
            
            # æ·»åŠ URL
            article_info['url'] = self.driver.current_url
            
            logger.success(f"æˆåŠŸæå–æ–‡ç« ä¿¡æ¯: {article_info['title'][:30]}...")
            return article_info
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {"error": f"å†…å®¹æå–å¤±è´¥: {str(e)}"}
    
    def process_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†URLåˆ—è¡¨"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURL...")
            
            articles = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="å¤„ç†URL")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        articles.append({
                            'title': f"æ— æ•ˆURL_{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': "æ— æ•ˆçš„URLæ ¼å¼",
                            'url': url
                        })
                        continue
                    
                    # æå–æ–‡ç« ä¿¡æ¯
                    article_info = self.extract_article_info(url)
                    
                    if 'error' not in article_info:
                        articles.append(article_info)
                        if len(article_info.get('content', '').strip()) > 100:
                            success_count += 1
                    else:
                        logger.error(f"å¤„ç†URLå¤±è´¥: {article_info.get('error')}")
                        articles.append({
                            'title': f"é”™è¯¯_æ–‡ç« _{i+1}",
                            'author': "æœªçŸ¥",
                            'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': f"å¤„ç†å¤±è´¥: {article_info.get('error')}",
                            'url': url
                        })
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    if i < len(urls) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªURL
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
                    articles.append({
                        'title': f"å¼‚å¸¸_æ–‡ç« _{i+1}",
                        'author': "æœªçŸ¥",
                        'publish_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'content': f"å¤„ç†å¼‚å¸¸: {str(e)}",
                        'url': url
                    })
            
            logger.success(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(articles)}")
            return articles
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return []
    
    def process_urls_as_pdf(self, urls: List[str], output_dir: str = "output/pdf") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºPDFæ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºPDF...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆPDF")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆPDFæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    pdf_filename = f"{safe_title}.pdf"
                    pdf_path = os.path.join(output_dir, pdf_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = pdf_path
                    while os.path.exists(pdf_path):
                        name_without_ext = safe_title
                        pdf_filename = f"{name_without_ext}_{counter}.pdf"
                        pdf_path = os.path.join(output_dir, pdf_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºPDF
                    if self.save_as_pdf(url, pdf_path):
                        saved_files.append(pdf_path)
                        success_count += 1
                        logger.success(f"PDFå·²ä¿å­˜: {pdf_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"PDFç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡PDFç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def process_urls_as_docx(self, urls: List[str], output_dir: str = "output/docx") -> List[str]:
        """æ‰¹é‡å¤„ç†URLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£æ ¼å¼"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(urls)} ä¸ªURLå¹¶ä¿å­˜ä¸ºWordæ–‡æ¡£...")
            
            os.makedirs(output_dir, exist_ok=True)
            saved_files = []
            success_count = 0
            
            from tqdm import tqdm
            for i, url in enumerate(tqdm(urls, desc="ç”ŸæˆWordæ–‡æ¡£")):
                try:
                    logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(urls)} ä¸ªURL...")
                    
                    # éªŒè¯URLæ ¼å¼
                    if not self._is_valid_wechat_url(url):
                        logger.warning(f"æ— æ•ˆçš„å¾®ä¿¡æ–‡ç« URL: {url}")
                        continue
                    
                    # å…ˆè·å–æ–‡ç« ä¿¡æ¯ç”¨äºå‘½å
                    article_info = self.extract_article_info(url)
                    
                    if 'error' in article_info:
                        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {article_info.get('error')}")
                        continue
                    
                    # ç”ŸæˆWordæ–‡ä»¶å
                    title = article_info.get('title', f'æ–‡ç« _{i+1}')
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:50]
                    docx_filename = f"{safe_title}.docx"
                    docx_path = os.path.join(output_dir, docx_filename)
                    
                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    original_path = docx_path
                    while os.path.exists(docx_path):
                        name_without_ext = safe_title
                        docx_filename = f"{name_without_ext}_{counter}.docx"
                        docx_path = os.path.join(output_dir, docx_filename)
                        counter += 1
                    
                    # ä¿å­˜ä¸ºWordæ–‡æ¡£
                    if self.save_as_docx(url, docx_path):
                        saved_files.append(docx_path)
                        success_count += 1
                        logger.success(f"Wordæ–‡æ¡£å·²ä¿å­˜: {docx_filename}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if i < len(urls) - 1:
                        delay = random.uniform(2, 5)
                        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªURLæ—¶å‡ºé”™: {e}")
            
            logger.success(f"Wordæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
            return saved_files
            
        except Exception as e:
            logger.error(f"æ‰¹é‡Wordæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _is_valid_wechat_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¾®ä¿¡æ–‡ç« URL"""
        try:
            # æ£€æŸ¥åŸºæœ¬æ ¼å¼
            if not url or not url.startswith('http'):
                return False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡æ–‡ç« URL
            if 'mp.weixin.qq.com/s' not in url:
                return False
            
            return True
            
        except Exception:
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver:
                self.driver.quit()
                logger.debug("æµè§ˆå™¨å·²å…³é—­")
        except Exception as e:
            logger.debug(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
        finally:
            self.driver = None
    
    def save_complete_html(self, url: str, output_path: str) -> bool:
        """ä¿å­˜å®Œæ•´çš„HTMLæ–‡ä»¶ï¼ŒåŒ…æ‹¬å›¾ç‰‡å’Œæ ·å¼ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜å®Œæ•´HTML: {url}")
            
            # ä½¿ç”¨å·²æœ‰çš„æ–‡ç« æå–æ–¹æ³•ï¼Œé¿å…é‡å¤æ»šåŠ¨
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # è·å–æ–‡ç« ä¿¡æ¯
            title = article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
            author = article_data.get('author', 'æœªçŸ¥ä½œè€…')
            publish_date = article_data.get('publish_date', 'æœªçŸ¥æ—¶é—´')
            content_soup = article_data.get('content_soup')
            images = article_data.get('images', [])
            
            if not content_soup:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« å†…å®¹")
                return False
            
            # åˆ›å»ºå®Œæ•´çš„HTMLæ–‡æ¡£
            html_doc = BeautifulSoup('<!DOCTYPE html><html><head></head><body></body></html>', 'html.parser')
            
            # è®¾ç½®HTMLå¤´éƒ¨
            html_doc.head.append(html_doc.new_tag('meta', charset='utf-8'))
            html_doc.head.append(html_doc.new_tag('meta', attrs={'name': 'viewport', 'content': 'width=device-width, initial-scale=1.0'}))
            
            title_tag = html_doc.new_tag('title')
            title_tag.string = title
            html_doc.head.append(title_tag)
            
            # æ·»åŠ CSSæ ·å¼
            style_tag = html_doc.new_tag('style')
            style_tag.string = """
                body { 
                    max-width: 800px; 
                    margin: 0 auto; 
                    padding: 20px; 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; 
                    line-height: 1.6;
                    color: #333;
                }
                .article-header {
                    text-align: center;
                    margin-bottom: 30px;
                    padding-bottom: 20px;
                    border-bottom: 1px solid #eee;
                }
                .article-title {
                    font-size: 24px;
                    font-weight: bold;
                    margin-bottom: 10px;
                    color: #2c3e50;
                }
                .article-meta {
                    color: #666;
                    font-size: 14px;
                }
                .article-content {
                    font-size: 16px;
                    line-height: 1.8;
                }
                img { 
                    max-width: 100%; 
                    height: auto; 
                    display: block;
                    margin: 15px auto;
                    border-radius: 4px;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                }
                p { margin: 15px 0; }
                blockquote {
                    border-left: 4px solid #3498db;
                    margin: 20px 0;
                    padding-left: 15px;
                    color: #666;
                    font-style: italic;
                }
                .image-placeholder {
                    background: #f8f9fa;
                    border: 2px dashed #dee2e6;
                    padding: 20px;
                    text-align: center;
                    color: #6c757d;
                    margin: 15px 0;
                    border-radius: 4px;
                }
            """
            html_doc.head.append(style_tag)
            
            # åˆ›å»ºæ–‡ç« å¤´éƒ¨
            header_div = html_doc.new_tag('div', class_='article-header')
            
            title_h1 = html_doc.new_tag('h1', class_='article-title')
            title_h1.string = title
            header_div.append(title_h1)
            
            meta_div = html_doc.new_tag('div', class_='article-meta')
            meta_div.string = f"ä½œè€…: {author} | å‘å¸ƒæ—¶é—´: {publish_date} | åŸæ–‡é“¾æ¥: {url}"
            header_div.append(meta_div)
            
            html_doc.body.append(header_div)
            
            # åˆ›å»ºæ–‡ç« å†…å®¹å®¹å™¨
            content_div = html_doc.new_tag('div', class_='article-content')
            
            # å¤„ç†å›¾ç‰‡å¹¶æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
            base_dir = os.path.dirname(output_path)
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:20]
            images_dir = os.path.join(base_dir, f"images_{safe_title}")
            os.makedirs(images_dir, exist_ok=True)
            
            # æ›¿æ¢å›¾ç‰‡é“¾æ¥
            img_count = 0
            for img_tag in content_soup.find_all('img'):
                img_src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
                if img_src:
                    # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°å›¾ç‰‡
                    local_img_path = None
                    for img_info in images:
                        if img_info.get('url') == img_src or img_src in img_info.get('url', ''):
                            if img_info.get('local_path') and os.path.exists(img_info['local_path']):
                                # å¤åˆ¶å›¾ç‰‡åˆ°HTMLç›®å½•
                                img_filename = f"img_{img_count:03d}.jpg"
                                new_img_path = os.path.join(images_dir, img_filename)
                                try:
                                    import shutil
                                    shutil.copy2(img_info['local_path'], new_img_path)
                                    local_img_path = f"images_{safe_title}/{img_filename}"
                                    img_count += 1
                                except Exception as e:
                                    logger.debug(f"å¤åˆ¶å›¾ç‰‡å¤±è´¥: {e}")
                                break
                    
                    if local_img_path:
                        img_tag['src'] = local_img_path
                        # æ¸…ç†å…¶ä»–å±æ€§
                        for attr in ['data-src', 'data-original', 'data-lazy-src']:
                            if img_tag.get(attr):
                                del img_tag[attr]
                    else:
                        # åˆ›å»ºå›¾ç‰‡å ä½ç¬¦
                        placeholder_div = html_doc.new_tag('div', class_='image-placeholder')
                        placeholder_div.string = f"[å›¾ç‰‡: {img_src}]"
                        img_tag.replace_with(placeholder_div)
            
            # å°†å¤„ç†åçš„å†…å®¹æ·»åŠ åˆ°HTMLæ–‡æ¡£
            for element in content_soup.children:
                if hasattr(element, 'name'):
                    content_div.append(element)
            
            html_doc.body.append(content_div)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(str(html_doc.prettify()))
            
            logger.success(f"å®Œæ•´HTMLå·²ä¿å­˜: {output_path}")
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å›¾ç‰‡={img_count}å¼ , å›¾ç‰‡ç›®å½•={images_dir}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å®Œæ•´HTMLå¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def extract_full_article_content(self, url: str, download_images: bool = True) -> dict:
        """
        æå–å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ŒåŒ…æ‹¬HTMLå†…å®¹å’Œå›¾ç‰‡
        
        Args:
            url: æ–‡ç« URL
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
            
        Returns:
            åŒ…å«å®Œæ•´å†…å®¹çš„å­—å…¸
        """
        try:
            if not self.driver:
                self.driver = self.setup_browser(self.headless)
                if not self.driver:
                    return {"error": "æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"}

            logger.info(f"ğŸš€ å¼€å§‹æå–å®Œæ•´æ–‡ç« å†…å®¹: {url}")
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 10)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except TimeoutException:
                logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­æå–")
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            self._wait_for_basic_page_load()
            
            # æ»šåŠ¨åŠ è½½å®Œæ•´å†…å®¹
            self._human_like_scroll_and_load()
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = self._extract_article_content()
            if 'error' in basic_info:
                return basic_info
            
            # æå–å®Œæ•´HTMLå†…å®¹
            html_content = self._extract_html_content()
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self._extract_publish_date()
            
            # å¤„ç†å›¾ç‰‡
            images_info = []
            if download_images:
                images_info = self._extract_and_download_images()
            
            # æ„å»ºå®Œæ•´çš„æ–‡ç« ä¿¡æ¯
            full_article = {
                'title': basic_info.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                'author': basic_info.get('author', 'æœªçŸ¥ä½œè€…'),
                'url': basic_info.get('url', url),
                'publish_date': publish_date,
                'html_content': html_content,
                'text_content': self._html_to_text(html_content),
                'images': images_info,
                'extraction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.success(f"âœ… æˆåŠŸæå–å®Œæ•´æ–‡ç« å†…å®¹: {full_article['title'][:30]}...")
            logger.info(f"ğŸ“Š å†…å®¹ç»Ÿè®¡: æ–‡å­—é•¿åº¦={len(full_article['text_content'])}, å›¾ç‰‡æ•°é‡={len(images_info)}")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ æå–å®Œæ•´æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return {"error": f"æå–å¤±è´¥: {str(e)}"}
    
    def _extract_html_content(self) -> str:
        """æå–æ–‡ç« çš„HTMLå†…å®¹"""
        try:
            # å¾®ä¿¡æ–‡ç« å†…å®¹çš„å¸¸è§é€‰æ‹©å™¨
            content_selectors = [
                '#js_content',
                '.rich_media_content', 
                '.appmsg_content_text',
                '.article_content',
                '[data-role="article-content"]'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if content_elem:
                        # è·å–å…ƒç´ çš„HTMLå†…å®¹
                        html_content = content_elem.get_attribute('innerHTML')
                        if html_content and len(html_content.strip()) > 100:
                            logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(html_content)}")
                            return html_content
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•è·å–æ•´ä¸ªbody
            logger.warning("æœªæ‰¾åˆ°ä¸“ç”¨å†…å®¹å®¹å™¨ï¼Œå°è¯•æå–bodyå†…å®¹")
            body_elem = self.driver.find_element(By.TAG_NAME, "body")
            return body_elem.get_attribute('innerHTML') or ""
            
        except Exception as e:
            logger.error(f"æå–HTMLå†…å®¹å¤±è´¥: {e}")
            return ""
    
    def _extract_publish_date(self) -> str:
        """æå–æ–‡ç« å‘å¸ƒæ—¶é—´"""
        try:
            # å‘å¸ƒæ—¶é—´çš„å¸¸è§é€‰æ‹©å™¨
            date_selectors = [
                '#publish_time',
                '.rich_media_meta_text',
                '.publish_time',
                '[data-role="publish-time"]',
                '.time'
            ]
            
            for selector in date_selectors:
                try:
                    date_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if date_elem and date_elem.text.strip():
                        date_text = date_elem.text.strip()
                        logger.debug(f"é€šè¿‡é€‰æ‹©å™¨ {selector} æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                        return date_text
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»é¡µé¢æºç ä¸­æå–
            page_source = self.driver.page_source
            import re
            
            # å°è¯•åŒ¹é…å¸¸è§çš„æ—¶é—´æ ¼å¼
            date_patterns = [
                r'publish_time["\s]*[=:]["\s]*("[^"]+)"',
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{1,2}æœˆ\d{1,2}æ—¥)'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_source)
                if match:
                    date_text = match.group(1)
                    logger.debug(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°å‘å¸ƒæ—¶é—´: {date_text}")
                    return date_text
            
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.debug(f"æå–å‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def _extract_and_download_images(self) -> List[Dict[str, str]]:
        """æå–å¹¶ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡"""
        try:
            images_info = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            logger.info(f"ğŸ–¼ï¸ å‘ç° {len(img_elements)} å¼ å›¾ç‰‡")
            
            for i, img_elem in enumerate(img_elements):
                try:
                    # è·å–å›¾ç‰‡URL
                    img_src = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
                    
                    if not img_src or img_src.startswith('data:'):
                        continue  # è·³è¿‡base64å›¾ç‰‡å’Œæ— æ•ˆå›¾ç‰‡
                    
                    # è·å–å›¾ç‰‡çš„altå±æ€§ä½œä¸ºæè¿°
                    img_alt = img_elem.get_attribute('alt') or f"å›¾ç‰‡_{i+1}"
                    
                    # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
                    img_filename = f"image_{i+1:03d}.jpg"
                    
                    # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
                    images_dir = os.path.join("output", "images", 
                                            datetime.now().strftime('%Y%m%d'))
                    os.makedirs(images_dir, exist_ok=True)
                    
                    # æœ¬åœ°ä¿å­˜è·¯å¾„
                    local_path = os.path.join(images_dir, img_filename)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    if self._download_image(img_src, local_path):
                        images_info.append({
                            'original_url': img_src,
                            'local_path': local_path,
                            'filename': img_filename,
                            'alt': img_alt,
                            'index': i + 1
                        })
                        logger.debug(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {img_filename}")
                    else:
                        logger.warning(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_src}")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡ {i+1} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.success(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(images_info)}/{len(img_elements)}")
            return images_info
            
        except Exception as e:
            logger.error(f"æå–å’Œä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return []
    
    def _html_to_text(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
        try:
            from bs4 import BeautifulSoup
            
            # è§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for element in soup(["script", "style"]):
                element.decompose()
            
            # è·å–çº¯æ–‡æœ¬
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            clean_text = '\n'.join(lines)
            
            return clean_text
            
        except Exception as e:
            logger.error(f"HTMLè½¬æ–‡æœ¬å¤±è´¥: {e}")
            return ""
    
    def save_as_markdown(self, url: str, output_path: str) -> bool:
        """ä¿å­˜URLä¸ºMarkdownæ ¼å¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜Markdownæ–‡æ¡£: {url}")
            
            # ä½¿ç”¨å·²æœ‰çš„æ–‡ç« æå–æ–¹æ³•ï¼Œé¿å…é‡å¤æ»šåŠ¨
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # ç”ŸæˆMarkdownå†…å®¹
            markdown_content = []
            
            # æ·»åŠ æ ‡é¢˜
            title = article_data.get('title', 'å¾®ä¿¡æ–‡ç« ')
            markdown_content.append(f"# {title}\n")
            
            # æ·»åŠ å…ƒä¿¡æ¯
            author = article_data.get('author', 'æœªçŸ¥')
            publish_date = article_data.get('publish_date', 'æœªçŸ¥')
            markdown_content.append(f"**ä½œè€…**: {author}")
            markdown_content.append(f"**å‘å¸ƒæ—¶é—´**: {publish_date}")
            markdown_content.append(f"**åŸæ–‡é“¾æ¥**: {url}")
            markdown_content.append("\n---\n")
            
            # è½¬æ¢HTMLå†…å®¹ä¸ºMarkdown
            content_soup = article_data.get('content_soup')
            if content_soup:
                markdown_text = self._convert_soup_to_markdown(content_soup)
                markdown_content.append(markdown_text)
            else:
                markdown_content.append("æ— æ³•æå–æ–‡ç« å†…å®¹")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜Markdownæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(markdown_content))
            
            logger.success(f"Markdownæ–‡æ¡£ä¿å­˜æˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜Markdownæ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def save_as_json(self, url: str, output_path: str) -> bool:
        """ä¿å­˜URLä¸ºJSONæ ¼å¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            logger.info(f"æ­£åœ¨ä¿å­˜JSONæ•°æ®: {url}")
            
            # ä½¿ç”¨å·²æœ‰çš„æ–‡ç« æå–æ–¹æ³•ï¼Œé¿å…é‡å¤æ»šåŠ¨
            article_data = self._extract_wechat_article_with_selenium(url)
            
            if not article_data or 'error' in article_data:
                logger.error("æ— æ³•è·å–æ–‡ç« å†…å®¹")
                return False
            
            # æ„å»ºJSONæ•°æ®
            json_data = {
                "title": article_data.get('title', ''),
                "author": article_data.get('author', ''),
                "publish_date": article_data.get('publish_date', ''),
                "url": url,
                "extraction_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "content": {
                    "html": str(article_data.get('content_soup', '')),
                    "text": article_data.get('content_soup', BeautifulSoup('', 'html.parser')).get_text(strip=True) if article_data.get('content_soup') else ''
                },
                "images": article_data.get('images', []),
                "metadata": {
                    "total_images": len(article_data.get('images', [])),
                    "content_length": len(article_data.get('content_soup', BeautifulSoup('', 'html.parser')).get_text(strip=True)) if article_data.get('content_soup') else 0,
                    "extraction_method": "selenium"
                }
            }
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.success(f"JSONæ•°æ®ä¿å­˜æˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ•°æ®å¤±è´¥: {e}")
            return False

    def _convert_soup_to_markdown(self, soup: BeautifulSoup) -> str:
        """å°†BeautifulSoupå¯¹è±¡è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        try:
            markdown_lines = []
            
            for element in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'img', 'strong', 'b', 'em', 'i']):
                if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    level = int(element.name[1])
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"{'#' * level} {text}\n")
                        
                elif element.name == 'img':
                    src = element.get('src', '')
                    alt = element.get('alt', 'å›¾ç‰‡')
                    if src:
                        markdown_lines.append(f"![{alt}]({src})\n")
                        
                elif element.name in ['strong', 'b']:
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"**{text}**")
                        
                elif element.name in ['em', 'i']:
                    text = element.get_text(strip=True)
                    if text:
                        markdown_lines.append(f"*{text}*")
                        
                elif element.name in ['p', 'div']:
                    text = element.get_text(strip=True)
                    if text and len(text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        markdown_lines.append(f"{text}\n")
            
            return '\n'.join(markdown_lines)
            
        except Exception as e:
            logger.error(f"è½¬æ¢Markdownå¤±è´¥: {e}")
            return soup.get_text(strip=True) if soup else ""




def main():
    """æµ‹è¯•å·¥å…·"""
    # ç¤ºä¾‹URL
    test_url = "https://mp.weixin.qq.com/s/LiS8ytwKsKP9yAHGp96G_Q"
    
    scraper = SimpleUrlScraper(headless=False)  # ä½¿ç”¨å¯è§æ¨¡å¼ä¾¿äºå¤„ç†éªŒè¯ç 
    
    try:
        # æµ‹è¯•PDFç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pdf_path = f"output/pdf/test_article_{timestamp}.pdf"
        
        if scraper.save_as_pdf(test_url, pdf_path):
            logger.success(f"PDFæµ‹è¯•æˆåŠŸ: {pdf_path}")
        
        # æµ‹è¯•å®Œæ•´HTMLä¿å­˜
        html_path = f"output/html/test_article_{timestamp}.html"
        
        if scraper.save_complete_html(test_url, html_path):
            logger.success(f"å®Œæ•´HTMLæµ‹è¯•æˆåŠŸ: {html_path}")
        
        # æµ‹è¯•åŸºæœ¬æ–‡ç« ä¿¡æ¯æå–
        article = scraper.extract_article_info(test_url)
        
        if 'error' not in article:
            logger.success(f"æ–‡ç« ä¿¡æ¯æå–æˆåŠŸ: {article.get('title')}")
            
        else:
            logger.error(f"å¤„ç†å¤±è´¥: {article.get('error')}")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        scraper.cleanup()


if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦å®Œæ•´å†…å®¹ä¸Šä¼ å™¨ - Feishu Full Content Uploader

å°†å¾®ä¿¡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆæ–‡å­—+å›¾ç‰‡ï¼‰ä¸Šä¼ åˆ°é£ä¹¦çŸ¥è¯†åº“ï¼Œä¿æŒåŸæœ‰æ ¼å¼
"""

import os
import json
import requests
import time
from typing import List, Dict, Any, Optional
from loguru import logger
from datetime import datetime
from bs4 import BeautifulSoup
import re

from feishu_user_client import FeishuUserAPIClient


class FeishuContentUploader:
    """é£ä¹¦å®Œæ•´å†…å®¹ä¸Šä¼ å™¨"""
    
    def __init__(self, app_id: str, app_secret: str, access_token: str = None):
        """åˆå§‹åŒ–é£ä¹¦å†…å®¹ä¸Šä¼ å™¨
        
        Args:
            app_id: é£ä¹¦åº”ç”¨ID
            app_secret: é£ä¹¦åº”ç”¨å¯†é’¥
            access_token: ç”¨æˆ·è®¿é—®ä»¤ç‰Œï¼ˆå¯é€‰ï¼‰
        """
        self.client = FeishuUserAPIClient(app_id, app_secret, access_token)
        logger.info("é£ä¹¦å®Œæ•´å†…å®¹ä¸Šä¼ å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def upload_full_article_content(self, full_content: Dict[str, Any], space_id: str, parent_node_token: str = None) -> Optional[str]:
        """ä¸Šä¼ å®Œæ•´çš„æ–‡ç« å†…å®¹åˆ°é£ä¹¦çŸ¥è¯†åº“
        
        Args:
            full_content: å®Œæ•´æ–‡ç« å†…å®¹ï¼ˆæ¥è‡ªextract_full_article_contentï¼‰
            space_id: é£ä¹¦çŸ¥è¯†åº“ç©ºé—´ID
            parent_node_token: çˆ¶èŠ‚ç‚¹tokenï¼ŒæŒ‡å®šä¸Šä¼ ä½ç½®
            
        Returns:
            æ–‡æ¡£URLï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            if 'error' in full_content:
                logger.error(f"è¾“å…¥å†…å®¹åŒ…å«é”™è¯¯: {full_content.get('error')}")
                return None
            
            title = full_content.get('title', 'æœªçŸ¥æ ‡é¢˜')
            logger.info(f"ğŸš€ å¼€å§‹ä¸Šä¼ å®Œæ•´æ–‡ç« å†…å®¹åˆ°é£ä¹¦: {title}")
            
            # 1. ä¸Šä¼ æ‰€æœ‰å›¾ç‰‡åˆ°é£ä¹¦äº‘ç›˜
            logger.info("ğŸ“¤ ç¬¬1æ­¥: ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦äº‘ç›˜...")
            image_map = self._upload_images_to_feishu(full_content.get('images', []))
            
            # 2. å¤„ç†HTMLå†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡URLä¸ºé£ä¹¦é“¾æ¥
            logger.info("ğŸ”„ ç¬¬2æ­¥: å¤„ç†HTMLå†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡é“¾æ¥...")
            processed_html = self._process_html_content(
                full_content.get('html_content', ''),
                image_map
            )
            
            # 3. è½¬æ¢ä¸ºé£ä¹¦æ”¯æŒçš„æ ¼å¼
            logger.info("ğŸ“ ç¬¬3æ­¥: è½¬æ¢å†…å®¹æ ¼å¼...")
            feishu_content = self._convert_to_feishu_format(
                title=title,
                author=full_content.get('author', 'æœªçŸ¥ä½œè€…'),
                publish_date=full_content.get('publish_date', ''),
                url=full_content.get('url', ''),
                html_content=processed_html,
                text_content=full_content.get('text_content', ''),
                images_count=len(full_content.get('images', []))
            )
            
            # 4. åˆ›å»ºé£ä¹¦æ–‡æ¡£
            logger.info("ğŸ“‹ ç¬¬4æ­¥: åˆ›å»ºé£ä¹¦çŸ¥è¯†åº“æ–‡æ¡£...")
            doc_url = self._create_feishu_document(
                title=title,
                content=feishu_content,
                space_id=space_id,
                parent_node_token=parent_node_token
            )
            
            if doc_url:
                logger.success(f"ğŸ‰ æ–‡ç« å†…å®¹ä¸Šä¼ æˆåŠŸ: {title}")
                logger.success(f"ğŸ“– æ–‡æ¡£é“¾æ¥: {doc_url}")
            else:
                logger.error(f"âŒ æ–‡ç« å†…å®¹ä¸Šä¼ å¤±è´¥: {title}")
            
            return doc_url
            
        except Exception as e:
            logger.error(f"ä¸Šä¼ å®Œæ•´æ–‡ç« å†…å®¹å¼‚å¸¸: {e}")
            return None
    
    def _upload_images_to_feishu(self, images: List[Dict[str, str]]) -> Dict[str, str]:
        """ä¸Šä¼ å›¾ç‰‡åˆ°é£ä¹¦äº‘ç›˜å¹¶è·å–é“¾æ¥
        
        Args:
            images: å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            æœ¬åœ°è·¯å¾„åˆ°é£ä¹¦æ–‡ä»¶tokençš„æ˜ å°„
        """
        image_map = {}
        
        if not images:
            logger.info("ğŸ“· æ²¡æœ‰å›¾ç‰‡éœ€è¦ä¸Šä¼ ")
            return image_map
        
        logger.info(f"ğŸ“· å¼€å§‹ä¸Šä¼  {len(images)} å¼ å›¾ç‰‡åˆ°é£ä¹¦äº‘ç›˜...")
        
        for i, img_info in enumerate(images):
            try:
                local_path = img_info.get('local_path')
                filename = img_info.get('filename')
                
                if not local_path or not os.path.exists(local_path):
                    logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
                    continue
                
                logger.debug(f"ä¸Šä¼ å›¾ç‰‡ {i+1}/{len(images)}: {filename}")
                
                # ä¸Šä¼ åˆ°é£ä¹¦äº‘ç›˜
                file_token = self.client.upload_file_to_drive(local_path)
                
                if file_token:
                    image_map[local_path] = file_token
                    logger.debug(f"âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {filename} -> {file_token}")
                else:
                    logger.warning(f"âŒ å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {filename}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(images) - 1:
                    time.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"ä¸Šä¼ å›¾ç‰‡ {img_info.get('filename', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.success(f"ğŸ“· å›¾ç‰‡ä¸Šä¼ å®Œæˆ: {len(image_map)}/{len(images)} å¼ æˆåŠŸ")
        return image_map
    
    def _process_html_content(self, html_content: str, image_map: Dict[str, str]) -> str:
        """å¤„ç†HTMLå†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡URLä¸ºé£ä¹¦é“¾æ¥
        
        Args:
            html_content: åŸå§‹HTMLå†…å®¹
            image_map: æœ¬åœ°è·¯å¾„åˆ°é£ä¹¦æ–‡ä»¶tokençš„æ˜ å°„
            
        Returns:
            å¤„ç†åçš„HTMLå†…å®¹
        """
        if not html_content:
            return ""
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
            img_tags = soup.find_all('img')
            replaced_count = 0
            
            for img_tag in img_tags:
                try:
                    # è·å–åŸå§‹å›¾ç‰‡URL
                    original_src = img_tag.get('src') or img_tag.get('data-src')
                    
                    if not original_src:
                        continue
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ°æ–‡ä»¶
                    matching_local_path = None
                    for local_path, file_token in image_map.items():
                        # é€šè¿‡æ–‡ä»¶ååŒ¹é…ï¼ˆç®€å•ç­–ç•¥ï¼‰
                        if local_path in image_map:
                            matching_local_path = local_path
                            break
                    
                    if matching_local_path:
                        # æ›¿æ¢ä¸ºé£ä¹¦äº‘ç›˜é“¾æ¥
                        feishu_file_token = image_map[matching_local_path]
                        # æ„å»ºé£ä¹¦å›¾ç‰‡æ˜¾ç¤ºURL
                        feishu_img_url = f"https://open.feishu.cn/open-apis/drive/v1/medias/{feishu_file_token}/download"
                        
                        img_tag['src'] = feishu_img_url
                        # ç§»é™¤data-srcå±æ€§
                        if img_tag.get('data-src'):
                            del img_tag['data-src']
                        
                        replaced_count += 1
                        logger.debug(f"æ›¿æ¢å›¾ç‰‡é“¾æ¥: {original_src[:50]}... -> {feishu_img_url}")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†å›¾ç‰‡æ ‡ç­¾æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"ğŸ”„ HTMLå›¾ç‰‡é“¾æ¥æ›¿æ¢å®Œæˆ: {replaced_count} å¼ å›¾ç‰‡")
            return str(soup)
            
        except Exception as e:
            logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {e}")
            return html_content
    
    def _convert_to_feishu_format(self, title: str, author: str, publish_date: str, 
                                 url: str, html_content: str, text_content: str, 
                                 images_count: int) -> str:
        """å°†å†…å®¹è½¬æ¢ä¸ºé£ä¹¦æ”¯æŒçš„æ ¼å¼
        
        Args:
            title: æ–‡ç« æ ‡é¢˜
            author: ä½œè€…
            publish_date: å‘å¸ƒæ—¶é—´
            url: åŸæ–‡é“¾æ¥
            html_content: HTMLå†…å®¹
            text_content: çº¯æ–‡æœ¬å†…å®¹
            images_count: å›¾ç‰‡æ•°é‡
            
        Returns:
            é£ä¹¦æ ¼å¼çš„å†…å®¹
        """
        try:
            # é£ä¹¦æ”¯æŒMarkdownæ ¼å¼ï¼Œæˆ‘ä»¬æ„å»ºMarkdownå†…å®¹
            feishu_content = f"""# {title}

## ğŸ“‹ æ–‡ç« ä¿¡æ¯

- **ä½œè€…**: {author}
- **å‘å¸ƒæ—¶é—´**: {publish_date}
- **åŸæ–‡é“¾æ¥**: [{title}]({url})
- **å›¾ç‰‡æ•°é‡**: {images_count} å¼ 
- **é‡‡é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“„ æ–‡ç« å†…å®¹

"""
            
            # å°è¯•å°†HTMLè½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æœ¬
            if html_content:
                # ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢
                markdown_content = self._html_to_markdown(html_content)
                feishu_content += markdown_content
            else:
                # å¦‚æœæ²¡æœ‰HTMLå†…å®¹ï¼Œä½¿ç”¨çº¯æ–‡æœ¬
                feishu_content += text_content
            
            # æ·»åŠ è„šæ³¨
            feishu_content += f"""

---

> ğŸ“ æ­¤æ–‡æ¡£ç”±å¾®ä¿¡æ–‡ç« çˆ¬è™«è‡ªåŠ¨ç”Ÿæˆ  
> ğŸ•’ ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
> ğŸ”— åŸå§‹é“¾æ¥: {url}

"""
            
            return feishu_content
            
        except Exception as e:
            logger.error(f"è½¬æ¢é£ä¹¦æ ¼å¼æ—¶å‡ºé”™: {e}")
            # è¿”å›åŸºæœ¬æ ¼å¼
            return f"""# {title}

**ä½œè€…**: {author}  
**å‘å¸ƒæ—¶é—´**: {publish_date}  
**åŸæ–‡é“¾æ¥**: {url}

{text_content}

---
æ­¤æ–‡æ¡£ç”±å¾®ä¿¡æ–‡ç« çˆ¬è™«è‡ªåŠ¨ç”Ÿæˆ
"""
    
    def _html_to_markdown(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            Markdownæ ¼å¼çš„å†…å®¹
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'meta', 'link']):
                element.decompose()
            
            # ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢
            markdown_lines = []
            
            for element in soup.find_all():
                if element.name == 'h1':
                    markdown_lines.append(f"\n# {element.get_text().strip()}\n")
                elif element.name == 'h2':
                    markdown_lines.append(f"\n## {element.get_text().strip()}\n")
                elif element.name == 'h3':
                    markdown_lines.append(f"\n### {element.get_text().strip()}\n")
                elif element.name == 'p':
                    text = element.get_text().strip()
                    if text:
                        markdown_lines.append(f"\n{text}\n")
                elif element.name == 'strong' or element.name == 'b':
                    text = element.get_text().strip()
                    if text:
                        markdown_lines.append(f"**{text}**")
                elif element.name == 'em' or element.name == 'i':
                    text = element.get_text().strip()
                    if text:
                        markdown_lines.append(f"*{text}*")
                elif element.name == 'img':
                    src = element.get('src', '')
                    alt = element.get('alt', 'å›¾ç‰‡')
                    if src:
                        markdown_lines.append(f"\n![{alt}]({src})\n")
                elif element.name == 'br':
                    markdown_lines.append("\n")
            
            # å¦‚æœè½¬æ¢ç»“æœä¸ºç©ºï¼Œä½¿ç”¨çº¯æ–‡æœ¬
            if not markdown_lines:
                return soup.get_text(separator='\n', strip=True)
            
            return ''.join(markdown_lines)
            
        except Exception as e:
            logger.debug(f"HTMLåˆ°Markdownè½¬æ¢å¤±è´¥: {e}")
            # é™çº§ä¸ºçº¯æ–‡æœ¬
            soup = BeautifulSoup(html_content, 'html.parser')
            return soup.get_text(separator='\n', strip=True)
    
    def _create_feishu_document(self, title: str, content: str, space_id: str, 
                               parent_node_token: str = None) -> Optional[str]:
        """åˆ›å»ºé£ä¹¦æ–‡æ¡£å¹¶å†™å…¥å†…å®¹
        
        Args:
            title: æ–‡æ¡£æ ‡é¢˜
            content: æ–‡æ¡£å†…å®¹
            space_id: çŸ¥è¯†åº“ç©ºé—´ID
            parent_node_token: çˆ¶èŠ‚ç‚¹token
            
        Returns:
            æ–‡æ¡£URLï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # åˆ›å»ºç©ºæ–‡æ¡£
            node_token = self.client.create_wiki_document(
                space_id=space_id,
                title=title,
                file_token=None,  # ä¸ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ›å»ºç©ºæ–‡æ¡£
                parent_node_token=parent_node_token,
                file_type="docx"
            )
            
            if not node_token:
                logger.error("åˆ›å»ºé£ä¹¦æ–‡æ¡£èŠ‚ç‚¹å¤±è´¥")
                return None
            
            logger.success(f"âœ… æ–‡æ¡£èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {node_token}")
            
            # å°è¯•å‘æ–‡æ¡£å†™å…¥å†…å®¹ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰
            # æ³¨æ„ï¼šé£ä¹¦æ–‡æ¡£å†…å®¹ç¼–è¾‘å¯èƒ½éœ€è¦ç‰¹æ®Šçš„APIè°ƒç”¨
            success = self._write_content_to_document(node_token, content)
            
            if success:
                logger.success(f"âœ… æ–‡æ¡£å†…å®¹å†™å…¥æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸ æ–‡æ¡£åˆ›å»ºæˆåŠŸä½†å†…å®¹å†™å…¥å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘æ–‡æ¡£")
            
            # ç”Ÿæˆæ–‡æ¡£URL
            doc_url = f"https://thedream.feishu.cn/wiki/{node_token}"
            return doc_url
            
        except Exception as e:
            logger.error(f"åˆ›å»ºé£ä¹¦æ–‡æ¡£å¼‚å¸¸: {e}")
            return None
    
    def _write_content_to_document(self, node_token: str, content: str) -> bool:
        """å‘é£ä¹¦æ–‡æ¡£å†™å…¥å†…å®¹
        
        Args:
            node_token: æ–‡æ¡£èŠ‚ç‚¹token
            content: è¦å†™å…¥çš„å†…å®¹
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨é£ä¹¦çš„æ–‡æ¡£ç¼–è¾‘API
            # ç”±äºé£ä¹¦æ–‡æ¡£ç¼–è¾‘APIæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œå…ˆä¿å­˜å†…å®¹åˆ°æ–‡æ¡£æè¿°æˆ–æ³¨é‡Šä¸­
            
            # æ–¹æ³•1: å°è¯•ä¿å­˜ä¸ºæ–‡æ¡£æ ‡ç­¾æˆ–æè¿°
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…æƒ…å†µå¯èƒ½éœ€è¦ä½¿ç”¨æ›´å¤æ‚çš„æ–‡æ¡£ç¼–è¾‘API
            
            logger.info(f"ğŸ“ å°è¯•å°†å†…å®¹å†™å…¥æ–‡æ¡£: {node_token}")
            logger.info(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # ä¸´æ—¶æ–¹æ¡ˆï¼šå°†å†…å®¹ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·æ‰‹åŠ¨å¤åˆ¶ç²˜è´´
            self._save_content_for_manual_copy(node_token, content)
            
            return True
            
        except Exception as e:
            logger.error(f"å†™å…¥æ–‡æ¡£å†…å®¹å¼‚å¸¸: {e}")
            return False
    
    def _save_content_for_manual_copy(self, node_token: str, content: str):
        """ä¿å­˜å†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¾›ç”¨æˆ·æ‰‹åŠ¨å¤åˆ¶
        
        Args:
            node_token: æ–‡æ¡£èŠ‚ç‚¹token
            content: å†…å®¹
        """
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = "output/feishu_content"
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜Markdownå†…å®¹
            md_file = os.path.join(output_dir, f"{node_token}_content.md")
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"ğŸ“ å†…å®¹å·²ä¿å­˜åˆ°: {md_file}")
            logger.info(f"ğŸ’¡ æ‚¨å¯ä»¥å¤åˆ¶æ­¤æ–‡ä»¶çš„å†…å®¹åˆ°é£ä¹¦æ–‡æ¡£ä¸­")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å†…å®¹æ–‡ä»¶å¼‚å¸¸: {e}")


def test_content_uploader():
    """æµ‹è¯•å®Œæ•´å†…å®¹ä¸Šä¼ å™¨"""
    
    # é…ç½®ä¿¡æ¯
    app_id = "cli_a8c822312a75901c"
    app_secret = "NDbCyKEwEIA8CZo2KHyqueIOlcafErko"
    space_id = "7429397059456081921"  # çŸ¥è¯†åº“ID
    parent_node_token = "Rkr5w3y8hib7dRk1KpFcMZ7tnGc"  # ç›®æ ‡ä½ç½®
    
    # ä½¿ç”¨æµ‹è¯•æå–çš„å†…å®¹
    test_content_file = "output/full_content_test/extracted_content.json"
    
    if not os.path.exists(test_content_file):
        logger.error(f"æµ‹è¯•å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {test_content_file}")
        return
    
    try:
        # è¯»å–æµ‹è¯•å†…å®¹
        with open(test_content_file, 'r', encoding='utf-8') as f:
            full_content = json.load(f)
        
        logger.info("ğŸ“– è¯»å–æµ‹è¯•å†…å®¹æˆåŠŸ")
        logger.info(f"ğŸ“ æ ‡é¢˜: {full_content.get('title', 'N/A')}")
        logger.info(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {len(full_content.get('images', []))}")
        
        # åˆå§‹åŒ–ä¸Šä¼ å™¨
        uploader = FeishuContentUploader(app_id, app_secret)
        
        # ä¸Šä¼ å†…å®¹
        doc_url = uploader.upload_full_article_content(
            full_content=full_content,
            space_id=space_id,
            parent_node_token=parent_node_token
        )
        
        if doc_url:
            logger.success(f"ğŸ‰ æµ‹è¯•ä¸Šä¼ æˆåŠŸ!")
            logger.success(f"ğŸ“– æ–‡æ¡£é“¾æ¥: {doc_url}")
        else:
            logger.error("âŒ æµ‹è¯•ä¸Šä¼ å¤±è´¥")
    
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")


if __name__ == "__main__":
    test_content_uploader() 